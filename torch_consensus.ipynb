{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trying to modify a PyTorch tutorial to suit our needs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple\n",
    "from itertools import count\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "from torch.autograd import Variable\n",
    "\n",
    "# set up matplotlib\n",
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "\n",
    "plt.ion()\n",
    "\n",
    "# if gpu is to be used\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a simple environment that satisfies openAI gym requirements\n",
    "# Ref: https://towardsdatascience.com/creating-a-custom-openai-gym-environment-for-stock-trading-be532be3910e\n",
    "# Ref: https://github.com/openai/gym/blob/master/docs/creating-environments.md\n",
    "U_VELOCITY = 1\n",
    "U_ACCELERATION = 2\n",
    "O_VELOCITY = 1\n",
    "O_ACCELERATION = 2\n",
    "\n",
    "input_type = U_ACCELERATION\n",
    "observe_type = O_VELOCITY\n",
    "N = 10\n",
    "env = gym.make('ConsensusEnv:ConsensusContEnv-v0', N=N, dt=0.1, Delta=0.05,\n",
    "              input_type=input_type, observe_type=observe_type).unwrapped\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replay Memory and other packages\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test out functions to auto-run the training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'utils.agents' from '/Users/zz/Documents/GT20F/7000/GNN_experiments/utils/agents.py'>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from utils.ReplayMemory import * \n",
    "from utils.networks import *\n",
    "from utils.agents import *\n",
    "\n",
    "from importlib import reload\n",
    "import utils\n",
    "reload(utils.ReplayMemory)\n",
    "reload(utils.networks)\n",
    "reload(utils.agents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(utils.ReplayMemory)\n",
    "reload(utils.networks)\n",
    "reload(utils.agents)\n",
    "from utils.ReplayMemory import * \n",
    "from utils.networks import *\n",
    "from utils.agents import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "na = env.na\n",
    "ns = env.nf\n",
    "hidden = 32\n",
    "\n",
    "test_agents = [\n",
    "    LearnerAgent(device, N, ns, na, hidden),\n",
    "    RewardAgent(device, N, ns, na, hidden),\n",
    "    RewardActionAgent(device, N, ns, na, hidden)\n",
    "]\n",
    "\n",
    "num_episode=5\n",
    "test_interval=2\n",
    "num_test=2\n",
    "num_iteration=200\n",
    "BATCH_SIZE=128\n",
    "debug=True\n",
    "num_sample=50\n",
    "\n",
    "if input_type == U_ACCELERATION:\n",
    "    action_space = [-env.a_max, env.a_max]\n",
    "else:\n",
    "    action_space = [-env.v_max, env.v_max]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode  0  finished; t =  199\n",
      "Finished episode  0  with  199  steps, and rewards =  [-251.11914981 -291.36713243 -220.24629986 -237.88564374 -243.02959101\n",
      " -372.25400294 -265.6448024  -391.98667503 -321.9655733  -307.10024736] ;\n",
      "cumulative reward =  -271310.589116754\n",
      "Finished episode  1  with  199  steps, and rewards =  [-350.86176789 -384.78971752 -298.12004072 -405.14136268 -374.80127399\n",
      " -479.67580277 -383.49706958 -427.52902349 -263.57492306 -269.16934183] ;\n",
      "cumulative reward =  -341555.8227882968\n",
      "Episode  1  finished; t =  199\n",
      "Episode  2  finished; t =  199\n",
      "Finished episode  0  with  199  steps, and rewards =  [-441.87989561 -454.44968415 -635.27043628 -660.64520187 -433.35208989\n",
      " -635.10477321 -428.33539534 -388.6669188  -433.48192649 -529.32743464] ;\n",
      "cumulative reward =  -453539.35288381006\n",
      "Finished episode  1  with  199  steps, and rewards =  [-1486.52939612 -1099.55527799 -1486.45847416 -1008.37178213\n",
      " -1487.314863   -1017.43118255 -1221.08864964 -1052.20055909\n",
      " -1307.92157536 -1373.02029049] ;\n",
      "cumulative reward =  -818705.4806240638\n",
      "Episode  3  finished; t =  199\n",
      "Episode  4  finished; t =  199\n",
      "Finished episode  0  with  199  steps, and rewards =  [ -779.29314862  -778.28441622  -777.39045352  -808.78496591\n",
      " -1369.94381959  -777.38547693  -777.94997342 -1215.36732101\n",
      " -1118.59787604  -782.67682714] ;\n",
      "cumulative reward =  -580621.8888298265\n",
      "Finished episode  1  with  199  steps, and rewards =  [ -343.85944612  -346.21438222  -388.0852005   -361.70886955\n",
      "  -466.38175174 -1017.1086271   -358.45316223  -354.48577565\n",
      " -1020.08184138  -345.55523235] ;\n",
      "cumulative reward =  -440995.01134383504\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-114-a5b94f23a512>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0magent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtest_agents\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_episode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_interval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_iteration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_sample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction_space\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdebug\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Documents/GT20F/7000/GNN_experiments/utils/agents.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(agent, env, num_episode, test_interval, num_test, num_iteration, BATCH_SIZE, num_sample, action_space, debug, memory)\u001b[0m\n\u001b[1;32m    242\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m                 action = agent.select_action(state[i], **{\n\u001b[0;32m--> 244\u001b[0;31m                     \u001b[0;34m'steps_done'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'num_sample'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'action_space'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0maction_space\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    245\u001b[0m                 })\n\u001b[1;32m    246\u001b[0m                 \u001b[0mactions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/GT20F/7000/GNN_experiments/utils/agents.py\u001b[0m in \u001b[0;36mselect_action\u001b[0;34m(self, state, **kwargs)\u001b[0m\n\u001b[1;32m    157\u001b[0m                     \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_sample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mna\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'float32'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m                 ) * (action_space[1]-action_space[0])+action_space[0]\n\u001b[0;32m--> 159\u001b[0;31m             \u001b[0mrewards\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_sample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    160\u001b[0m \u001b[0;31m#             print(rewards.shape, rewards.max(0))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m             \u001b[0mbestind\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/GT20F/7000/GNN_experiments/utils/networks.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, y)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;31m#         x = torch.relu(self.fc1( torch.stack( (x, y), dim=1 ).squeeze()))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc1\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0;31m# x = torch.sigmoid(x) # Range = [0,1]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    545\u001b[0m                     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m                 \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 547\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Test run\n",
    "for agent in test_agents:\n",
    "    train(agent, env, num_episode, test_interval, num_test, num_iteration, BATCH_SIZE, num_sample, action_space, debug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test result at episode  0\n",
      "Took  99  steps to converge\n",
      "Finished episode  0  with  99 ;\n",
      "cumulative reward =  -4314.68178065254\n",
      "Finished episode  1  with  199 ;\n",
      "cumulative reward =  -22076.033485254622\n",
      "Took  132  steps to converge\n",
      "Finished episode  2  with  132 ;\n",
      "cumulative reward =  -14837.78344482907\n",
      "Took  99  steps to converge\n",
      "Finished episode  3  with  99 ;\n",
      "cumulative reward =  -8539.266339615975\n",
      "Took  153  steps to converge\n",
      "Finished episode  4  with  153 ;\n",
      "cumulative reward =  -12715.55367379755\n",
      "Finished episode  5  with  199 ;\n",
      "cumulative reward =  -17448.84475412078\n",
      "Took  186  steps to converge\n",
      "Finished episode  6  with  186 ;\n",
      "cumulative reward =  -11381.30331411782\n",
      "Took  109  steps to converge\n",
      "Finished episode  7  with  109 ;\n",
      "cumulative reward =  -10375.658744607448\n",
      "Took  186  steps to converge\n",
      "Finished episode  8  with  186 ;\n",
      "cumulative reward =  -14858.191163654928\n",
      "Took  79  steps to converge\n",
      "Finished episode  9  with  79 ;\n",
      "cumulative reward =  -5326.252007568048\n",
      "Took  134  steps to converge\n",
      "Finished episode  10  with  134 ;\n",
      "cumulative reward =  -9137.226706920923\n",
      "Took  102  steps to converge\n",
      "Finished episode  11  with  102 ;\n",
      "cumulative reward =  -8470.847065940232\n",
      "Took  95  steps to converge\n",
      "Finished episode  12  with  95 ;\n",
      "cumulative reward =  -9385.717391531707\n",
      "Took  81  steps to converge\n",
      "Finished episode  13  with  81 ;\n",
      "cumulative reward =  -7685.545046568412\n",
      "Took  176  steps to converge\n",
      "Finished episode  14  with  176 ;\n",
      "cumulative reward =  -20525.50569984319\n",
      "Took  156  steps to converge\n",
      "Finished episode  15  with  156 ;\n",
      "cumulative reward =  -7657.819796562442\n",
      "Took  73  steps to converge\n",
      "Finished episode  16  with  73 ;\n",
      "cumulative reward =  -8818.791983891284\n",
      "Took  180  steps to converge\n",
      "Finished episode  17  with  180 ;\n",
      "cumulative reward =  -13722.08188077296\n",
      "Took  86  steps to converge\n",
      "Finished episode  18  with  86 ;\n",
      "cumulative reward =  -11413.034344253198\n",
      "Took  75  steps to converge\n",
      "Finished episode  19  with  75 ;\n",
      "cumulative reward =  -11862.624118271906\n",
      "Test result at episode  50\n",
      "Took  79  steps to converge\n",
      "Finished episode  0  with  79 ;\n",
      "cumulative reward =  -3832.364581809242\n",
      "Took  84  steps to converge\n",
      "Finished episode  1  with  84 ;\n",
      "cumulative reward =  -9992.897070367251\n",
      "Took  114  steps to converge\n",
      "Finished episode  2  with  114 ;\n",
      "cumulative reward =  -8684.05229954565\n",
      "Took  132  steps to converge\n",
      "Finished episode  3  with  132 ;\n",
      "cumulative reward =  -7431.411422364173\n",
      "Took  85  steps to converge\n",
      "Finished episode  4  with  85 ;\n",
      "cumulative reward =  -6795.517183106983\n",
      "Took  77  steps to converge\n",
      "Finished episode  5  with  77 ;\n",
      "cumulative reward =  -5236.922945616786\n",
      "Took  91  steps to converge\n",
      "Finished episode  6  with  91 ;\n",
      "cumulative reward =  -9160.359257489687\n",
      "Took  32  steps to converge\n",
      "Finished episode  7  with  32 ;\n",
      "cumulative reward =  -722.842751648596\n",
      "Took  126  steps to converge\n",
      "Finished episode  8  with  126 ;\n",
      "cumulative reward =  -12782.442205442765\n",
      "Took  42  steps to converge\n",
      "Finished episode  9  with  42 ;\n",
      "cumulative reward =  -1308.0344672772985\n",
      "Took  66  steps to converge\n",
      "Finished episode  10  with  66 ;\n",
      "cumulative reward =  -2279.6012614723327\n",
      "Took  115  steps to converge\n",
      "Finished episode  11  with  115 ;\n",
      "cumulative reward =  -13982.708839411613\n",
      "Took  108  steps to converge\n",
      "Finished episode  12  with  108 ;\n",
      "cumulative reward =  -10673.709391829756\n",
      "Took  82  steps to converge\n",
      "Finished episode  13  with  82 ;\n",
      "cumulative reward =  -6182.248421674413\n",
      "Took  51  steps to converge\n",
      "Finished episode  14  with  51 ;\n",
      "cumulative reward =  -2937.353193650303\n",
      "Took  142  steps to converge\n",
      "Finished episode  15  with  142 ;\n",
      "cumulative reward =  -12459.00354733638\n",
      "Took  126  steps to converge\n",
      "Finished episode  16  with  126 ;\n",
      "cumulative reward =  -6792.344335294191\n",
      "Took  114  steps to converge\n",
      "Finished episode  17  with  114 ;\n",
      "cumulative reward =  -8110.248422463976\n",
      "Took  107  steps to converge\n",
      "Finished episode  18  with  107 ;\n",
      "cumulative reward =  -7269.148249061103\n",
      "Took  197  steps to converge\n",
      "Finished episode  19  with  197 ;\n",
      "cumulative reward =  -18329.348062971803\n",
      "Test result at episode  100\n",
      "Took  82  steps to converge\n",
      "Finished episode  0  with  82 ;\n",
      "cumulative reward =  -4928.772556637939\n",
      "Finished episode  1  with  199 ;\n",
      "cumulative reward =  -15580.205715111668\n",
      "Took  41  steps to converge\n",
      "Finished episode  2  with  41 ;\n",
      "cumulative reward =  -899.3522260258401\n",
      "Took  147  steps to converge\n",
      "Finished episode  3  with  147 ;\n",
      "cumulative reward =  -13828.27896968971\n",
      "Took  102  steps to converge\n",
      "Finished episode  4  with  102 ;\n",
      "cumulative reward =  -8262.683664493166\n",
      "Took  150  steps to converge\n",
      "Finished episode  5  with  150 ;\n",
      "cumulative reward =  -10275.445740342595\n",
      "Took  87  steps to converge\n",
      "Finished episode  6  with  87 ;\n",
      "cumulative reward =  -9136.954241200958\n",
      "Took  127  steps to converge\n",
      "Finished episode  7  with  127 ;\n",
      "cumulative reward =  -9388.316451562774\n",
      "Took  70  steps to converge\n",
      "Finished episode  8  with  70 ;\n",
      "cumulative reward =  -2643.254201551027\n",
      "Took  100  steps to converge\n",
      "Finished episode  9  with  100 ;\n",
      "cumulative reward =  -6727.005293926971\n",
      "Took  128  steps to converge\n",
      "Finished episode  10  with  128 ;\n",
      "cumulative reward =  -6006.886194455219\n",
      "Took  142  steps to converge\n",
      "Finished episode  11  with  142 ;\n",
      "cumulative reward =  -6291.563455850149\n",
      "Took  140  steps to converge\n",
      "Finished episode  12  with  140 ;\n",
      "cumulative reward =  -11706.052525211133\n",
      "Took  53  steps to converge\n",
      "Finished episode  13  with  53 ;\n",
      "cumulative reward =  -2541.809542532529\n",
      "Took  94  steps to converge\n",
      "Finished episode  14  with  94 ;\n",
      "cumulative reward =  -4952.287272241025\n",
      "Took  100  steps to converge\n",
      "Finished episode  15  with  100 ;\n",
      "cumulative reward =  -5061.2863451505855\n",
      "Took  74  steps to converge\n",
      "Finished episode  16  with  74 ;\n",
      "cumulative reward =  -4311.35868825089\n",
      "Took  126  steps to converge\n",
      "Finished episode  17  with  126 ;\n",
      "cumulative reward =  -17210.22724150284\n",
      "Took  63  steps to converge\n",
      "Finished episode  18  with  63 ;\n",
      "cumulative reward =  -2577.6821932732178\n",
      "Took  94  steps to converge\n",
      "Finished episode  19  with  94 ;\n",
      "cumulative reward =  -4781.180303683586\n",
      "Test result at episode  150\n",
      "Finished episode  0  with  199 ;\n",
      "cumulative reward =  -11381.190359871905\n",
      "Took  160  steps to converge\n",
      "Finished episode  1  with  160 ;\n",
      "cumulative reward =  -18862.131099516013\n",
      "Finished episode  2  with  199 ;\n",
      "cumulative reward =  -9679.965687274203\n",
      "Took  195  steps to converge\n",
      "Finished episode  3  with  195 ;\n",
      "cumulative reward =  -13783.494298200338\n",
      "Took  160  steps to converge\n",
      "Finished episode  4  with  160 ;\n",
      "cumulative reward =  -13657.702136400507\n",
      "Took  52  steps to converge\n",
      "Finished episode  5  with  52 ;\n",
      "cumulative reward =  -898.1153143593651\n",
      "Finished episode  6  with  199 ;\n",
      "cumulative reward =  -17289.553848800202\n",
      "Finished episode  7  with  199 ;\n",
      "cumulative reward =  -21979.3894692778\n",
      "Took  93  steps to converge\n",
      "Finished episode  8  with  93 ;\n",
      "cumulative reward =  -5080.120323947108\n",
      "Took  52  steps to converge\n",
      "Finished episode  9  with  52 ;\n",
      "cumulative reward =  -925.7086469462552\n",
      "Finished episode  10  with  199 ;\n",
      "cumulative reward =  -13154.222931199214\n",
      "Took  63  steps to converge\n",
      "Finished episode  11  with  63 ;\n",
      "cumulative reward =  -4196.136402398186\n",
      "Took  143  steps to converge\n",
      "Finished episode  12  with  143 ;\n",
      "cumulative reward =  -13190.227880453036\n",
      "Finished episode  13  with  199 ;\n",
      "cumulative reward =  -16549.86128966392\n",
      "Finished episode  14  with  199 ;\n",
      "cumulative reward =  -14644.116451140855\n",
      "Finished episode  15  with  199 ;\n",
      "cumulative reward =  -16276.952426610342\n",
      "Finished episode  16  with  199 ;\n",
      "cumulative reward =  -23350.46019671205\n",
      "Took  199  steps to converge\n",
      "Finished episode  17  with  199 ;\n",
      "cumulative reward =  -17567.901125793895\n",
      "Finished episode  18  with  199 ;\n",
      "cumulative reward =  -13361.304095373316\n",
      "Took  72  steps to converge\n",
      "Finished episode  19  with  72 ;\n",
      "cumulative reward =  -3467.4186270299065\n",
      "Test result at episode  200\n",
      "Took  188  steps to converge\n",
      "Finished episode  0  with  188 ;\n",
      "cumulative reward =  -20519.265506746586\n",
      "Finished episode  1  with  199 ;\n",
      "cumulative reward =  -29053.559747212847\n",
      "Took  130  steps to converge\n",
      "Finished episode  2  with  130 ;\n",
      "cumulative reward =  -13389.58826270516\n",
      "Took  32  steps to converge\n",
      "Finished episode  3  with  32 ;\n",
      "cumulative reward =  -790.7134494519963\n",
      "Took  125  steps to converge\n",
      "Finished episode  4  with  125 ;\n",
      "cumulative reward =  -11175.612793915761\n",
      "Took  93  steps to converge\n",
      "Finished episode  5  with  93 ;\n",
      "cumulative reward =  -12261.243938980586\n",
      "Finished episode  6  with  199 ;\n",
      "cumulative reward =  -26028.30085721742\n",
      "Finished episode  7  with  199 ;\n",
      "cumulative reward =  -26309.832128141796\n",
      "Took  186  steps to converge\n",
      "Finished episode  8  with  186 ;\n",
      "cumulative reward =  -18661.756286540607\n",
      "Took  67  steps to converge\n",
      "Finished episode  9  with  67 ;\n",
      "cumulative reward =  -3165.5090616014004\n",
      "Took  138  steps to converge\n",
      "Finished episode  10  with  138 ;\n",
      "cumulative reward =  -13107.128146681729\n",
      "Took  152  steps to converge\n",
      "Finished episode  11  with  152 ;\n",
      "cumulative reward =  -17417.9563162993\n",
      "Took  28  steps to converge\n",
      "Finished episode  12  with  28 ;\n",
      "cumulative reward =  -714.5978874373106\n",
      "Took  150  steps to converge\n",
      "Finished episode  13  with  150 ;\n",
      "cumulative reward =  -10325.552388434313\n",
      "Took  29  steps to converge\n",
      "Finished episode  14  with  29 ;\n",
      "cumulative reward =  -571.2366083455788\n",
      "Took  109  steps to converge\n",
      "Finished episode  15  with  109 ;\n",
      "cumulative reward =  -11090.012257903245\n",
      "Took  24  steps to converge\n",
      "Finished episode  16  with  24 ;\n",
      "cumulative reward =  -538.2077083608432\n",
      "Took  32  steps to converge\n",
      "Finished episode  17  with  32 ;\n",
      "cumulative reward =  -916.1251354270815\n",
      "Took  199  steps to converge\n",
      "Finished episode  18  with  199 ;\n",
      "cumulative reward =  -19630.801489387253\n",
      "Took  72  steps to converge\n",
      "Finished episode  19  with  72 ;\n",
      "cumulative reward =  -7961.817242713133\n",
      "Test result at episode  250\n",
      "Took  62  steps to converge\n",
      "Finished episode  0  with  62 ;\n",
      "cumulative reward =  -1880.1783972054773\n",
      "Took  89  steps to converge\n",
      "Finished episode  1  with  89 ;\n",
      "cumulative reward =  -5339.2831759017745\n",
      "Took  116  steps to converge\n",
      "Finished episode  2  with  116 ;\n",
      "cumulative reward =  -9504.461310671075\n",
      "Took  95  steps to converge\n",
      "Finished episode  3  with  95 ;\n",
      "cumulative reward =  -10840.653514712434\n",
      "Finished episode  4  with  199 ;\n",
      "cumulative reward =  -22637.754981211718\n",
      "Finished episode  5  with  199 ;\n",
      "cumulative reward =  -27183.517391885896\n",
      "Took  37  steps to converge\n",
      "Finished episode  6  with  37 ;\n",
      "cumulative reward =  -688.4474096190506\n",
      "Took  118  steps to converge\n",
      "Finished episode  7  with  118 ;\n",
      "cumulative reward =  -10191.281262146324\n",
      "Took  72  steps to converge\n",
      "Finished episode  8  with  72 ;\n",
      "cumulative reward =  -3710.3090126967904\n",
      "Finished episode  9  with  199 ;\n",
      "cumulative reward =  -27176.565431970008\n",
      "Took  92  steps to converge\n",
      "Finished episode  10  with  92 ;\n",
      "cumulative reward =  -4198.283006171679\n",
      "Took  22  steps to converge\n",
      "Finished episode  11  with  22 ;\n",
      "cumulative reward =  -385.49668481584615\n",
      "Took  78  steps to converge\n",
      "Finished episode  12  with  78 ;\n",
      "cumulative reward =  -3778.9876634266725\n",
      "Took  95  steps to converge\n",
      "Finished episode  13  with  95 ;\n",
      "cumulative reward =  -5538.823440903788\n",
      "Took  101  steps to converge\n",
      "Finished episode  14  with  101 ;\n",
      "cumulative reward =  -4927.509438105309\n",
      "Took  130  steps to converge\n",
      "Finished episode  15  with  130 ;\n",
      "cumulative reward =  -10111.207286537407\n",
      "Took  86  steps to converge\n",
      "Finished episode  16  with  86 ;\n",
      "cumulative reward =  -4165.52868376989\n",
      "Took  142  steps to converge\n",
      "Finished episode  17  with  142 ;\n",
      "cumulative reward =  -15969.170555799099\n",
      "Took  107  steps to converge\n",
      "Finished episode  18  with  107 ;\n",
      "cumulative reward =  -8434.982741714033\n",
      "Finished episode  19  with  199 ;\n",
      "cumulative reward =  -14577.368711156709\n",
      "Test result at episode  300\n",
      "Took  74  steps to converge\n",
      "Finished episode  0  with  74 ;\n",
      "cumulative reward =  -3387.5965142943246\n",
      "Finished episode  1  with  199 ;\n",
      "cumulative reward =  -26175.194048628244\n",
      "Took  46  steps to converge\n",
      "Finished episode  2  with  46 ;\n",
      "cumulative reward =  -773.9159310802809\n",
      "Finished episode  3  with  199 ;\n",
      "cumulative reward =  -21884.452639219715\n",
      "Finished episode  4  with  199 ;\n",
      "cumulative reward =  -20970.465821304555\n",
      "Took  26  steps to converge\n",
      "Finished episode  5  with  26 ;\n",
      "cumulative reward =  -485.7361261098236\n",
      "Finished episode  6  with  199 ;\n",
      "cumulative reward =  -16022.703717255397\n",
      "Finished episode  7  with  199 ;\n",
      "cumulative reward =  -30026.104285429778\n",
      "Finished episode  8  with  199 ;\n",
      "cumulative reward =  -19003.181849173747\n",
      "Took  58  steps to converge\n",
      "Finished episode  9  with  58 ;\n",
      "cumulative reward =  -1746.9392678951926\n",
      "Finished episode  10  with  199 ;\n",
      "cumulative reward =  -16770.110764513658\n",
      "Finished episode  11  with  199 ;\n",
      "cumulative reward =  -10935.192996501315\n",
      "Finished episode  12  with  199 ;\n",
      "cumulative reward =  -16591.338074033454\n",
      "Finished episode  13  with  199 ;\n",
      "cumulative reward =  -21934.40045093512\n",
      "Took  183  steps to converge\n",
      "Finished episode  14  with  183 ;\n",
      "cumulative reward =  -17111.82628663854\n",
      "Took  199  steps to converge\n",
      "Finished episode  15  with  199 ;\n",
      "cumulative reward =  -20359.48227210479\n",
      "Finished episode  16  with  199 ;\n",
      "cumulative reward =  -19771.703806407622\n",
      "Took  25  steps to converge\n",
      "Finished episode  17  with  25 ;\n",
      "cumulative reward =  -471.91574078839426\n",
      "Took  41  steps to converge\n",
      "Finished episode  18  with  41 ;\n",
      "cumulative reward =  -595.7344020872303\n",
      "Finished episode  19  with  199 ;\n",
      "cumulative reward =  -17735.77994188262\n",
      "Test result at episode  350\n",
      "Took  146  steps to converge\n",
      "Finished episode  0  with  146 ;\n",
      "cumulative reward =  -9742.543420237203\n",
      "Took  106  steps to converge\n",
      "Finished episode  1  with  106 ;\n",
      "cumulative reward =  -4429.424308613721\n",
      "Took  85  steps to converge\n",
      "Finished episode  2  with  85 ;\n",
      "cumulative reward =  -8204.540505933683\n",
      "Took  60  steps to converge\n",
      "Finished episode  3  with  60 ;\n",
      "cumulative reward =  -2695.0687551241213\n",
      "Took  42  steps to converge\n",
      "Finished episode  4  with  42 ;\n",
      "cumulative reward =  -822.973886589964\n",
      "Finished episode  5  with  199 ;\n",
      "cumulative reward =  -27421.46653335905\n",
      "Took  199  steps to converge\n",
      "Finished episode  6  with  199 ;\n",
      "cumulative reward =  -11523.38133006674\n",
      "Took  40  steps to converge\n",
      "Finished episode  7  with  40 ;\n",
      "cumulative reward =  -937.5933945569329\n",
      "Took  70  steps to converge\n",
      "Finished episode  8  with  70 ;\n",
      "cumulative reward =  -3823.718853832095\n",
      "Took  105  steps to converge\n",
      "Finished episode  9  with  105 ;\n",
      "cumulative reward =  -4476.172830369202\n",
      "Took  28  steps to converge\n",
      "Finished episode  10  with  28 ;\n",
      "cumulative reward =  -353.6560899976741\n",
      "Took  76  steps to converge\n",
      "Finished episode  11  with  76 ;\n",
      "cumulative reward =  -6679.927065953855\n",
      "Took  109  steps to converge\n",
      "Finished episode  12  with  109 ;\n",
      "cumulative reward =  -6428.037437896109\n",
      "Took  87  steps to converge\n",
      "Finished episode  13  with  87 ;\n",
      "cumulative reward =  -7121.360555540134\n",
      "Finished episode  14  with  199 ;\n",
      "cumulative reward =  -20292.60905764306\n",
      "Took  153  steps to converge\n",
      "Finished episode  15  with  153 ;\n",
      "cumulative reward =  -5620.143525826975\n",
      "Took  94  steps to converge\n",
      "Finished episode  16  with  94 ;\n",
      "cumulative reward =  -4969.729690155647\n",
      "Finished episode  17  with  199 ;\n",
      "cumulative reward =  -29682.10276606632\n",
      "Finished episode  18  with  199 ;\n",
      "cumulative reward =  -23069.906021365554\n",
      "Took  93  steps to converge\n",
      "Finished episode  19  with  93 ;\n",
      "cumulative reward =  -4317.909297756299\n",
      "Test result at episode  400\n",
      "Took  95  steps to converge\n",
      "Finished episode  0  with  95 ;\n",
      "cumulative reward =  -9850.077790655932\n",
      "Took  157  steps to converge\n",
      "Finished episode  1  with  157 ;\n",
      "cumulative reward =  -17872.951272945305\n",
      "Took  69  steps to converge\n",
      "Finished episode  2  with  69 ;\n",
      "cumulative reward =  -9244.759032392898\n",
      "Took  89  steps to converge\n",
      "Finished episode  3  with  89 ;\n",
      "cumulative reward =  -10047.946913165913\n",
      "Took  156  steps to converge\n",
      "Finished episode  4  with  156 ;\n",
      "cumulative reward =  -20186.753227705885\n",
      "Took  26  steps to converge\n",
      "Finished episode  5  with  26 ;\n",
      "cumulative reward =  -527.5825440269876\n",
      "Finished episode  6  with  199 ;\n",
      "cumulative reward =  -23802.214888920986\n",
      "Finished episode  7  with  199 ;\n",
      "cumulative reward =  -27355.660295222362\n",
      "Took  90  steps to converge\n",
      "Finished episode  8  with  90 ;\n",
      "cumulative reward =  -4719.57692499612\n",
      "Took  65  steps to converge\n",
      "Finished episode  9  with  65 ;\n",
      "cumulative reward =  -3756.5776571590472\n",
      "Finished episode  10  with  199 ;\n",
      "cumulative reward =  -34247.65412696828\n",
      "Finished episode  11  with  199 ;\n",
      "cumulative reward =  -24077.92036624001\n",
      "Finished episode  12  with  199 ;\n",
      "cumulative reward =  -22191.084287260277\n",
      "Took  184  steps to converge\n",
      "Finished episode  13  with  184 ;\n",
      "cumulative reward =  -14554.134748454113\n",
      "Took  159  steps to converge\n",
      "Finished episode  14  with  159 ;\n",
      "cumulative reward =  -17560.71029959333\n",
      "Took  40  steps to converge\n",
      "Finished episode  15  with  40 ;\n",
      "cumulative reward =  -919.4735005675248\n",
      "Finished episode  16  with  199 ;\n",
      "cumulative reward =  -16363.206111020321\n",
      "Took  150  steps to converge\n",
      "Finished episode  17  with  150 ;\n",
      "cumulative reward =  -14262.267853129812\n",
      "Finished episode  18  with  199 ;\n",
      "cumulative reward =  -16878.068657142416\n",
      "Took  128  steps to converge\n",
      "Finished episode  19  with  128 ;\n",
      "cumulative reward =  -10202.0784718449\n",
      "Test result at episode  450\n",
      "Took  108  steps to converge\n",
      "Finished episode  0  with  108 ;\n",
      "cumulative reward =  -9754.146842854241\n",
      "Took  72  steps to converge\n",
      "Finished episode  1  with  72 ;\n",
      "cumulative reward =  -4277.936430206649\n",
      "Took  71  steps to converge\n",
      "Finished episode  2  with  71 ;\n",
      "cumulative reward =  -5271.442819203769\n",
      "Took  106  steps to converge\n",
      "Finished episode  3  with  106 ;\n",
      "cumulative reward =  -12356.54558598774\n",
      "Took  77  steps to converge\n",
      "Finished episode  4  with  77 ;\n",
      "cumulative reward =  -8157.925654380498\n",
      "Took  69  steps to converge\n",
      "Finished episode  5  with  69 ;\n",
      "cumulative reward =  -4623.330693672842\n",
      "Took  61  steps to converge\n",
      "Finished episode  6  with  61 ;\n",
      "cumulative reward =  -4504.328618178052\n",
      "Took  92  steps to converge\n",
      "Finished episode  7  with  92 ;\n",
      "cumulative reward =  -8901.0618715831\n",
      "Took  62  steps to converge\n",
      "Finished episode  8  with  62 ;\n",
      "cumulative reward =  -8654.560887707388\n",
      "Took  68  steps to converge\n",
      "Finished episode  9  with  68 ;\n",
      "cumulative reward =  -3916.0386565943413\n",
      "Took  38  steps to converge\n",
      "Finished episode  10  with  38 ;\n",
      "cumulative reward =  -792.193339712105\n",
      "Took  96  steps to converge\n",
      "Finished episode  11  with  96 ;\n",
      "cumulative reward =  -3927.2476066303484\n",
      "Took  131  steps to converge\n",
      "Finished episode  12  with  131 ;\n",
      "cumulative reward =  -14749.351661877166\n",
      "Took  193  steps to converge\n",
      "Finished episode  13  with  193 ;\n",
      "cumulative reward =  -16956.86705920021\n",
      "Finished episode  14  with  199 ;\n",
      "cumulative reward =  -26398.128311380297\n",
      "Took  114  steps to converge\n",
      "Finished episode  15  with  114 ;\n",
      "cumulative reward =  -12618.08850430314\n",
      "Took  106  steps to converge\n",
      "Finished episode  16  with  106 ;\n",
      "cumulative reward =  -10599.594191170467\n",
      "Took  54  steps to converge\n",
      "Finished episode  17  with  54 ;\n",
      "cumulative reward =  -3030.6687701083274\n",
      "Took  62  steps to converge\n",
      "Finished episode  18  with  62 ;\n",
      "cumulative reward =  -4569.501682052857\n",
      "Took  104  steps to converge\n",
      "Finished episode  19  with  104 ;\n",
      "cumulative reward =  -11498.134300397764\n"
     ]
    }
   ],
   "source": [
    "# Real run\n",
    "num_episode=500\n",
    "test_interval=50\n",
    "num_test=20\n",
    "num_iteration=200\n",
    "BATCH_SIZE=128\n",
    "debug=False\n",
    "# num_sample=50\n",
    "\n",
    "agent1 = LearnerAgent(device, N, ns, na, hidden)\n",
    "a1hist = train(agent1, env, num_episode, test_interval, num_test, num_iteration, BATCH_SIZE, num_sample, action_space, debug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test result at episode  0\n",
      "Finished episode  0  with  199 ;\n",
      "cumulative reward =  -16338.826646201742\n",
      "Finished episode  1  with  199 ;\n",
      "cumulative reward =  -12455.095716841332\n",
      "Finished episode  2  with  199 ;\n",
      "cumulative reward =  -9839.494291086286\n",
      "Took  165  steps to converge\n",
      "Finished episode  3  with  165 ;\n",
      "cumulative reward =  -11156.634993743024\n",
      "Finished episode  4  with  199 ;\n",
      "cumulative reward =  -28193.348964943518\n",
      "Took  35  steps to converge\n",
      "Finished episode  5  with  35 ;\n",
      "cumulative reward =  -679.8317379085505\n",
      "Finished episode  6  with  199 ;\n",
      "cumulative reward =  -10007.597662945722\n",
      "Took  169  steps to converge\n",
      "Finished episode  7  with  169 ;\n",
      "cumulative reward =  -12565.321048305836\n",
      "Finished episode  8  with  199 ;\n",
      "cumulative reward =  -10251.479711453601\n",
      "Took  57  steps to converge\n",
      "Finished episode  9  with  57 ;\n",
      "cumulative reward =  -2711.9868151024993\n",
      "Finished episode  10  with  199 ;\n",
      "cumulative reward =  -10550.53075574147\n",
      "Took  169  steps to converge\n",
      "Finished episode  11  with  169 ;\n",
      "cumulative reward =  -16901.399170006025\n",
      "Took  117  steps to converge\n",
      "Finished episode  12  with  117 ;\n",
      "cumulative reward =  -6167.0841509878355\n",
      "Took  56  steps to converge\n",
      "Finished episode  13  with  56 ;\n",
      "cumulative reward =  -2037.5023546656691\n",
      "Took  41  steps to converge\n",
      "Finished episode  14  with  41 ;\n",
      "cumulative reward =  -1564.659350794507\n",
      "Finished episode  15  with  199 ;\n",
      "cumulative reward =  -29181.41783861702\n",
      "Took  44  steps to converge\n",
      "Finished episode  16  with  44 ;\n",
      "cumulative reward =  -1849.1109637680593\n",
      "Took  32  steps to converge\n",
      "Finished episode  17  with  32 ;\n",
      "cumulative reward =  -574.3132421282479\n",
      "Finished episode  18  with  199 ;\n",
      "cumulative reward =  -9384.2711546131\n",
      "Took  41  steps to converge\n",
      "Finished episode  19  with  41 ;\n",
      "cumulative reward =  -1679.9525576778153\n",
      "Test result at episode  50\n",
      "Took  77  steps to converge\n",
      "Finished episode  0  with  77 ;\n",
      "cumulative reward =  -5331.922514632553\n",
      "Finished episode  1  with  199 ;\n",
      "cumulative reward =  -24312.89418061944\n",
      "Took  25  steps to converge\n",
      "Finished episode  2  with  25 ;\n",
      "cumulative reward =  -463.8998291146276\n",
      "Finished episode  3  with  199 ;\n",
      "cumulative reward =  -18456.748700188782\n",
      "Finished episode  4  with  199 ;\n",
      "cumulative reward =  -16448.303745229063\n",
      "Finished episode  5  with  199 ;\n",
      "cumulative reward =  -25587.59586342388\n",
      "Finished episode  6  with  199 ;\n",
      "cumulative reward =  -9674.728745900871\n",
      "Took  77  steps to converge\n",
      "Finished episode  7  with  77 ;\n",
      "cumulative reward =  -3608.997341793312\n",
      "Took  111  steps to converge\n",
      "Finished episode  8  with  111 ;\n",
      "cumulative reward =  -7372.819020207316\n",
      "Finished episode  9  with  199 ;\n",
      "cumulative reward =  -8671.07716612133\n",
      "Took  29  steps to converge\n",
      "Finished episode  10  with  29 ;\n",
      "cumulative reward =  -422.7815910771807\n",
      "Finished episode  11  with  199 ;\n",
      "cumulative reward =  -9716.002059088849\n",
      "Took  61  steps to converge\n",
      "Finished episode  12  with  61 ;\n",
      "cumulative reward =  -2515.764593895486\n",
      "Finished episode  13  with  199 ;\n",
      "cumulative reward =  -24645.07956098426\n",
      "Finished episode  14  with  199 ;\n",
      "cumulative reward =  -13658.789674333195\n",
      "Finished episode  15  with  199 ;\n",
      "cumulative reward =  -24138.424469035876\n",
      "Took  145  steps to converge\n",
      "Finished episode  16  with  145 ;\n",
      "cumulative reward =  -9409.311100157058\n",
      "Finished episode  17  with  199 ;\n",
      "cumulative reward =  -18449.727572978445\n",
      "Finished episode  18  with  199 ;\n",
      "cumulative reward =  -26453.720176060724\n",
      "Finished episode  19  with  199 ;\n",
      "cumulative reward =  -25723.89635474146\n",
      "Test result at episode  100\n",
      "Took  129  steps to converge\n",
      "Finished episode  0  with  129 ;\n",
      "cumulative reward =  -10443.626378938498\n",
      "Took  159  steps to converge\n",
      "Finished episode  1  with  159 ;\n",
      "cumulative reward =  -7401.822781764135\n",
      "Took  144  steps to converge\n",
      "Finished episode  2  with  144 ;\n",
      "cumulative reward =  -5597.502528332464\n",
      "Took  85  steps to converge\n",
      "Finished episode  3  with  85 ;\n",
      "cumulative reward =  -3049.934152902942\n",
      "Took  150  steps to converge\n",
      "Finished episode  4  with  150 ;\n",
      "cumulative reward =  -11797.317410898671\n",
      "Took  147  steps to converge\n",
      "Finished episode  5  with  147 ;\n",
      "cumulative reward =  -6889.707456991158\n",
      "Took  124  steps to converge\n",
      "Finished episode  6  with  124 ;\n",
      "cumulative reward =  -14828.610608308441\n",
      "Finished episode  7  with  199 ;\n",
      "cumulative reward =  -20043.236833355575\n",
      "Took  147  steps to converge\n",
      "Finished episode  8  with  147 ;\n",
      "cumulative reward =  -8000.203817942244\n",
      "Took  189  steps to converge\n",
      "Finished episode  9  with  189 ;\n",
      "cumulative reward =  -11977.31778713079\n",
      "Took  136  steps to converge\n",
      "Finished episode  10  with  136 ;\n",
      "cumulative reward =  -7480.121332164857\n",
      "Took  197  steps to converge\n",
      "Finished episode  11  with  197 ;\n",
      "cumulative reward =  -16680.63946562821\n",
      "Finished episode  12  with  199 ;\n",
      "cumulative reward =  -17394.14740658205\n",
      "Took  40  steps to converge\n",
      "Finished episode  13  with  40 ;\n",
      "cumulative reward =  -1009.1872947730077\n",
      "Took  111  steps to converge\n",
      "Finished episode  14  with  111 ;\n",
      "cumulative reward =  -12515.764537263642\n",
      "Finished episode  15  with  199 ;\n",
      "cumulative reward =  -18212.583220762568\n",
      "Took  84  steps to converge\n",
      "Finished episode  16  with  84 ;\n",
      "cumulative reward =  -4546.512858678093\n",
      "Finished episode  17  with  199 ;\n",
      "cumulative reward =  -10824.94350114317\n",
      "Took  75  steps to converge\n",
      "Finished episode  18  with  75 ;\n",
      "cumulative reward =  -4129.573885705522\n",
      "Took  111  steps to converge\n",
      "Finished episode  19  with  111 ;\n",
      "cumulative reward =  -7435.909741152869\n",
      "Test result at episode  150\n",
      "Finished episode  0  with  199 ;\n",
      "cumulative reward =  -19454.331353718288\n",
      "Finished episode  1  with  199 ;\n",
      "cumulative reward =  -14093.368567563177\n",
      "Took  30  steps to converge\n",
      "Finished episode  2  with  30 ;\n",
      "cumulative reward =  -574.0120603763385\n",
      "Finished episode  3  with  199 ;\n",
      "cumulative reward =  -18251.86023992923\n",
      "Finished episode  4  with  199 ;\n",
      "cumulative reward =  -17099.88629873998\n",
      "Finished episode  5  with  199 ;\n",
      "cumulative reward =  -24879.182026174087\n",
      "Finished episode  6  with  199 ;\n",
      "cumulative reward =  -19982.252577767784\n",
      "Finished episode  7  with  199 ;\n",
      "cumulative reward =  -30894.4418449335\n",
      "Finished episode  8  with  199 ;\n",
      "cumulative reward =  -22977.688495847557\n",
      "Finished episode  9  with  199 ;\n",
      "cumulative reward =  -18128.26106878956\n",
      "Finished episode  10  with  199 ;\n",
      "cumulative reward =  -17978.85585659792\n",
      "Finished episode  11  with  199 ;\n",
      "cumulative reward =  -16063.184903226147\n",
      "Took  59  steps to converge\n",
      "Finished episode  12  with  59 ;\n",
      "cumulative reward =  -2602.4663077460445\n",
      "Finished episode  13  with  199 ;\n",
      "cumulative reward =  -16418.317468767083\n",
      "Took  84  steps to converge\n",
      "Finished episode  14  with  84 ;\n",
      "cumulative reward =  -2943.690850323869\n",
      "Finished episode  15  with  199 ;\n",
      "cumulative reward =  -13585.087102110849\n",
      "Finished episode  16  with  199 ;\n",
      "cumulative reward =  -29015.57261868429\n",
      "Finished episode  17  with  199 ;\n",
      "cumulative reward =  -20822.615391850948\n",
      "Took  31  steps to converge\n",
      "Finished episode  18  with  31 ;\n",
      "cumulative reward =  -363.126574994698\n",
      "Took  114  steps to converge\n",
      "Finished episode  19  with  114 ;\n",
      "cumulative reward =  -10977.053739616587\n",
      "Test result at episode  200\n",
      "Took  66  steps to converge\n",
      "Finished episode  0  with  66 ;\n",
      "cumulative reward =  -2135.6275090197655\n",
      "Finished episode  1  with  199 ;\n",
      "cumulative reward =  -32661.849998806243\n",
      "Finished episode  2  with  199 ;\n",
      "cumulative reward =  -29295.81086418289\n",
      "Took  92  steps to converge\n",
      "Finished episode  3  with  92 ;\n",
      "cumulative reward =  -4917.869195416325\n",
      "Finished episode  4  with  199 ;\n",
      "cumulative reward =  -14503.131544487422\n",
      "Finished episode  5  with  199 ;\n",
      "cumulative reward =  -32304.082472266367\n",
      "Took  98  steps to converge\n",
      "Finished episode  6  with  98 ;\n",
      "cumulative reward =  -9723.92010614526\n",
      "Finished episode  7  with  199 ;\n",
      "cumulative reward =  -30605.376327728456\n",
      "Finished episode  8  with  199 ;\n",
      "cumulative reward =  -26837.711766719796\n",
      "Finished episode  9  with  199 ;\n",
      "cumulative reward =  -17509.002415551517\n",
      "Finished episode  10  with  199 ;\n",
      "cumulative reward =  -26270.972451798552\n",
      "Took  29  steps to converge\n",
      "Finished episode  11  with  29 ;\n",
      "cumulative reward =  -506.3726064359376\n",
      "Finished episode  12  with  199 ;\n",
      "cumulative reward =  -23927.10420203287\n",
      "Took  27  steps to converge\n",
      "Finished episode  13  with  27 ;\n",
      "cumulative reward =  -487.2676913445484\n",
      "Finished episode  14  with  199 ;\n",
      "cumulative reward =  -17935.961524080696\n",
      "Took  31  steps to converge\n",
      "Finished episode  15  with  31 ;\n",
      "cumulative reward =  -585.8500141640591\n",
      "Took  135  steps to converge\n",
      "Finished episode  16  with  135 ;\n",
      "cumulative reward =  -13978.809837018132\n",
      "Took  93  steps to converge\n",
      "Finished episode  17  with  93 ;\n",
      "cumulative reward =  -6735.854993263222\n",
      "Took  169  steps to converge\n",
      "Finished episode  18  with  169 ;\n",
      "cumulative reward =  -17764.397265385487\n",
      "Finished episode  19  with  199 ;\n",
      "cumulative reward =  -31616.77803522931\n",
      "Test result at episode  250\n",
      "Took  132  steps to converge\n",
      "Finished episode  0  with  132 ;\n",
      "cumulative reward =  -6792.88770010781\n",
      "Took  148  steps to converge\n",
      "Finished episode  1  with  148 ;\n",
      "cumulative reward =  -13786.858798578807\n",
      "Finished episode  2  with  199 ;\n",
      "cumulative reward =  -24321.045788854895\n",
      "Took  102  steps to converge\n",
      "Finished episode  3  with  102 ;\n",
      "cumulative reward =  -8032.585125911558\n",
      "Finished episode  4  with  199 ;\n",
      "cumulative reward =  -34254.26305154822\n",
      "Finished episode  5  with  199 ;\n",
      "cumulative reward =  -30070.860804577453\n",
      "Took  75  steps to converge\n",
      "Finished episode  6  with  75 ;\n",
      "cumulative reward =  -4434.3130314845575\n",
      "Took  49  steps to converge\n",
      "Finished episode  7  with  49 ;\n",
      "cumulative reward =  -2439.491382810357\n",
      "Took  189  steps to converge\n",
      "Finished episode  8  with  189 ;\n",
      "cumulative reward =  -19980.020411683658\n",
      "Took  140  steps to converge\n",
      "Finished episode  9  with  140 ;\n",
      "cumulative reward =  -13460.012078914038\n",
      "Took  100  steps to converge\n",
      "Finished episode  10  with  100 ;\n",
      "cumulative reward =  -5959.236761940537\n",
      "Took  40  steps to converge\n",
      "Finished episode  11  with  40 ;\n",
      "cumulative reward =  -774.6824103851822\n",
      "Finished episode  12  with  199 ;\n",
      "cumulative reward =  -23635.44599610466\n",
      "Took  102  steps to converge\n",
      "Finished episode  13  with  102 ;\n",
      "cumulative reward =  -5930.274071025799\n",
      "Took  129  steps to converge\n",
      "Finished episode  14  with  129 ;\n",
      "cumulative reward =  -10554.023050895592\n",
      "Took  74  steps to converge\n",
      "Finished episode  15  with  74 ;\n",
      "cumulative reward =  -5270.203789097416\n",
      "Took  91  steps to converge\n",
      "Finished episode  16  with  91 ;\n",
      "cumulative reward =  -9766.993699149585\n",
      "Took  142  steps to converge\n",
      "Finished episode  17  with  142 ;\n",
      "cumulative reward =  -10328.577670407418\n",
      "Took  115  steps to converge\n",
      "Finished episode  18  with  115 ;\n",
      "cumulative reward =  -9482.48492767193\n",
      "Took  40  steps to converge\n",
      "Finished episode  19  with  40 ;\n",
      "cumulative reward =  -975.1877093229513\n",
      "Test result at episode  300\n",
      "Took  96  steps to converge\n",
      "Finished episode  0  with  96 ;\n",
      "cumulative reward =  -6370.400645481347\n",
      "Took  119  steps to converge\n",
      "Finished episode  1  with  119 ;\n",
      "cumulative reward =  -13964.065170943266\n",
      "Took  33  steps to converge\n",
      "Finished episode  2  with  33 ;\n",
      "cumulative reward =  -673.0901228705461\n",
      "Took  89  steps to converge\n",
      "Finished episode  3  with  89 ;\n",
      "cumulative reward =  -7001.530036635064\n",
      "Took  112  steps to converge\n",
      "Finished episode  4  with  112 ;\n",
      "cumulative reward =  -6845.833618911964\n",
      "Finished episode  5  with  199 ;\n",
      "cumulative reward =  -13815.961255661996\n",
      "Took  81  steps to converge\n",
      "Finished episode  6  with  81 ;\n",
      "cumulative reward =  -6481.603625754915\n",
      "Took  127  steps to converge\n",
      "Finished episode  7  with  127 ;\n",
      "cumulative reward =  -11925.29795815444\n",
      "Took  76  steps to converge\n",
      "Finished episode  8  with  76 ;\n",
      "cumulative reward =  -5076.306761516548\n",
      "Took  86  steps to converge\n",
      "Finished episode  9  with  86 ;\n",
      "cumulative reward =  -5737.759439233923\n",
      "Took  51  steps to converge\n",
      "Finished episode  10  with  51 ;\n",
      "cumulative reward =  -1872.520731118459\n",
      "Took  31  steps to converge\n",
      "Finished episode  11  with  31 ;\n",
      "cumulative reward =  -1000.4737333699528\n",
      "Took  89  steps to converge\n",
      "Finished episode  12  with  89 ;\n",
      "cumulative reward =  -7204.694932519003\n",
      "Took  165  steps to converge\n",
      "Finished episode  13  with  165 ;\n",
      "cumulative reward =  -11004.535976036861\n",
      "Took  115  steps to converge\n",
      "Finished episode  14  with  115 ;\n",
      "cumulative reward =  -9523.35984935354\n",
      "Took  102  steps to converge\n",
      "Finished episode  15  with  102 ;\n",
      "cumulative reward =  -14248.847973950997\n",
      "Took  28  steps to converge\n",
      "Finished episode  16  with  28 ;\n",
      "cumulative reward =  -416.73252460613514\n",
      "Took  98  steps to converge\n",
      "Finished episode  17  with  98 ;\n",
      "cumulative reward =  -6268.238253848488\n",
      "Took  99  steps to converge\n",
      "Finished episode  18  with  99 ;\n",
      "cumulative reward =  -7544.497105610502\n",
      "Took  70  steps to converge\n",
      "Finished episode  19  with  70 ;\n",
      "cumulative reward =  -5304.89509977705\n",
      "Test result at episode  350\n",
      "Took  37  steps to converge\n",
      "Finished episode  0  with  37 ;\n",
      "cumulative reward =  -693.0609942721923\n",
      "Took  40  steps to converge\n",
      "Finished episode  1  with  40 ;\n",
      "cumulative reward =  -907.0016045335586\n",
      "Finished episode  2  with  199 ;\n",
      "cumulative reward =  -19158.707910446657\n",
      "Took  89  steps to converge\n",
      "Finished episode  3  with  89 ;\n",
      "cumulative reward =  -5936.769334429072\n",
      "Finished episode  4  with  199 ;\n",
      "cumulative reward =  -9638.742337322197\n",
      "Finished episode  5  with  199 ;\n",
      "cumulative reward =  -16583.00343042425\n",
      "Finished episode  6  with  199 ;\n",
      "cumulative reward =  -13019.501366626719\n",
      "Finished episode  7  with  199 ;\n",
      "cumulative reward =  -10659.64567478569\n",
      "Took  110  steps to converge\n",
      "Finished episode  8  with  110 ;\n",
      "cumulative reward =  -6540.585352901567\n",
      "Finished episode  9  with  199 ;\n",
      "cumulative reward =  -9418.333593194287\n",
      "Took  102  steps to converge\n",
      "Finished episode  10  with  102 ;\n",
      "cumulative reward =  -6694.901800355445\n",
      "Took  30  steps to converge\n",
      "Finished episode  11  with  30 ;\n",
      "cumulative reward =  -699.1557005026646\n",
      "Took  81  steps to converge\n",
      "Finished episode  12  with  81 ;\n",
      "cumulative reward =  -4758.136538103749\n",
      "Took  91  steps to converge\n",
      "Finished episode  13  with  91 ;\n",
      "cumulative reward =  -4801.686206840282\n",
      "Finished episode  14  with  199 ;\n",
      "cumulative reward =  -10845.142280130285\n",
      "Took  26  steps to converge\n",
      "Finished episode  15  with  26 ;\n",
      "cumulative reward =  -562.1112572911067\n",
      "Finished episode  16  with  199 ;\n",
      "cumulative reward =  -23909.99246577345\n",
      "Finished episode  17  with  199 ;\n",
      "cumulative reward =  -19067.70255391357\n",
      "Finished episode  18  with  199 ;\n",
      "cumulative reward =  -6765.152831093728\n",
      "Finished episode  19  with  199 ;\n",
      "cumulative reward =  -9263.489100207398\n",
      "Test result at episode  400\n",
      "Finished episode  0  with  199 ;\n",
      "cumulative reward =  -16943.586895511504\n",
      "Took  33  steps to converge\n",
      "Finished episode  1  with  33 ;\n",
      "cumulative reward =  -459.66693391308445\n",
      "Took  45  steps to converge\n",
      "Finished episode  2  with  45 ;\n",
      "cumulative reward =  -1402.7222037212812\n",
      "Finished episode  3  with  199 ;\n",
      "cumulative reward =  -23064.15861024262\n",
      "Finished episode  4  with  199 ;\n",
      "cumulative reward =  -22543.061422617862\n",
      "Finished episode  5  with  199 ;\n",
      "cumulative reward =  -29369.109523018666\n",
      "Took  116  steps to converge\n",
      "Finished episode  6  with  116 ;\n",
      "cumulative reward =  -8466.345778751569\n",
      "Finished episode  7  with  199 ;\n",
      "cumulative reward =  -22705.151177193773\n",
      "Finished episode  8  with  199 ;\n",
      "cumulative reward =  -22359.166432318456\n",
      "Took  31  steps to converge\n",
      "Finished episode  9  with  31 ;\n",
      "cumulative reward =  -664.8140505721761\n",
      "Took  89  steps to converge\n",
      "Finished episode  10  with  89 ;\n",
      "cumulative reward =  -7365.362705635092\n",
      "Took  60  steps to converge\n",
      "Finished episode  11  with  60 ;\n",
      "cumulative reward =  -2000.387910901266\n",
      "Finished episode  12  with  199 ;\n",
      "cumulative reward =  -21533.980888977574\n",
      "Finished episode  13  with  199 ;\n",
      "cumulative reward =  -26670.677696866605\n",
      "Finished episode  14  with  199 ;\n",
      "cumulative reward =  -19579.798496133626\n",
      "Took  91  steps to converge\n",
      "Finished episode  15  with  91 ;\n",
      "cumulative reward =  -6535.5613329899215\n",
      "Finished episode  16  with  199 ;\n",
      "cumulative reward =  -15860.724089278308\n",
      "Took  56  steps to converge\n",
      "Finished episode  17  with  56 ;\n",
      "cumulative reward =  -1655.192350115634\n",
      "Took  155  steps to converge\n",
      "Finished episode  18  with  155 ;\n",
      "cumulative reward =  -19872.56091835348\n",
      "Took  32  steps to converge\n",
      "Finished episode  19  with  32 ;\n",
      "cumulative reward =  -641.8404847397237\n",
      "Test result at episode  450\n",
      "Took  171  steps to converge\n",
      "Finished episode  0  with  171 ;\n",
      "cumulative reward =  -15765.452012895974\n",
      "Took  30  steps to converge\n",
      "Finished episode  1  with  30 ;\n",
      "cumulative reward =  -460.72738338684155\n",
      "Took  164  steps to converge\n",
      "Finished episode  2  with  164 ;\n",
      "cumulative reward =  -15141.961898076775\n",
      "Finished episode  3  with  199 ;\n",
      "cumulative reward =  -13454.765973815382\n",
      "Finished episode  4  with  199 ;\n",
      "cumulative reward =  -15159.736202233786\n",
      "Finished episode  5  with  199 ;\n",
      "cumulative reward =  -18326.42843752897\n",
      "Finished episode  6  with  199 ;\n",
      "cumulative reward =  -14570.91295426865\n",
      "Finished episode  7  with  199 ;\n",
      "cumulative reward =  -22569.225093869078\n",
      "Finished episode  8  with  199 ;\n",
      "cumulative reward =  -18518.318090569403\n",
      "Took  110  steps to converge\n",
      "Finished episode  9  with  110 ;\n",
      "cumulative reward =  -14119.547929118882\n",
      "Finished episode  10  with  199 ;\n",
      "cumulative reward =  -14587.493435263266\n",
      "Finished episode  11  with  199 ;\n",
      "cumulative reward =  -18069.150574103824\n",
      "Finished episode  12  with  199 ;\n",
      "cumulative reward =  -19505.15615585721\n",
      "Finished episode  13  with  199 ;\n",
      "cumulative reward =  -15502.236498139899\n",
      "Took  92  steps to converge\n",
      "Finished episode  14  with  92 ;\n",
      "cumulative reward =  -9385.080547463129\n",
      "Took  116  steps to converge\n",
      "Finished episode  15  with  116 ;\n",
      "cumulative reward =  -14786.707919438071\n",
      "Finished episode  16  with  199 ;\n",
      "cumulative reward =  -28706.32561953156\n",
      "Took  28  steps to converge\n",
      "Finished episode  17  with  28 ;\n",
      "cumulative reward =  -630.7009457838906\n",
      "Took  96  steps to converge\n",
      "Finished episode  18  with  96 ;\n",
      "cumulative reward =  -9564.96150625688\n",
      "Finished episode  19  with  199 ;\n",
      "cumulative reward =  -21478.30026744268\n"
     ]
    }
   ],
   "source": [
    "# Real run\n",
    "num_episode=500\n",
    "test_interval=50\n",
    "num_test=20\n",
    "num_iteration=200\n",
    "BATCH_SIZE=128\n",
    "debug=False\n",
    "env2 = gym.make('ConsensusEnv:ConsensusContEnv-v0', N=N, dt=0.1, Delta=0.05,\n",
    "              input_type=input_type, observe_type=O_ACCELERATION).unwrapped\n",
    "agent4 = LearnerAgent(device, N, env2.ns, env2.na, hidden)\n",
    "a4hist = train(agent4, env2, num_episode, test_interval, num_test, num_iteration, BATCH_SIZE, num_sample, action_space, debug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test result at episode  0\n",
      "Took  17  steps to converge\n",
      "Finished episode  0  with  17 ;\n",
      "cumulative reward =  -1288.5595338277585\n",
      "Took  22  steps to converge\n",
      "Finished episode  1  with  22 ;\n",
      "cumulative reward =  -1798.9139804532465\n",
      "Took  18  steps to converge\n",
      "Finished episode  2  with  18 ;\n",
      "cumulative reward =  -1489.8759030800295\n",
      "Took  17  steps to converge\n",
      "Finished episode  3  with  17 ;\n",
      "cumulative reward =  -1448.423887887857\n",
      "Took  22  steps to converge\n",
      "Finished episode  4  with  22 ;\n",
      "cumulative reward =  -1855.4319110567665\n",
      "Took  23  steps to converge\n",
      "Finished episode  5  with  23 ;\n",
      "cumulative reward =  -1420.3477309046607\n",
      "Took  22  steps to converge\n",
      "Finished episode  6  with  22 ;\n",
      "cumulative reward =  -1502.672846994754\n",
      "Took  35  steps to converge\n",
      "Finished episode  7  with  35 ;\n",
      "cumulative reward =  -3042.9041584092706\n",
      "Took  20  steps to converge\n",
      "Finished episode  8  with  20 ;\n",
      "cumulative reward =  -1469.3617134198325\n",
      "Took  27  steps to converge\n",
      "Finished episode  9  with  27 ;\n",
      "cumulative reward =  -2467.8572157041217\n",
      "Took  17  steps to converge\n",
      "Finished episode  10  with  17 ;\n",
      "cumulative reward =  -1383.6105051080165\n",
      "Took  19  steps to converge\n",
      "Finished episode  11  with  19 ;\n",
      "cumulative reward =  -1287.7048742420757\n",
      "Took  21  steps to converge\n",
      "Finished episode  12  with  21 ;\n",
      "cumulative reward =  -1579.4629510933974\n",
      "Took  15  steps to converge\n",
      "Finished episode  13  with  15 ;\n",
      "cumulative reward =  -1177.9220254546321\n",
      "Took  24  steps to converge\n",
      "Finished episode  14  with  24 ;\n",
      "cumulative reward =  -1687.5230061851264\n",
      "Took  21  steps to converge\n",
      "Finished episode  15  with  21 ;\n",
      "cumulative reward =  -1695.1452336203076\n",
      "Took  22  steps to converge\n",
      "Finished episode  16  with  22 ;\n",
      "cumulative reward =  -2002.0632075329268\n",
      "Took  21  steps to converge\n",
      "Finished episode  17  with  21 ;\n",
      "cumulative reward =  -1469.2846972143448\n",
      "Took  17  steps to converge\n",
      "Finished episode  18  with  17 ;\n",
      "cumulative reward =  -1349.0089066825512\n",
      "Took  18  steps to converge\n",
      "Finished episode  19  with  18 ;\n",
      "cumulative reward =  -1356.1381154895987\n",
      "Test result at episode  50\n",
      "Finished episode  0  with  199 ;\n",
      "cumulative reward =  -14867.145747765728\n",
      "Finished episode  1  with  199 ;\n",
      "cumulative reward =  -14811.979377517615\n",
      "Took  8  steps to converge\n",
      "Finished episode  2  with  8 ;\n",
      "cumulative reward =  -62.52884104320307\n",
      "Finished episode  3  with  199 ;\n",
      "cumulative reward =  -13383.204845220776\n",
      "Took  6  steps to converge\n",
      "Finished episode  4  with  6 ;\n",
      "cumulative reward =  -62.56396267508147\n",
      "Finished episode  5  with  199 ;\n",
      "cumulative reward =  -23003.293305624276\n",
      "Took  13  steps to converge\n",
      "Finished episode  6  with  13 ;\n",
      "cumulative reward =  -329.62119471917197\n",
      "Finished episode  7  with  199 ;\n",
      "cumulative reward =  -13570.53688232057\n",
      "Took  7  steps to converge\n",
      "Finished episode  8  with  7 ;\n",
      "cumulative reward =  -108.66615929030098\n",
      "Finished episode  9  with  199 ;\n",
      "cumulative reward =  -14799.2421690655\n",
      "Took  5  steps to converge\n",
      "Finished episode  10  with  5 ;\n",
      "cumulative reward =  -85.4453584216058\n",
      "Finished episode  11  with  199 ;\n",
      "cumulative reward =  -17279.509950782885\n",
      "Took  18  steps to converge\n",
      "Finished episode  12  with  18 ;\n",
      "cumulative reward =  -182.16988803358373\n",
      "Finished episode  13  with  199 ;\n",
      "cumulative reward =  -14110.355885986351\n",
      "Took  9  steps to converge\n",
      "Finished episode  14  with  9 ;\n",
      "cumulative reward =  -132.5054516399564\n",
      "Finished episode  15  with  199 ;\n",
      "cumulative reward =  -28454.461827969957\n",
      "Took  12  steps to converge\n",
      "Finished episode  16  with  12 ;\n",
      "cumulative reward =  -180.61821770508016\n",
      "Finished episode  17  with  199 ;\n",
      "cumulative reward =  -14539.273864132736\n",
      "Finished episode  18  with  199 ;\n",
      "cumulative reward =  -14115.846112828103\n",
      "Finished episode  19  with  199 ;\n",
      "cumulative reward =  -14151.309350370888\n",
      "Test result at episode  100\n",
      "Took  38  steps to converge\n",
      "Finished episode  0  with  38 ;\n",
      "cumulative reward =  -184.26199773458504\n",
      "Finished episode  1  with  199 ;\n",
      "cumulative reward =  -12705.960399943337\n",
      "Finished episode  2  with  199 ;\n",
      "cumulative reward =  -17170.345077291804\n",
      "Took  7  steps to converge\n",
      "Finished episode  3  with  7 ;\n",
      "cumulative reward =  -70.87850898345913\n",
      "Finished episode  4  with  199 ;\n",
      "cumulative reward =  -7174.4551146131125\n",
      "Took  89  steps to converge\n",
      "Finished episode  5  with  89 ;\n",
      "cumulative reward =  -390.0770590839629\n",
      "Finished episode  6  with  199 ;\n",
      "cumulative reward =  -18194.556822166956\n",
      "Finished episode  7  with  199 ;\n",
      "cumulative reward =  -13481.667619894824\n",
      "Finished episode  8  with  199 ;\n",
      "cumulative reward =  -6956.247599999772\n",
      "Finished episode  9  with  199 ;\n",
      "cumulative reward =  -8871.656217579453\n",
      "Took  13  steps to converge\n",
      "Finished episode  10  with  13 ;\n",
      "cumulative reward =  -115.84856847922104\n",
      "Took  72  steps to converge\n",
      "Finished episode  11  with  72 ;\n",
      "cumulative reward =  -307.5217800110843\n",
      "Took  4  steps to converge\n",
      "Finished episode  12  with  4 ;\n",
      "cumulative reward =  -44.89000008014044\n",
      "Took  4  steps to converge\n",
      "Finished episode  13  with  4 ;\n",
      "cumulative reward =  -36.46954611548868\n",
      "Finished episode  14  with  199 ;\n",
      "cumulative reward =  -8003.018691248095\n",
      "Finished episode  15  with  199 ;\n",
      "cumulative reward =  -16115.874571051692\n",
      "Took  12  steps to converge\n",
      "Finished episode  16  with  12 ;\n",
      "cumulative reward =  -82.04418827710683\n",
      "Took  4  steps to converge\n",
      "Finished episode  17  with  4 ;\n",
      "cumulative reward =  -51.75779834421065\n",
      "Finished episode  18  with  199 ;\n",
      "cumulative reward =  -8001.687994335495\n",
      "Took  83  steps to converge\n",
      "Finished episode  19  with  83 ;\n",
      "cumulative reward =  -6229.509577413788\n",
      "Test result at episode  150\n",
      "Took  35  steps to converge\n",
      "Finished episode  0  with  35 ;\n",
      "cumulative reward =  -215.80875285059238\n",
      "Finished episode  1  with  199 ;\n",
      "cumulative reward =  -13519.918168542243\n",
      "Took  93  steps to converge\n",
      "Finished episode  2  with  93 ;\n",
      "cumulative reward =  -5172.072179995126\n",
      "Took  150  steps to converge\n",
      "Finished episode  3  with  150 ;\n",
      "cumulative reward =  -4208.055213097076\n",
      "Took  137  steps to converge\n",
      "Finished episode  4  with  137 ;\n",
      "cumulative reward =  -5053.845167183892\n",
      "Finished episode  5  with  199 ;\n",
      "cumulative reward =  -9084.301302882615\n",
      "Finished episode  6  with  199 ;\n",
      "cumulative reward =  -9951.810512088343\n",
      "Took  54  steps to converge\n",
      "Finished episode  7  with  54 ;\n",
      "cumulative reward =  -1335.6489871257502\n",
      "Took  130  steps to converge\n",
      "Finished episode  8  with  130 ;\n",
      "cumulative reward =  -4230.4806773432765\n",
      "Took  59  steps to converge\n",
      "Finished episode  9  with  59 ;\n",
      "cumulative reward =  -2359.5802013860302\n",
      "Finished episode  10  with  199 ;\n",
      "cumulative reward =  -10395.71898992657\n",
      "Finished episode  11  with  199 ;\n",
      "cumulative reward =  -12319.02947995028\n",
      "Took  107  steps to converge\n",
      "Finished episode  12  with  107 ;\n",
      "cumulative reward =  -6526.426586982147\n",
      "Took  14  steps to converge\n",
      "Finished episode  13  with  14 ;\n",
      "cumulative reward =  -107.58487247503169\n",
      "Took  30  steps to converge\n",
      "Finished episode  14  with  30 ;\n",
      "cumulative reward =  -701.1270725338098\n",
      "Took  115  steps to converge\n",
      "Finished episode  15  with  115 ;\n",
      "cumulative reward =  -2434.440218907188\n",
      "Finished episode  16  with  199 ;\n",
      "cumulative reward =  -7157.565135147458\n",
      "Finished episode  17  with  199 ;\n",
      "cumulative reward =  -9046.010798276915\n",
      "Took  14  steps to converge\n",
      "Finished episode  18  with  14 ;\n",
      "cumulative reward =  -111.62405708223699\n",
      "Took  58  steps to converge\n",
      "Finished episode  19  with  58 ;\n",
      "cumulative reward =  -2568.2458025115034\n",
      "Test result at episode  200\n",
      "Took  13  steps to converge\n",
      "Finished episode  0  with  13 ;\n",
      "cumulative reward =  -200.30538290364555\n",
      "Finished episode  1  with  199 ;\n",
      "cumulative reward =  -12037.105770335842\n",
      "Finished episode  2  with  199 ;\n",
      "cumulative reward =  -12424.03288927851\n",
      "Took  10  steps to converge\n",
      "Finished episode  3  with  10 ;\n",
      "cumulative reward =  -124.38348945687004\n",
      "Took  4  steps to converge\n",
      "Finished episode  4  with  4 ;\n",
      "cumulative reward =  -65.93731359835097\n",
      "Took  12  steps to converge\n",
      "Finished episode  5  with  12 ;\n",
      "cumulative reward =  -192.50576268633418\n",
      "Took  5  steps to converge\n",
      "Finished episode  6  with  5 ;\n",
      "cumulative reward =  -83.1238610638495\n",
      "Took  11  steps to converge\n",
      "Finished episode  7  with  11 ;\n",
      "cumulative reward =  -131.61931125452142\n",
      "Finished episode  8  with  199 ;\n",
      "cumulative reward =  -11893.548016681667\n",
      "Finished episode  9  with  199 ;\n",
      "cumulative reward =  -17591.926136174443\n",
      "Finished episode  10  with  199 ;\n",
      "cumulative reward =  -12235.193697745028\n",
      "Finished episode  11  with  199 ;\n",
      "cumulative reward =  -10899.073924817685\n",
      "Took  43  steps to converge\n",
      "Finished episode  12  with  43 ;\n",
      "cumulative reward =  -1839.4445584087814\n",
      "Finished episode  13  with  199 ;\n",
      "cumulative reward =  -12691.367582700843\n",
      "Finished episode  14  with  199 ;\n",
      "cumulative reward =  -11858.559065025242\n",
      "Took  10  steps to converge\n",
      "Finished episode  15  with  10 ;\n",
      "cumulative reward =  -200.89540751215534\n",
      "Took  19  steps to converge\n",
      "Finished episode  16  with  19 ;\n",
      "cumulative reward =  -143.52924596533882\n",
      "Took  39  steps to converge\n",
      "Finished episode  17  with  39 ;\n",
      "cumulative reward =  -243.25563428716055\n",
      "Took  41  steps to converge\n",
      "Finished episode  18  with  41 ;\n",
      "cumulative reward =  -174.03986409659774\n",
      "Took  18  steps to converge\n",
      "Finished episode  19  with  18 ;\n",
      "cumulative reward =  -390.53611211394474\n",
      "Test result at episode  250\n",
      "Finished episode  0  with  199 ;\n",
      "cumulative reward =  -1098.5840348572895\n",
      "Finished episode  1  with  199 ;\n",
      "cumulative reward =  -7944.551038409945\n",
      "Took  136  steps to converge\n",
      "Finished episode  2  with  136 ;\n",
      "cumulative reward =  -780.2137814703427\n",
      "Finished episode  3  with  199 ;\n",
      "cumulative reward =  -1112.6504809793455\n",
      "Finished episode  4  with  199 ;\n",
      "cumulative reward =  -7214.845590260205\n",
      "Finished episode  5  with  199 ;\n",
      "cumulative reward =  -6228.40242660139\n",
      "Finished episode  6  with  199 ;\n",
      "cumulative reward =  -1172.488401561464\n",
      "Finished episode  7  with  199 ;\n",
      "cumulative reward =  -2942.874072990165\n",
      "Finished episode  8  with  199 ;\n",
      "cumulative reward =  -1190.1250094607929\n",
      "Finished episode  9  with  199 ;\n",
      "cumulative reward =  -1197.887729392342\n",
      "Finished episode  10  with  199 ;\n",
      "cumulative reward =  -1108.9508517135523\n",
      "Finished episode  11  with  199 ;\n",
      "cumulative reward =  -1247.5606654375858\n",
      "Finished episode  12  with  199 ;\n",
      "cumulative reward =  -2506.5996405563405\n",
      "Finished episode  13  with  199 ;\n",
      "cumulative reward =  -1133.044516626472\n",
      "Finished episode  14  with  199 ;\n",
      "cumulative reward =  -1156.4086123550178\n",
      "Finished episode  15  with  199 ;\n",
      "cumulative reward =  -1089.9804121951126\n",
      "Finished episode  16  with  199 ;\n",
      "cumulative reward =  -1130.3900431531763\n",
      "Finished episode  17  with  199 ;\n",
      "cumulative reward =  -8559.794612958785\n",
      "Finished episode  18  with  199 ;\n",
      "cumulative reward =  -1371.8745487590577\n",
      "Finished episode  19  with  199 ;\n",
      "cumulative reward =  -7956.642229968868\n",
      "Test result at episode  300\n",
      "Took  14  steps to converge\n",
      "Finished episode  0  with  14 ;\n",
      "cumulative reward =  -95.22380065692722\n",
      "Finished episode  1  with  199 ;\n",
      "cumulative reward =  -9912.738990206044\n",
      "Took  5  steps to converge\n",
      "Finished episode  2  with  5 ;\n",
      "cumulative reward =  -82.52656446494498\n",
      "Took  37  steps to converge\n",
      "Finished episode  3  with  37 ;\n",
      "cumulative reward =  -532.2129254929622\n",
      "Took  13  steps to converge\n",
      "Finished episode  4  with  13 ;\n",
      "cumulative reward =  -116.12781262230104\n",
      "Took  9  steps to converge\n",
      "Finished episode  5  with  9 ;\n",
      "cumulative reward =  -81.36995329882822\n",
      "Finished episode  6  with  199 ;\n",
      "cumulative reward =  -9406.140712302027\n",
      "Finished episode  7  with  199 ;\n",
      "cumulative reward =  -9752.22027754351\n",
      "Took  138  steps to converge\n",
      "Finished episode  8  with  138 ;\n",
      "cumulative reward =  -9540.581803172947\n",
      "Took  12  steps to converge\n",
      "Finished episode  9  with  12 ;\n",
      "cumulative reward =  -60.33781578092163\n",
      "Finished episode  10  with  199 ;\n",
      "cumulative reward =  -10569.846307158281\n",
      "Finished episode  11  with  199 ;\n",
      "cumulative reward =  -9609.531581710227\n",
      "Finished episode  12  with  199 ;\n",
      "cumulative reward =  -13369.838370386557\n",
      "Finished episode  13  with  199 ;\n",
      "cumulative reward =  -9273.47041610914\n",
      "Took  4  steps to converge\n",
      "Finished episode  14  with  4 ;\n",
      "cumulative reward =  -106.85945485573257\n",
      "Finished episode  15  with  199 ;\n",
      "cumulative reward =  -11128.978167613766\n",
      "Finished episode  16  with  199 ;\n",
      "cumulative reward =  -8780.9581990962\n",
      "Finished episode  17  with  199 ;\n",
      "cumulative reward =  -9955.186017238695\n",
      "Took  6  steps to converge\n",
      "Finished episode  18  with  6 ;\n",
      "cumulative reward =  -137.3561479330248\n",
      "Took  8  steps to converge\n",
      "Finished episode  19  with  8 ;\n",
      "cumulative reward =  -98.62459905923599\n",
      "Test result at episode  350\n",
      "Took  181  steps to converge\n",
      "Finished episode  0  with  181 ;\n",
      "cumulative reward =  -4328.349361312573\n",
      "Took  28  steps to converge\n",
      "Finished episode  1  with  28 ;\n",
      "cumulative reward =  -256.6979070403824\n",
      "Finished episode  2  with  199 ;\n",
      "cumulative reward =  -5231.371781874305\n",
      "Finished episode  3  with  199 ;\n",
      "cumulative reward =  -7073.897223410459\n",
      "Finished episode  4  with  199 ;\n",
      "cumulative reward =  -8515.134198877064\n",
      "Took  179  steps to converge\n",
      "Finished episode  5  with  179 ;\n",
      "cumulative reward =  -881.1679059497656\n",
      "Took  47  steps to converge\n",
      "Finished episode  6  with  47 ;\n",
      "cumulative reward =  -294.13304519777176\n",
      "Took  164  steps to converge\n",
      "Finished episode  7  with  164 ;\n",
      "cumulative reward =  -4208.975573602867\n",
      "Took  199  steps to converge\n",
      "Finished episode  8  with  199 ;\n",
      "cumulative reward =  -4843.211314867085\n",
      "Took  80  steps to converge\n",
      "Finished episode  9  with  80 ;\n",
      "cumulative reward =  -394.7207696616139\n",
      "Took  48  steps to converge\n",
      "Finished episode  10  with  48 ;\n",
      "cumulative reward =  -269.46820056467163\n",
      "Took  34  steps to converge\n",
      "Finished episode  11  with  34 ;\n",
      "cumulative reward =  -215.2607612313558\n",
      "Took  43  steps to converge\n",
      "Finished episode  12  with  43 ;\n",
      "cumulative reward =  -247.4075599186019\n",
      "Finished episode  13  with  199 ;\n",
      "cumulative reward =  -8331.452881190935\n",
      "Took  85  steps to converge\n",
      "Finished episode  14  with  85 ;\n",
      "cumulative reward =  -415.70788234117015\n",
      "Took  18  steps to converge\n",
      "Finished episode  15  with  18 ;\n",
      "cumulative reward =  -103.06527072956808\n",
      "Took  7  steps to converge\n",
      "Finished episode  16  with  7 ;\n",
      "cumulative reward =  -88.59504671418478\n",
      "Finished episode  17  with  199 ;\n",
      "cumulative reward =  -8910.08347454103\n",
      "Took  41  steps to converge\n",
      "Finished episode  18  with  41 ;\n",
      "cumulative reward =  -248.976434426496\n",
      "Took  92  steps to converge\n",
      "Finished episode  19  with  92 ;\n",
      "cumulative reward =  -416.4672460430307\n",
      "Test result at episode  400\n",
      "Took  9  steps to converge\n",
      "Finished episode  0  with  9 ;\n",
      "cumulative reward =  -221.71450936060276\n",
      "Took  109  steps to converge\n",
      "Finished episode  1  with  109 ;\n",
      "cumulative reward =  -3113.619102657289\n",
      "Took  9  steps to converge\n",
      "Finished episode  2  with  9 ;\n",
      "cumulative reward =  -76.18812100058844\n",
      "Finished episode  3  with  199 ;\n",
      "cumulative reward =  -7895.9382215248415\n",
      "Took  12  steps to converge\n",
      "Finished episode  4  with  12 ;\n",
      "cumulative reward =  -122.37007715620939\n",
      "Took  14  steps to converge\n",
      "Finished episode  5  with  14 ;\n",
      "cumulative reward =  -250.19008531245515\n",
      "Took  10  steps to converge\n",
      "Finished episode  6  with  10 ;\n",
      "cumulative reward =  -145.65095406023553\n",
      "Finished episode  7  with  199 ;\n",
      "cumulative reward =  -8096.73161932289\n",
      "Took  23  steps to converge\n",
      "Finished episode  8  with  23 ;\n",
      "cumulative reward =  -174.11078418687921\n",
      "Finished episode  9  with  199 ;\n",
      "cumulative reward =  -7412.914357279758\n",
      "Took  12  steps to converge\n",
      "Finished episode  10  with  12 ;\n",
      "cumulative reward =  -96.5168519097376\n",
      "Took  3  steps to converge\n",
      "Finished episode  11  with  3 ;\n",
      "cumulative reward =  -42.272007010914855\n",
      "Took  5  steps to converge\n",
      "Finished episode  12  with  5 ;\n",
      "cumulative reward =  -94.92425426686471\n",
      "Took  5  steps to converge\n",
      "Finished episode  13  with  5 ;\n",
      "cumulative reward =  -72.21352158674071\n",
      "Finished episode  14  with  199 ;\n",
      "cumulative reward =  -7608.747719838415\n",
      "Took  10  steps to converge\n",
      "Finished episode  15  with  10 ;\n",
      "cumulative reward =  -88.52594968342368\n",
      "Took  15  steps to converge\n",
      "Finished episode  16  with  15 ;\n",
      "cumulative reward =  -87.01663894869733\n",
      "Took  11  steps to converge\n",
      "Finished episode  17  with  11 ;\n",
      "cumulative reward =  -305.04174711818547\n",
      "Finished episode  18  with  199 ;\n",
      "cumulative reward =  -8891.28855938495\n",
      "Took  10  steps to converge\n",
      "Finished episode  19  with  10 ;\n",
      "cumulative reward =  -138.32316223283362\n",
      "Test result at episode  450\n",
      "Finished episode  0  with  199 ;\n",
      "cumulative reward =  -9852.396038201434\n",
      "Took  45  steps to converge\n",
      "Finished episode  1  with  45 ;\n",
      "cumulative reward =  -1830.9307755383154\n",
      "Took  10  steps to converge\n",
      "Finished episode  2  with  10 ;\n",
      "cumulative reward =  -181.55864423228346\n",
      "Finished episode  3  with  199 ;\n",
      "cumulative reward =  -11801.489513736251\n",
      "Took  13  steps to converge\n",
      "Finished episode  4  with  13 ;\n",
      "cumulative reward =  -128.68290293729896\n",
      "Took  5  steps to converge\n",
      "Finished episode  5  with  5 ;\n",
      "cumulative reward =  -78.9907279019167\n",
      "Took  101  steps to converge\n",
      "Finished episode  6  with  101 ;\n",
      "cumulative reward =  -7892.206134000162\n",
      "Took  32  steps to converge\n",
      "Finished episode  7  with  32 ;\n",
      "cumulative reward =  -194.15707139683542\n",
      "Finished episode  8  with  199 ;\n",
      "cumulative reward =  -16748.893450422074\n",
      "Took  13  steps to converge\n",
      "Finished episode  9  with  13 ;\n",
      "cumulative reward =  -121.17168098204434\n",
      "Finished episode  10  with  199 ;\n",
      "cumulative reward =  -14720.214835962463\n",
      "Took  32  steps to converge\n",
      "Finished episode  11  with  32 ;\n",
      "cumulative reward =  -2017.6878583583127\n",
      "Took  13  steps to converge\n",
      "Finished episode  12  with  13 ;\n",
      "cumulative reward =  -154.5058850531777\n",
      "Finished episode  13  with  199 ;\n",
      "cumulative reward =  -10146.476926020176\n",
      "Finished episode  14  with  199 ;\n",
      "cumulative reward =  -15030.217369317177\n",
      "Finished episode  15  with  199 ;\n",
      "cumulative reward =  -10657.025076958858\n",
      "Took  23  steps to converge\n",
      "Finished episode  16  with  23 ;\n",
      "cumulative reward =  -150.0042447130424\n",
      "Took  33  steps to converge\n",
      "Finished episode  17  with  33 ;\n",
      "cumulative reward =  -172.96439837193884\n",
      "Took  11  steps to converge\n",
      "Finished episode  18  with  11 ;\n",
      "cumulative reward =  -80.86400861869278\n",
      "Took  5  steps to converge\n",
      "Finished episode  19  with  5 ;\n",
      "cumulative reward =  -89.13723172349685\n"
     ]
    }
   ],
   "source": [
    "# Real run\n",
    "num_episode=500\n",
    "test_interval=50\n",
    "num_test=20\n",
    "num_iteration=200\n",
    "BATCH_SIZE=128\n",
    "debug=False\n",
    "num_sample=50\n",
    "\n",
    "agent2 = RewardAgent(device, N, ns, na, hidden)\n",
    "a2hist = train(agent2, env, num_episode, test_interval, num_test, num_iteration, BATCH_SIZE, num_sample, action_space, debug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test result at episode  0\n",
      "Took  33  steps to converge\n",
      "Finished episode  0  with  33 ;\n",
      "cumulative reward =  -4015.3401921563964\n",
      "Took  27  steps to converge\n",
      "Finished episode  1  with  27 ;\n",
      "cumulative reward =  -2468.18956354625\n",
      "Took  32  steps to converge\n",
      "Finished episode  2  with  32 ;\n",
      "cumulative reward =  -3325.40063067563\n",
      "Took  39  steps to converge\n",
      "Finished episode  3  with  39 ;\n",
      "cumulative reward =  -4844.278389525747\n",
      "Took  34  steps to converge\n",
      "Finished episode  4  with  34 ;\n",
      "cumulative reward =  -3910.5814535276477\n",
      "Took  36  steps to converge\n",
      "Finished episode  5  with  36 ;\n",
      "cumulative reward =  -4043.9231612566973\n",
      "Took  27  steps to converge\n",
      "Finished episode  6  with  27 ;\n",
      "cumulative reward =  -2844.5921399634026\n",
      "Took  28  steps to converge\n",
      "Finished episode  7  with  28 ;\n",
      "cumulative reward =  -2980.7105458133947\n",
      "Took  25  steps to converge\n",
      "Finished episode  8  with  25 ;\n",
      "cumulative reward =  -1868.815955183464\n",
      "Took  32  steps to converge\n",
      "Finished episode  9  with  32 ;\n",
      "cumulative reward =  -3738.1338510831683\n",
      "Took  33  steps to converge\n",
      "Finished episode  10  with  33 ;\n",
      "cumulative reward =  -3752.9431302653243\n",
      "Took  38  steps to converge\n",
      "Finished episode  11  with  38 ;\n",
      "cumulative reward =  -5183.880428971178\n",
      "Took  29  steps to converge\n",
      "Finished episode  12  with  29 ;\n",
      "cumulative reward =  -2871.0935163896265\n",
      "Took  33  steps to converge\n",
      "Finished episode  13  with  33 ;\n",
      "cumulative reward =  -3525.085643581827\n",
      "Took  33  steps to converge\n",
      "Finished episode  14  with  33 ;\n",
      "cumulative reward =  -3843.2800141718317\n",
      "Took  36  steps to converge\n",
      "Finished episode  15  with  36 ;\n",
      "cumulative reward =  -4078.7295292212293\n",
      "Took  32  steps to converge\n",
      "Finished episode  16  with  32 ;\n",
      "cumulative reward =  -3695.3652438139525\n",
      "Took  33  steps to converge\n",
      "Finished episode  17  with  33 ;\n",
      "cumulative reward =  -3641.218235160805\n",
      "Took  30  steps to converge\n",
      "Finished episode  18  with  30 ;\n",
      "cumulative reward =  -3286.5756108055075\n",
      "Took  36  steps to converge\n",
      "Finished episode  19  with  36 ;\n",
      "cumulative reward =  -4526.7752358636935\n",
      "Test result at episode  50\n",
      "Took  25  steps to converge\n",
      "Finished episode  0  with  25 ;\n",
      "cumulative reward =  -2475.0207812065955\n",
      "Took  31  steps to converge\n",
      "Finished episode  1  with  31 ;\n",
      "cumulative reward =  -3164.5021216064756\n",
      "Took  29  steps to converge\n",
      "Finished episode  2  with  29 ;\n",
      "cumulative reward =  -2961.354321090711\n",
      "Took  30  steps to converge\n",
      "Finished episode  3  with  30 ;\n",
      "cumulative reward =  -3435.384534452097\n",
      "Took  31  steps to converge\n",
      "Finished episode  4  with  31 ;\n",
      "cumulative reward =  -3542.7879764078016\n",
      "Took  36  steps to converge\n",
      "Finished episode  5  with  36 ;\n",
      "cumulative reward =  -3897.5902005293756\n",
      "Took  35  steps to converge\n",
      "Finished episode  6  with  35 ;\n",
      "cumulative reward =  -4638.355634422733\n",
      "Took  33  steps to converge\n",
      "Finished episode  7  with  33 ;\n",
      "cumulative reward =  -3621.446878739221\n",
      "Took  28  steps to converge\n",
      "Finished episode  8  with  28 ;\n",
      "cumulative reward =  -3088.478077928048\n",
      "Took  28  steps to converge\n",
      "Finished episode  9  with  28 ;\n",
      "cumulative reward =  -3101.867215072709\n",
      "Took  32  steps to converge\n",
      "Finished episode  10  with  32 ;\n",
      "cumulative reward =  -3548.9209238017843\n",
      "Took  35  steps to converge\n",
      "Finished episode  11  with  35 ;\n",
      "cumulative reward =  -4083.8140824720613\n",
      "Took  31  steps to converge\n",
      "Finished episode  12  with  31 ;\n",
      "cumulative reward =  -3337.349615341275\n",
      "Took  33  steps to converge\n",
      "Finished episode  13  with  33 ;\n",
      "cumulative reward =  -3844.5893952875517\n",
      "Took  29  steps to converge\n",
      "Finished episode  14  with  29 ;\n",
      "cumulative reward =  -3201.0925565923376\n",
      "Took  36  steps to converge\n",
      "Finished episode  15  with  36 ;\n",
      "cumulative reward =  -3861.097273695964\n",
      "Took  29  steps to converge\n",
      "Finished episode  16  with  29 ;\n",
      "cumulative reward =  -3202.014176590879\n",
      "Took  35  steps to converge\n",
      "Finished episode  17  with  35 ;\n",
      "cumulative reward =  -4097.985790347456\n",
      "Took  36  steps to converge\n",
      "Finished episode  18  with  36 ;\n",
      "cumulative reward =  -4345.164662852766\n",
      "Took  31  steps to converge\n",
      "Finished episode  19  with  31 ;\n",
      "cumulative reward =  -3477.7807567479535\n",
      "Test result at episode  100\n",
      "Took  30  steps to converge\n",
      "Finished episode  0  with  30 ;\n",
      "cumulative reward =  -3448.792974957067\n",
      "Took  32  steps to converge\n",
      "Finished episode  1  with  32 ;\n",
      "cumulative reward =  -3492.9066405103085\n",
      "Took  32  steps to converge\n",
      "Finished episode  2  with  32 ;\n",
      "cumulative reward =  -3802.7255086034465\n",
      "Took  35  steps to converge\n",
      "Finished episode  3  with  35 ;\n",
      "cumulative reward =  -4166.8007669171575\n",
      "Took  34  steps to converge\n",
      "Finished episode  4  with  34 ;\n",
      "cumulative reward =  -3980.051006598256\n",
      "Took  31  steps to converge\n",
      "Finished episode  5  with  31 ;\n",
      "cumulative reward =  -3418.3609616421186\n",
      "Took  37  steps to converge\n",
      "Finished episode  6  with  37 ;\n",
      "cumulative reward =  -4219.456480489408\n",
      "Took  32  steps to converge\n",
      "Finished episode  7  with  32 ;\n",
      "cumulative reward =  -3321.9509142412485\n",
      "Took  32  steps to converge\n",
      "Finished episode  8  with  32 ;\n",
      "cumulative reward =  -3667.096389547881\n",
      "Took  35  steps to converge\n",
      "Finished episode  9  with  35 ;\n",
      "cumulative reward =  -3812.661848003109\n",
      "Took  32  steps to converge\n",
      "Finished episode  10  with  32 ;\n",
      "cumulative reward =  -3686.741933262231\n",
      "Took  31  steps to converge\n",
      "Finished episode  11  with  31 ;\n",
      "cumulative reward =  -3134.843689755157\n",
      "Took  30  steps to converge\n",
      "Finished episode  12  with  30 ;\n",
      "cumulative reward =  -2593.6638458089874\n",
      "Took  29  steps to converge\n",
      "Finished episode  13  with  29 ;\n",
      "cumulative reward =  -3189.7544394949796\n",
      "Took  28  steps to converge\n",
      "Finished episode  14  with  28 ;\n",
      "cumulative reward =  -2777.4940568150146\n",
      "Took  29  steps to converge\n",
      "Finished episode  15  with  29 ;\n",
      "cumulative reward =  -3008.4985110613566\n",
      "Took  36  steps to converge\n",
      "Finished episode  16  with  36 ;\n",
      "cumulative reward =  -4318.5881830048975\n",
      "Took  29  steps to converge\n",
      "Finished episode  17  with  29 ;\n",
      "cumulative reward =  -2886.7904015082454\n",
      "Took  34  steps to converge\n",
      "Finished episode  18  with  34 ;\n",
      "cumulative reward =  -3714.784018541924\n",
      "Took  35  steps to converge\n",
      "Finished episode  19  with  35 ;\n",
      "cumulative reward =  -4048.481298963444\n",
      "Test result at episode  150\n",
      "Took  30  steps to converge\n",
      "Finished episode  0  with  30 ;\n",
      "cumulative reward =  -3095.7683927963885\n",
      "Took  33  steps to converge\n",
      "Finished episode  1  with  33 ;\n",
      "cumulative reward =  -3740.959300096219\n",
      "Took  35  steps to converge\n",
      "Finished episode  2  with  35 ;\n",
      "cumulative reward =  -3892.8673979540754\n",
      "Took  35  steps to converge\n",
      "Finished episode  3  with  35 ;\n",
      "cumulative reward =  -3958.1735616651526\n",
      "Took  36  steps to converge\n",
      "Finished episode  4  with  36 ;\n",
      "cumulative reward =  -3620.183523343161\n",
      "Took  34  steps to converge\n",
      "Finished episode  5  with  34 ;\n",
      "cumulative reward =  -4000.025073959495\n",
      "Took  31  steps to converge\n",
      "Finished episode  6  with  31 ;\n",
      "cumulative reward =  -3183.0164144649452\n",
      "Took  30  steps to converge\n",
      "Finished episode  7  with  30 ;\n",
      "cumulative reward =  -3378.208576478795\n",
      "Took  28  steps to converge\n",
      "Finished episode  8  with  28 ;\n",
      "cumulative reward =  -2521.1461678473606\n",
      "Took  33  steps to converge\n",
      "Finished episode  9  with  33 ;\n",
      "cumulative reward =  -3895.1378318730476\n",
      "Took  31  steps to converge\n",
      "Finished episode  10  with  31 ;\n",
      "cumulative reward =  -3430.2461758417726\n",
      "Took  28  steps to converge\n",
      "Finished episode  11  with  28 ;\n",
      "cumulative reward =  -2869.8920022724906\n",
      "Took  30  steps to converge\n",
      "Finished episode  12  with  30 ;\n",
      "cumulative reward =  -3145.0268772657746\n",
      "Took  29  steps to converge\n",
      "Finished episode  13  with  29 ;\n",
      "cumulative reward =  -2794.593498392333\n",
      "Took  31  steps to converge\n",
      "Finished episode  14  with  31 ;\n",
      "cumulative reward =  -3229.66735030305\n",
      "Took  35  steps to converge\n",
      "Finished episode  15  with  35 ;\n",
      "cumulative reward =  -3986.865706406263\n",
      "Took  24  steps to converge\n",
      "Finished episode  16  with  24 ;\n",
      "cumulative reward =  -2284.714558454131\n",
      "Took  29  steps to converge\n",
      "Finished episode  17  with  29 ;\n",
      "cumulative reward =  -2530.2056340352583\n",
      "Took  31  steps to converge\n",
      "Finished episode  18  with  31 ;\n",
      "cumulative reward =  -3447.3983517576426\n",
      "Took  34  steps to converge\n",
      "Finished episode  19  with  34 ;\n",
      "cumulative reward =  -4015.980445137648\n",
      "Test result at episode  200\n",
      "Took  29  steps to converge\n",
      "Finished episode  0  with  29 ;\n",
      "cumulative reward =  -2992.6102255062274\n",
      "Took  32  steps to converge\n",
      "Finished episode  1  with  32 ;\n",
      "cumulative reward =  -3701.003182176324\n",
      "Took  31  steps to converge\n",
      "Finished episode  2  with  31 ;\n",
      "cumulative reward =  -3234.0002254453975\n",
      "Took  34  steps to converge\n",
      "Finished episode  3  with  34 ;\n",
      "cumulative reward =  -4101.524724368305\n",
      "Took  29  steps to converge\n",
      "Finished episode  4  with  29 ;\n",
      "cumulative reward =  -2920.637827549533\n",
      "Took  31  steps to converge\n",
      "Finished episode  5  with  31 ;\n",
      "cumulative reward =  -3421.3555192501644\n",
      "Took  32  steps to converge\n",
      "Finished episode  6  with  32 ;\n",
      "cumulative reward =  -3795.846906934641\n",
      "Took  29  steps to converge\n",
      "Finished episode  7  with  29 ;\n",
      "cumulative reward =  -3292.1509595805305\n",
      "Took  30  steps to converge\n",
      "Finished episode  8  with  30 ;\n",
      "cumulative reward =  -3317.294717906031\n",
      "Took  33  steps to converge\n",
      "Finished episode  9  with  33 ;\n",
      "cumulative reward =  -4152.241719448999\n",
      "Took  35  steps to converge\n",
      "Finished episode  10  with  35 ;\n",
      "cumulative reward =  -4607.974597834223\n",
      "Took  30  steps to converge\n",
      "Finished episode  11  with  30 ;\n",
      "cumulative reward =  -3590.1013013131155\n",
      "Took  36  steps to converge\n",
      "Finished episode  12  with  36 ;\n",
      "cumulative reward =  -4332.933328672696\n",
      "Took  32  steps to converge\n",
      "Finished episode  13  with  32 ;\n",
      "cumulative reward =  -3771.49461380112\n",
      "Took  31  steps to converge\n",
      "Finished episode  14  with  31 ;\n",
      "cumulative reward =  -3808.436375133248\n",
      "Took  29  steps to converge\n",
      "Finished episode  15  with  29 ;\n",
      "cumulative reward =  -2826.4298236241225\n",
      "Took  30  steps to converge\n",
      "Finished episode  16  with  30 ;\n",
      "cumulative reward =  -2853.1104439020655\n",
      "Took  36  steps to converge\n",
      "Finished episode  17  with  36 ;\n",
      "cumulative reward =  -3946.482300461974\n",
      "Took  28  steps to converge\n",
      "Finished episode  18  with  28 ;\n",
      "cumulative reward =  -2783.9396434926875\n",
      "Took  32  steps to converge\n",
      "Finished episode  19  with  32 ;\n",
      "cumulative reward =  -3588.51552796553\n",
      "Test result at episode  250\n",
      "Took  28  steps to converge\n",
      "Finished episode  0  with  28 ;\n",
      "cumulative reward =  -3019.447437147057\n",
      "Took  30  steps to converge\n",
      "Finished episode  1  with  30 ;\n",
      "cumulative reward =  -3015.0747096177806\n",
      "Took  33  steps to converge\n",
      "Finished episode  2  with  33 ;\n",
      "cumulative reward =  -3406.17042224838\n",
      "Took  29  steps to converge\n",
      "Finished episode  3  with  29 ;\n",
      "cumulative reward =  -3152.1357324401597\n",
      "Took  25  steps to converge\n",
      "Finished episode  4  with  25 ;\n",
      "cumulative reward =  -2217.457019278415\n",
      "Took  26  steps to converge\n",
      "Finished episode  5  with  26 ;\n",
      "cumulative reward =  -2386.7550911131975\n",
      "Took  28  steps to converge\n",
      "Finished episode  6  with  28 ;\n",
      "cumulative reward =  -3171.590980603363\n",
      "Took  35  steps to converge\n",
      "Finished episode  7  with  35 ;\n",
      "cumulative reward =  -4140.093649370109\n",
      "Took  30  steps to converge\n",
      "Finished episode  8  with  30 ;\n",
      "cumulative reward =  -3061.214846361336\n",
      "Took  32  steps to converge\n",
      "Finished episode  9  with  32 ;\n",
      "cumulative reward =  -3385.427849347492\n",
      "Took  36  steps to converge\n",
      "Finished episode  10  with  36 ;\n",
      "cumulative reward =  -4207.272124649289\n",
      "Took  32  steps to converge\n",
      "Finished episode  11  with  32 ;\n",
      "cumulative reward =  -3747.0199124013125\n",
      "Took  23  steps to converge\n",
      "Finished episode  12  with  23 ;\n",
      "cumulative reward =  -1933.073075069412\n",
      "Took  31  steps to converge\n",
      "Finished episode  13  with  31 ;\n",
      "cumulative reward =  -3309.0154399683615\n",
      "Took  30  steps to converge\n",
      "Finished episode  14  with  30 ;\n",
      "cumulative reward =  -2962.677738571827\n",
      "Took  28  steps to converge\n",
      "Finished episode  15  with  28 ;\n",
      "cumulative reward =  -2696.232944367091\n",
      "Took  33  steps to converge\n",
      "Finished episode  16  with  33 ;\n",
      "cumulative reward =  -3868.747600610112\n",
      "Took  28  steps to converge\n",
      "Finished episode  17  with  28 ;\n",
      "cumulative reward =  -3107.7953817726084\n",
      "Took  33  steps to converge\n",
      "Finished episode  18  with  33 ;\n",
      "cumulative reward =  -3362.1077044604376\n",
      "Took  31  steps to converge\n",
      "Finished episode  19  with  31 ;\n",
      "cumulative reward =  -3156.7886678016457\n",
      "Test result at episode  300\n",
      "Took  29  steps to converge\n",
      "Finished episode  0  with  29 ;\n",
      "cumulative reward =  -3109.2462279858255\n",
      "Took  33  steps to converge\n",
      "Finished episode  1  with  33 ;\n",
      "cumulative reward =  -3660.337449281762\n",
      "Took  31  steps to converge\n",
      "Finished episode  2  with  31 ;\n",
      "cumulative reward =  -3594.517486403709\n",
      "Took  31  steps to converge\n",
      "Finished episode  3  with  31 ;\n",
      "cumulative reward =  -3060.7668676514922\n",
      "Took  34  steps to converge\n",
      "Finished episode  4  with  34 ;\n",
      "cumulative reward =  -3960.090323861245\n",
      "Took  32  steps to converge\n",
      "Finished episode  5  with  32 ;\n",
      "cumulative reward =  -3335.0904766704634\n",
      "Took  30  steps to converge\n",
      "Finished episode  6  with  30 ;\n",
      "cumulative reward =  -3211.565189835847\n",
      "Took  31  steps to converge\n",
      "Finished episode  7  with  31 ;\n",
      "cumulative reward =  -3265.3154679153877\n",
      "Took  34  steps to converge\n",
      "Finished episode  8  with  34 ;\n",
      "cumulative reward =  -3830.847534834175\n",
      "Took  32  steps to converge\n",
      "Finished episode  9  with  32 ;\n",
      "cumulative reward =  -3650.7074697730964\n",
      "Took  34  steps to converge\n",
      "Finished episode  10  with  34 ;\n",
      "cumulative reward =  -3996.2339658005094\n",
      "Took  26  steps to converge\n",
      "Finished episode  11  with  26 ;\n",
      "cumulative reward =  -2687.722391263548\n",
      "Took  34  steps to converge\n",
      "Finished episode  12  with  34 ;\n",
      "cumulative reward =  -3754.1848748218663\n",
      "Took  30  steps to converge\n",
      "Finished episode  13  with  30 ;\n",
      "cumulative reward =  -3390.4398413435392\n",
      "Took  27  steps to converge\n",
      "Finished episode  14  with  27 ;\n",
      "cumulative reward =  -2685.416039672596\n",
      "Took  35  steps to converge\n",
      "Finished episode  15  with  35 ;\n",
      "cumulative reward =  -3816.4804318206184\n",
      "Took  33  steps to converge\n",
      "Finished episode  16  with  33 ;\n",
      "cumulative reward =  -3442.104034373117\n",
      "Took  29  steps to converge\n",
      "Finished episode  17  with  29 ;\n",
      "cumulative reward =  -3019.6039951003067\n",
      "Took  33  steps to converge\n",
      "Finished episode  18  with  33 ;\n",
      "cumulative reward =  -3697.518193514092\n",
      "Took  29  steps to converge\n",
      "Finished episode  19  with  29 ;\n",
      "cumulative reward =  -3463.6542039967253\n",
      "Test result at episode  350\n",
      "Took  36  steps to converge\n",
      "Finished episode  0  with  36 ;\n",
      "cumulative reward =  -4065.424231381578\n",
      "Took  31  steps to converge\n",
      "Finished episode  1  with  31 ;\n",
      "cumulative reward =  -3030.195251398753\n",
      "Took  31  steps to converge\n",
      "Finished episode  2  with  31 ;\n",
      "cumulative reward =  -3389.7685558211447\n",
      "Took  30  steps to converge\n",
      "Finished episode  3  with  30 ;\n",
      "cumulative reward =  -2934.8977867508065\n",
      "Took  35  steps to converge\n",
      "Finished episode  4  with  35 ;\n",
      "cumulative reward =  -4560.812618176689\n",
      "Took  31  steps to converge\n",
      "Finished episode  5  with  31 ;\n",
      "cumulative reward =  -3303.922846271483\n",
      "Took  31  steps to converge\n",
      "Finished episode  6  with  31 ;\n",
      "cumulative reward =  -3474.144876585749\n",
      "Took  34  steps to converge\n",
      "Finished episode  7  with  34 ;\n",
      "cumulative reward =  -3959.9799088810205\n",
      "Took  24  steps to converge\n",
      "Finished episode  8  with  24 ;\n",
      "cumulative reward =  -2192.8163845474387\n",
      "Took  32  steps to converge\n",
      "Finished episode  9  with  32 ;\n",
      "cumulative reward =  -3491.6032790147824\n",
      "Took  35  steps to converge\n",
      "Finished episode  10  with  35 ;\n",
      "cumulative reward =  -3677.987102220176\n",
      "Took  32  steps to converge\n",
      "Finished episode  11  with  32 ;\n",
      "cumulative reward =  -3475.276020638919\n",
      "Took  34  steps to converge\n",
      "Finished episode  12  with  34 ;\n",
      "cumulative reward =  -4223.009248411626\n",
      "Took  30  steps to converge\n",
      "Finished episode  13  with  30 ;\n",
      "cumulative reward =  -3097.8550016105387\n",
      "Took  32  steps to converge\n",
      "Finished episode  14  with  32 ;\n",
      "cumulative reward =  -3688.619105089753\n",
      "Took  30  steps to converge\n",
      "Finished episode  15  with  30 ;\n",
      "cumulative reward =  -3041.843648065056\n",
      "Took  27  steps to converge\n",
      "Finished episode  16  with  27 ;\n",
      "cumulative reward =  -2298.9275099918455\n",
      "Took  31  steps to converge\n",
      "Finished episode  17  with  31 ;\n",
      "cumulative reward =  -3379.722461012792\n",
      "Took  28  steps to converge\n",
      "Finished episode  18  with  28 ;\n",
      "cumulative reward =  -2489.1201493551907\n",
      "Took  30  steps to converge\n",
      "Finished episode  19  with  30 ;\n",
      "cumulative reward =  -3344.475372924362\n",
      "Test result at episode  400\n",
      "Took  31  steps to converge\n",
      "Finished episode  0  with  31 ;\n",
      "cumulative reward =  -3386.4263959645564\n",
      "Took  36  steps to converge\n",
      "Finished episode  1  with  36 ;\n",
      "cumulative reward =  -4440.978535269708\n",
      "Took  31  steps to converge\n",
      "Finished episode  2  with  31 ;\n",
      "cumulative reward =  -3633.183026699304\n",
      "Took  32  steps to converge\n",
      "Finished episode  3  with  32 ;\n",
      "cumulative reward =  -3323.6452301799354\n",
      "Took  33  steps to converge\n",
      "Finished episode  4  with  33 ;\n",
      "cumulative reward =  -3335.0033286669714\n",
      "Took  27  steps to converge\n",
      "Finished episode  5  with  27 ;\n",
      "cumulative reward =  -2517.843928895567\n",
      "Took  34  steps to converge\n",
      "Finished episode  6  with  34 ;\n",
      "cumulative reward =  -4148.754833955538\n",
      "Took  36  steps to converge\n",
      "Finished episode  7  with  36 ;\n",
      "cumulative reward =  -4057.193883480838\n",
      "Took  32  steps to converge\n",
      "Finished episode  8  with  32 ;\n",
      "cumulative reward =  -3800.7742717881015\n",
      "Took  35  steps to converge\n",
      "Finished episode  9  with  35 ;\n",
      "cumulative reward =  -4500.670833796367\n",
      "Took  31  steps to converge\n",
      "Finished episode  10  with  31 ;\n",
      "cumulative reward =  -3235.132000114665\n",
      "Took  28  steps to converge\n",
      "Finished episode  11  with  28 ;\n",
      "cumulative reward =  -2854.369889468602\n",
      "Took  33  steps to converge\n",
      "Finished episode  12  with  33 ;\n",
      "cumulative reward =  -3486.723041821158\n",
      "Took  26  steps to converge\n",
      "Finished episode  13  with  26 ;\n",
      "cumulative reward =  -2461.91097534586\n",
      "Took  36  steps to converge\n",
      "Finished episode  14  with  36 ;\n",
      "cumulative reward =  -4473.339572208276\n",
      "Took  32  steps to converge\n",
      "Finished episode  15  with  32 ;\n",
      "cumulative reward =  -3860.487838605513\n",
      "Took  28  steps to converge\n",
      "Finished episode  16  with  28 ;\n",
      "cumulative reward =  -3059.696154714615\n",
      "Took  34  steps to converge\n",
      "Finished episode  17  with  34 ;\n",
      "cumulative reward =  -4005.4438883106873\n",
      "Took  30  steps to converge\n",
      "Finished episode  18  with  30 ;\n",
      "cumulative reward =  -3357.3522774446874\n",
      "Took  25  steps to converge\n",
      "Finished episode  19  with  25 ;\n",
      "cumulative reward =  -2355.9700407815494\n",
      "Test result at episode  450\n",
      "Took  31  steps to converge\n",
      "Finished episode  0  with  31 ;\n",
      "cumulative reward =  -3411.8970314663648\n",
      "Took  33  steps to converge\n",
      "Finished episode  1  with  33 ;\n",
      "cumulative reward =  -3814.500986328531\n",
      "Took  30  steps to converge\n",
      "Finished episode  2  with  30 ;\n",
      "cumulative reward =  -3142.9182168495986\n",
      "Took  29  steps to converge\n",
      "Finished episode  3  with  29 ;\n",
      "cumulative reward =  -2745.0207165528122\n",
      "Took  32  steps to converge\n",
      "Finished episode  4  with  32 ;\n",
      "cumulative reward =  -3101.4976291639264\n",
      "Took  36  steps to converge\n",
      "Finished episode  5  with  36 ;\n",
      "cumulative reward =  -3854.8766285977363\n",
      "Took  29  steps to converge\n",
      "Finished episode  6  with  29 ;\n",
      "cumulative reward =  -3228.258461998687\n",
      "Took  34  steps to converge\n",
      "Finished episode  7  with  34 ;\n",
      "cumulative reward =  -3858.17099791014\n",
      "Took  29  steps to converge\n",
      "Finished episode  8  with  29 ;\n",
      "cumulative reward =  -3106.589283743084\n",
      "Took  30  steps to converge\n",
      "Finished episode  9  with  30 ;\n",
      "cumulative reward =  -3076.27935771427\n",
      "Took  33  steps to converge\n",
      "Finished episode  10  with  33 ;\n",
      "cumulative reward =  -3700.7157343926183\n",
      "Took  28  steps to converge\n",
      "Finished episode  11  with  28 ;\n",
      "cumulative reward =  -2800.8300392354\n",
      "Took  32  steps to converge\n",
      "Finished episode  12  with  32 ;\n",
      "cumulative reward =  -3432.0910409592134\n",
      "Took  28  steps to converge\n",
      "Finished episode  13  with  28 ;\n",
      "cumulative reward =  -2708.487428015267\n",
      "Took  33  steps to converge\n",
      "Finished episode  14  with  33 ;\n",
      "cumulative reward =  -3568.9237020772857\n",
      "Took  34  steps to converge\n",
      "Finished episode  15  with  34 ;\n",
      "cumulative reward =  -4129.1048522871715\n",
      "Took  26  steps to converge\n",
      "Finished episode  16  with  26 ;\n",
      "cumulative reward =  -2706.512364067177\n",
      "Took  31  steps to converge\n",
      "Finished episode  17  with  31 ;\n",
      "cumulative reward =  -3299.3984047109225\n",
      "Took  33  steps to converge\n",
      "Finished episode  18  with  33 ;\n",
      "cumulative reward =  -3820.0825814947507\n",
      "Took  33  steps to converge\n",
      "Finished episode  19  with  33 ;\n",
      "cumulative reward =  -4312.365083737929\n"
     ]
    }
   ],
   "source": [
    "# Real run\n",
    "num_episode=500\n",
    "test_interval=50\n",
    "num_test=20\n",
    "num_iteration=200\n",
    "BATCH_SIZE=128\n",
    "debug=False\n",
    "num_sample=50\n",
    "\n",
    "agent3 = RewardActionAgent(device, N, ns, na, hidden)\n",
    "a3hist = train(agent3, env, num_episode, test_interval, num_test, num_iteration, BATCH_SIZE, num_sample, action_space, debug)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot your result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Randomly give a test\n",
    "def plot_test(agent, env, fnames=[], num_iteration=100, action_space=[-1,1], imdir=''):\n",
    "    reward_hist_hst = []\n",
    "    N=env.N\n",
    "    for e,f in enumerate(fnames):\n",
    "        steps = 0\n",
    "        agent.net.eval()\n",
    "        cum_reward = 0\n",
    "        reward_hist = []\n",
    "\n",
    "        state = env.reset()\n",
    "        state = torch.from_numpy(state).float()\n",
    "        state = Variable(state)\n",
    "        env.render()\n",
    "\n",
    "        for t in range(num_iteration):  \n",
    "            # Try to pick an action, react, and store the resulting behavior in the pool here\n",
    "            actions = []\n",
    "            for i in range(N):\n",
    "                action = agent.select_action(state[i], **{\n",
    "                    'steps_done':t, 'rand':False, 'num_sample':50, 'action_space':action_space\n",
    "                })\n",
    "                actions.append(action)\n",
    "            action = np.array(actions).T \n",
    "\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            next_state = Variable(torch.from_numpy(next_state).float()) # The float() probably avoids bug in net.forward()\n",
    "            state = next_state\n",
    "            cum_reward += sum(reward)\n",
    "            reward_hist.append(reward)\n",
    "\n",
    "            if len(f) > 0:\n",
    "                img = env.render(mode=\"rgb_array\")\n",
    "                plt.imshow(img)\n",
    "                plt.savefig(imdir + f + '-{:03d}.jpg'.format(t))\n",
    "            steps += 1\n",
    "\n",
    "            if done:\n",
    "                print(\"Took \", t, \" steps to converge\")\n",
    "                break\n",
    "        print(\"Finished episode \", e, \" with \", t, #\" steps, and rewards = \", reward, \n",
    "              \";\\ncumulative reward = \", cum_reward)\n",
    "        reward_hist_hst.append(reward_hist)\n",
    "    return reward_hist_hst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Took  37  steps to converge\n",
      "Finished episode  0  with  37 ;\n",
      "cumulative reward =  -1465.1837352449818\n",
      "Took  52  steps to converge\n",
      "Finished episode  1  with  52 ;\n",
      "cumulative reward =  -5876.460238255958\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[array([-2.33716208, -3.73704451, -5.02747582, -2.40692553, -2.44740167,\n",
       "         -3.94679476, -2.60440141, -3.27951971, -3.72613077, -2.23344064]),\n",
       "  array([-2.11748844, -3.67484099, -4.46607012, -2.24298366, -2.62978105,\n",
       "         -3.81602095, -2.32447237, -3.37081021, -4.24260602, -2.12890889]),\n",
       "  array([-2.24922877, -3.54982402, -3.86582979, -2.21011796, -2.78932804,\n",
       "         -3.59746529, -2.37276267, -3.53157339, -4.75010102, -2.17938635]),\n",
       "  array([-2.48267773, -3.35607537, -3.22638907, -2.29713953, -2.88043377,\n",
       "         -3.29900077, -2.5667454 , -3.66235269, -5.24366637, -2.22262514]),\n",
       "  array([-2.66451562, -3.07711079, -2.73503456, -2.32065565, -2.98349115,\n",
       "         -2.93475338, -2.77070226, -3.71837397, -5.69998311, -2.21436741]),\n",
       "  array([-2.80352244, -2.75844528, -2.4433168 , -2.27927635, -3.0840326 ,\n",
       "         -2.57695253, -3.00674093, -3.67907969, -6.03509721, -2.16523519]),\n",
       "  array([-2.88810805, -2.48467771, -2.21814931, -2.21451538, -3.08202662,\n",
       "         -2.28656731, -3.26736236, -3.54140878, -6.25745915, -2.08485004]),\n",
       "  array([-2.91405545, -2.23374033, -1.99841901, -2.12665485, -2.99317288,\n",
       "         -2.08315669, -3.43945459, -3.31182951, -6.45121765, -1.96829717]),\n",
       "  array([-2.86455121, -2.0303433 , -1.84953653, -2.03617427, -2.85013563,\n",
       "         -1.99649724, -3.52542347, -2.99074567, -6.67802095, -1.84962635]),\n",
       "  array([-2.72146436, -1.84794791, -1.7930359 , -1.94335548, -2.64345522,\n",
       "         -1.9425294 , -3.50405291, -2.5909283 , -6.9574052 , -1.78177697]),\n",
       "  array([-2.48003098, -1.6671929 , -1.70100085, -1.79976605, -2.35008276,\n",
       "         -1.81649749, -3.3549112 , -2.11803591, -7.31248241, -1.66161007]),\n",
       "  array([-2.15485161, -1.50769279, -1.5810871 , -1.61126923, -1.97360971,\n",
       "         -1.60847206, -3.07640333, -1.63612511, -7.75129648, -1.50188524]),\n",
       "  array([-1.8171359 , -1.42224416, -1.51057014, -1.41970262, -1.61202168,\n",
       "         -1.4041716 , -2.69785621, -1.37933467, -8.28156058, -1.3944583 ]),\n",
       "  array([-1.58020454, -1.42938094, -1.4428771 , -1.35457714, -1.45623664,\n",
       "         -1.36109059, -2.26155136, -1.57953545, -8.84946874, -1.41131013]),\n",
       "  array([-1.48257563, -1.45488076, -1.39567094, -1.3667567 , -1.48094059,\n",
       "         -1.37288887, -1.8405989 , -1.95427977, -9.45359467, -1.40588276]),\n",
       "  array([ -1.4484443 ,  -1.46016407,  -1.38544028,  -1.38529897,\n",
       "          -1.4960673 ,  -1.38947808,  -1.55622134,  -2.29647598,\n",
       "         -20.06812209,  -1.41429091]),\n",
       "  array([ -1.41799028,  -1.43487099,  -1.38469076,  -1.39051431,\n",
       "          -1.50086894,  -1.39913122,  -1.55637204,  -2.49151053,\n",
       "         -20.07847315,  -1.39666411]),\n",
       "  array([ -1.38391114,  -1.40979542,  -1.38319429,  -1.39379181,\n",
       "          -1.46253983,  -1.39882471,  -1.70950707,  -2.55539195,\n",
       "         -19.82466059,  -1.38139483]),\n",
       "  array([ -1.3760428 ,  -1.39364548,  -1.35920985,  -1.36359077,\n",
       "          -1.40508391,  -1.39013175,  -1.85519522,  -2.53763479,\n",
       "         -19.63335572,  -1.36055735]),\n",
       "  array([ -1.4341165 ,  -1.38000516,  -1.38335252,  -1.36269769,\n",
       "          -1.3774347 ,  -1.40092152,  -1.9533375 ,  -2.43286627,\n",
       "         -19.49945168,  -1.3712822 ]),\n",
       "  array([ -1.46307904,  -1.38302617,  -1.37030769,  -1.37644394,\n",
       "          -1.36026596,  -1.40407085,  -2.01255873,  -2.27994717,\n",
       "         -19.38281194,  -1.3756386 ]),\n",
       "  array([ -1.42644544,  -1.39943125,  -1.33688399,  -1.40199178,\n",
       "          -1.34725008,  -1.34500189,  -2.02390044,  -2.09198532,\n",
       "         -19.27847311,  -1.35780831]),\n",
       "  array([ -1.35873576,  -1.36644373,  -1.32953367,  -1.42717987,\n",
       "          -1.31628384,  -1.33150365,  -1.94371405,  -1.84772573,\n",
       "         -19.21798332,  -1.33016243]),\n",
       "  array([ -1.27646384,  -1.27177651,  -1.27442474,  -1.37305102,\n",
       "          -1.25977982,  -1.27116733,  -1.77896915,  -1.53778866,\n",
       "         -19.19476539,  -1.28519484]),\n",
       "  array([ -1.19331566,  -1.1712166 ,  -1.18213908,  -1.21012173,\n",
       "          -1.17795455,  -1.17972059,  -1.5610221 ,  -1.26331419,\n",
       "         -19.18871214,  -1.22706556]),\n",
       "  array([ -1.16846053,  -1.12217033,  -1.10450265,  -1.120617  ,\n",
       "          -1.1092674 ,  -1.13919219,  -1.33653688,  -1.11186994,\n",
       "         -19.02630593,  -1.15908872]),\n",
       "  array([ -1.05747988,  -1.03418687,  -1.02985897,  -1.18191699,\n",
       "          -1.03721803,  -1.03035078,  -1.12452544,  -1.01757661,\n",
       "         -18.34797646,  -1.02868953]),\n",
       "  array([ -0.94305024,  -0.99972886,  -0.97724973,  -1.20966618,\n",
       "          -0.96762716,  -0.94219735,  -0.9713098 ,  -0.97712336,\n",
       "         -17.63926383,  -0.94317602]),\n",
       "  array([ -0.8723285 ,  -0.95280326,  -0.86664325,  -1.16413444,\n",
       "          -0.86819932,  -0.8559135 ,  -0.85931309,  -0.89282888,\n",
       "         -16.90211847,  -0.85868236]),\n",
       "  array([ -0.78098495,  -0.8158801 ,  -0.7930098 ,  -1.0423516 ,\n",
       "          -0.78391522,  -0.79869821,  -0.79324376,  -0.82229554,\n",
       "         -16.13351548,  -0.79953671]),\n",
       "  array([ -0.71839345,  -0.70867193,  -0.7376785 ,  -0.837557  ,\n",
       "          -0.7180619 ,  -0.76508971,  -0.69853063,  -0.72272893,\n",
       "         -15.3492504 ,  -0.76072438]),\n",
       "  array([ -0.66076503,  -0.64773053,  -0.67644981,  -0.66347564,\n",
       "          -0.65711346,  -0.69623249,  -0.63434758,  -0.62874653,\n",
       "         -14.55867885,  -0.67636898]),\n",
       "  array([ -0.563883  ,  -0.53240269,  -0.57999668,  -0.5563681 ,\n",
       "          -0.54840595,  -0.5647791 ,  -0.54102924,  -0.53768718,\n",
       "         -13.74747609,  -0.53774451]),\n",
       "  array([-10.47933611, -10.40827184, -10.42272033, -10.48961125,\n",
       "         -10.40501118, -10.4147873 , -10.41873539, -10.47169911,\n",
       "         -12.81100969, -10.41487496]),\n",
       "  array([-10.35381136, -10.32056194, -10.31516225, -10.45339701,\n",
       "         -10.31710299, -10.30754307, -10.32925992, -10.47209885,\n",
       "         -11.96018182, -10.30736395]),\n",
       "  array([-10.26845121, -10.24216466, -10.2800993 , -10.40946321,\n",
       "         -10.24667048, -10.23933301, -10.23358922, -10.4668616 ,\n",
       "         -11.07737317, -10.24657522]),\n",
       "  array([-10.1574949 , -10.14541253, -10.19562439, -10.30812276,\n",
       "         -10.14349939, -10.14838108, -10.14989191, -10.40561928,\n",
       "         -10.1786583 , -10.17474339]),\n",
       "  array([-10.08993201, -10.08136885, -10.09805722, -10.13681136,\n",
       "         -10.08417767, -10.08569005, -10.09985647, -10.1140144 ,\n",
       "         -10.2645734 , -10.08569005])],\n",
       " [array([-4.18211901, -3.23627595, -3.54549923, -2.46584491, -3.08135944,\n",
       "         -3.7005688 , -4.11302648, -2.93803325, -2.79583739, -2.63252933]),\n",
       "  array([-3.70859253, -3.54283781, -3.51208417, -2.46390934, -2.87140811,\n",
       "         -4.09348947, -4.91758788, -2.58735271, -2.7793878 , -2.4946609 ]),\n",
       "  array([-3.24308645, -3.87327442, -3.47201699, -2.66800614, -2.70187962,\n",
       "         -4.61867085, -5.79369319, -2.59280841, -2.76364755, -2.53153792]),\n",
       "  array([-2.87718285, -4.16073059, -3.4220378 , -2.96269934, -2.64097343,\n",
       "         -5.15406316, -6.57977958, -2.78541208, -2.72155232, -2.63617554]),\n",
       "  array([-2.684905  , -4.41772492, -3.36355982, -3.22920282, -2.66258149,\n",
       "         -5.65404059, -7.33642186, -3.04667949, -2.74818255, -2.75923528]),\n",
       "  array([-2.99547542, -4.62280743, -3.28326784, -3.44507653, -2.82942636,\n",
       "         -6.16839296, -8.04849882, -3.29946117, -2.90446063, -2.98420302]),\n",
       "  array([-3.44056739, -4.73973713, -3.30423493, -3.60588521, -3.02179725,\n",
       "         -6.76930384, -8.63160181, -3.60258253, -3.06062099, -3.15278288]),\n",
       "  array([-3.86102018, -4.75962189, -3.33962062, -3.70830486, -3.16500984,\n",
       "         -7.44713621, -9.16114041, -3.8696237 , -3.18484944, -3.24497418]),\n",
       "  array([-4.21084955, -4.68596638, -3.34017259, -3.7174257 , -3.2606024 ,\n",
       "         -8.19159241, -9.71854216, -4.06503784, -3.26265338, -3.3001782 ]),\n",
       "  array([ -4.46540055,  -4.51355217,  -3.31286448,  -3.68645404,\n",
       "          -3.35041057,  -9.00481021, -10.3073208 ,  -4.18701333,\n",
       "          -3.30910557,  -3.3438923 ]),\n",
       "  array([ -4.62025931,  -4.25135706,  -3.32999269,  -3.62226088,\n",
       "          -3.4745735 ,  -9.88796462, -10.92969817,  -4.23654054,\n",
       "          -3.34258688,  -3.3509299 ]),\n",
       "  array([ -4.69084998,  -3.98479983,  -3.44180639,  -3.54908346,\n",
       "          -3.55013655, -10.8005285 , -11.60436929,  -4.23545473,\n",
       "          -3.39847417,  -3.3987026 ]),\n",
       "  array([ -4.71861522,  -3.7514533 ,  -3.66271283,  -3.56561555,\n",
       "          -3.59587075, -11.72916755, -12.35719069,  -4.2341722 ,\n",
       "          -3.49293164,  -3.49551986]),\n",
       "  array([ -4.68178641,  -3.64258411,  -3.97087941,  -3.63862491,\n",
       "          -3.61376567, -12.68050239, -13.13936914,  -4.23295551,\n",
       "          -3.57553377,  -3.57927405]),\n",
       "  array([ -4.55782711,  -3.74001156,  -4.21123705,  -3.75227996,\n",
       "          -3.74747597, -13.65188899, -13.93910468,  -4.24569621,\n",
       "          -3.71365976,  -3.71668399]),\n",
       "  array([ -4.37340095,  -4.02970312,  -4.38358447,  -3.92071327,\n",
       "          -3.99064981, -24.63463053, -14.76891413,  -4.33148635,\n",
       "          -3.90913511,  -3.91952835]),\n",
       "  array([ -4.26430071,  -4.30091878,  -4.53219006,  -4.09836299,\n",
       "          -4.19763245, -25.61694802, -15.61713695,  -4.49815393,\n",
       "          -4.09926962,  -4.11402596]),\n",
       "  array([ -4.28011985,  -4.49247798,  -4.59719875,  -4.25644652,\n",
       "          -4.37730419, -26.37105057, -26.36655261,  -4.6742742 ,\n",
       "          -4.27256041,  -4.25962585]),\n",
       "  array([ -4.26585278,  -4.44651991,  -4.44992471,  -4.27284034,\n",
       "          -4.34922242, -26.43494993, -26.41288422,  -4.68974581,\n",
       "          -4.29361397,  -4.28380246]),\n",
       "  array([ -4.25198938, -14.36381168,  -4.30673549,  -4.23829652,\n",
       "          -4.26639112, -26.32348166, -26.22911428, -14.75812639,\n",
       "          -4.29185754, -14.26859696]),\n",
       "  array([-14.22865345, -14.29708425, -14.25766857, -14.21039229,\n",
       "         -14.25332687, -26.24466986, -26.157579  , -14.85868534,\n",
       "         -14.24914084, -14.23378623]),\n",
       "  array([-14.19060816, -14.19816571, -14.18427943, -14.19485009,\n",
       "         -14.18171811, -26.2212435 , -26.22124351, -14.88663913,\n",
       "         -14.23218333, -14.18613415]),\n",
       "  array([-14.19312051, -14.22564257, -14.20796917, -14.22437744,\n",
       "         -14.19501097, -26.25783712, -26.25783712, -14.7999842 ,\n",
       "         -14.22315483, -14.20067636]),\n",
       "  array([-14.20776501, -14.30037867, -14.264486  , -14.21992122,\n",
       "         -14.20343447, -26.28163441, -26.28163441, -14.63386005,\n",
       "         -14.20371904, -14.22009731]),\n",
       "  array([-14.22647071, -14.28203835, -14.27925443, -14.19693915,\n",
       "         -14.19304555, -26.26586444, -26.26586444, -14.40557785,\n",
       "         -14.22100365, -14.19535521]),\n",
       "  array([-14.21957001, -14.26200947, -14.26108851, -14.16780823,\n",
       "         -14.16748078, -26.21062417, -26.21062417, -14.24522046,\n",
       "         -14.19651408, -14.1785838 ]),\n",
       "  array([-14.19467243, -14.21326681, -14.19616194, -14.1412287 ,\n",
       "         -14.14778059, -26.10567456, -26.10567456, -14.16143614,\n",
       "         -14.13549009, -14.19327864]),\n",
       "  array([-14.15117123, -14.13958598, -14.09975135, -14.08594811,\n",
       "         -14.13182696, -25.94902437, -25.94902437, -14.09933206,\n",
       "         -14.08336367, -14.21491517]),\n",
       "  array([-14.05585754, -14.07985418, -14.03318073, -14.03422188,\n",
       "         -14.11470449, -25.75413798, -25.75413798, -14.06258913,\n",
       "         -14.04643653, -14.18659991]),\n",
       "  array([-13.97168283, -14.05313041, -13.9600169 , -13.96349339,\n",
       "         -14.05675246, -25.51137517, -25.51137517, -13.96015954,\n",
       "         -13.9917501 , -14.10341612]),\n",
       "  array([-13.91066783, -13.98377976, -13.89841107, -13.91630026,\n",
       "         -14.00576888, -25.21677609, -25.21677609, -13.89890059,\n",
       "         -13.92012472, -13.9912875 ]),\n",
       "  array([-13.81253226, -13.86815806, -13.80905749, -13.84542922,\n",
       "         -13.88757749, -24.87247736, -24.87247736, -13.82832856,\n",
       "         -13.81380457, -13.83850985]),\n",
       "  array([-13.70862074, -13.74395697, -13.71599081, -13.76905641,\n",
       "         -13.71648384, -24.47428088, -24.47428088, -13.76300053,\n",
       "         -13.69834782, -13.71925696]),\n",
       "  array([-13.61781989, -13.60095325, -13.64644068, -13.65329496,\n",
       "         -13.61668012, -24.02175328, -24.02175328, -13.67082366,\n",
       "         -13.60983968, -13.6420079 ]),\n",
       "  array([-13.48983388, -13.48752223, -13.5380709 , -13.51863167,\n",
       "         -13.58327067, -23.51922988, -23.51922988, -13.55279087,\n",
       "         -13.49124665, -13.5169923 ]),\n",
       "  array([-13.32893381, -13.36472276, -13.34305019, -13.32916595,\n",
       "         -13.53757746, -22.97648106, -22.97648106, -13.36549664,\n",
       "         -13.3359703 , -13.34746186]),\n",
       "  array([-13.18306756, -13.21185094, -13.19073941, -13.18282166,\n",
       "         -13.40701903, -22.41349192, -22.41349192, -13.21068529,\n",
       "         -13.19414625, -13.2068689 ]),\n",
       "  array([-13.03559917, -13.03927429, -13.05578042, -13.0303679 ,\n",
       "         -13.19561817, -21.84062011, -21.84062011, -13.04143358,\n",
       "         -13.03199609, -13.03707194]),\n",
       "  array([-12.85143646, -12.86667085, -12.87608235, -12.88033666,\n",
       "         -12.93923502, -21.22487117, -21.22487117, -12.88255096,\n",
       "         -12.85262362, -12.85108687]),\n",
       "  array([-12.68283088, -12.68283776, -12.69348953, -12.76305496,\n",
       "         -12.68427661, -20.55433546, -20.55433546, -12.69550424,\n",
       "         -12.70742992, -12.71144014]),\n",
       "  array([-12.53426176, -12.60329059, -12.53282915, -12.61855261,\n",
       "         -12.52846813, -19.8358293 , -19.8358293 , -12.5181365 ,\n",
       "         -12.51807922, -12.52928916]),\n",
       "  array([-12.33466865, -12.49485673, -12.33595409, -12.42284038,\n",
       "         -12.33555454, -19.07154772, -19.07154772, -12.33377824,\n",
       "         -12.35606182, -12.37680439]),\n",
       "  array([-12.14102482, -12.32147075, -12.14126427, -12.19893976,\n",
       "         -12.16291643, -18.28089545, -18.28089545, -12.15582192,\n",
       "         -12.14240694, -12.20954192]),\n",
       "  array([-11.96793712, -12.07962896, -11.96484382, -11.9980178 ,\n",
       "         -11.98518246, -17.46949623, -17.46949623, -11.97838948,\n",
       "         -11.97295977, -11.99814372]),\n",
       "  array([-11.72924103, -11.84830466, -11.72860513, -11.75042537,\n",
       "         -11.75773535, -16.66114204, -16.66114204, -11.75244184,\n",
       "         -11.73198129, -11.75030569]),\n",
       "  array([-11.54254785, -11.58251011, -11.53727075, -11.53111515,\n",
       "         -11.54304711, -15.88025458, -15.88025458, -11.53685315,\n",
       "         -11.54987493, -11.57520888]),\n",
       "  array([-11.35469259, -11.35316264, -11.340015  , -11.36264333,\n",
       "         -11.35786888, -15.07048326, -15.07048326, -11.35119087,\n",
       "         -11.37375341, -11.4113309 ]),\n",
       "  array([-11.14796749, -11.16596137, -11.15146885, -11.14648824,\n",
       "         -11.16597992, -14.24039913, -14.24039913, -11.15996497,\n",
       "         -11.16197481, -11.19716432]),\n",
       "  array([-10.93603194, -10.93882555, -10.99175877, -10.93203854,\n",
       "         -10.92862157, -13.42923709, -13.42923709, -10.92672877,\n",
       "         -10.94428689, -10.96845685]),\n",
       "  array([-10.68573778, -10.6852674 , -10.74777065, -10.72798731,\n",
       "         -10.68785062, -12.61725446, -12.61725446, -10.68578623,\n",
       "         -10.6966453 , -10.7233466 ]),\n",
       "  array([-10.4810631 , -10.48297515, -10.51523299, -10.49360932,\n",
       "         -10.48389582, -11.70734358, -11.70734358, -10.48270067,\n",
       "         -10.48027548, -10.49289499]),\n",
       "  array([-10.25621453, -10.2483954 , -10.29301681, -10.27825503,\n",
       "         -10.24972806, -10.78789484, -10.78789484, -10.24831625,\n",
       "         -10.25689899, -10.27312178]),\n",
       "  array([-10.07906543, -10.08054514, -10.08766476, -10.1147077 ,\n",
       "         -10.08921605, -10.13702369, -10.13702369, -10.08384215,\n",
       "         -10.07401354, -10.07770116])]]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "h1test = plot_test(agent1, env, \n",
    "          fnames=['LearnerAgent_acc_test1', 'LearnerAgent_acc_test2'], \n",
    "          num_iteration=num_iteration, action_space=action_space, imdir='screencaps/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished episode  0  with  199 ;\n",
      "cumulative reward =  -28482.537104550815\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-fb080c8f12b5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m h2test = plot_test(agent2, env, \n\u001b[1;32m      2\u001b[0m           \u001b[0mfnames\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'RewardAgent_acc_test1'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'RewardAgent_acc_test2'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m           num_iteration=num_iteration, action_space=action_space)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-25-f9bba8966705>\u001b[0m in \u001b[0;36mplot_test\u001b[0;34m(agent, env, fnames, num_iteration, action_space)\u001b[0m\n\u001b[1;32m     32\u001b[0m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"rgb_array\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m             \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m             \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msavefig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'-{:03d}.jpg'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m             \u001b[0msteps\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36msavefig\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    841\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msavefig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    842\u001b[0m     \u001b[0mfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgcf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 843\u001b[0;31m     \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msavefig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    844\u001b[0m     \u001b[0mfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcanvas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw_idle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m   \u001b[0;31m# need this if 'transparent=True' to reset colors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    845\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/matplotlib/figure.py\u001b[0m in \u001b[0;36msavefig\u001b[0;34m(self, fname, transparent, **kwargs)\u001b[0m\n\u001b[1;32m   2309\u001b[0m                 \u001b[0mpatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_edgecolor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'none'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2310\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2311\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcanvas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_figure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2313\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtransparent\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/matplotlib/backend_bases.py\u001b[0m in \u001b[0;36mprint_figure\u001b[0;34m(self, filename, dpi, facecolor, edgecolor, orientation, format, bbox_inches, pad_inches, bbox_extra_artists, backend, **kwargs)\u001b[0m\n\u001b[1;32m   2215\u001b[0m                     \u001b[0morientation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morientation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2216\u001b[0m                     \u001b[0mbbox_inches_restore\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_bbox_inches_restore\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2217\u001b[0;31m                     **kwargs)\n\u001b[0m\u001b[1;32m   2218\u001b[0m             \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2219\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mbbox_inches\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mrestore_bbox\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/matplotlib/backend_bases.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1637\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1638\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1639\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1640\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1641\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/matplotlib/cbook/deprecation.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*inner_args, **inner_kwargs)\u001b[0m\n\u001b[1;32m    409\u001b[0m                          \u001b[0;32melse\u001b[0m \u001b[0mdeprecation_addendum\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    410\u001b[0m                 **kwargs)\n\u001b[0;32m--> 411\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minner_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0minner_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    412\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    413\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/matplotlib/cbook/deprecation.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*inner_args, **inner_kwargs)\u001b[0m\n\u001b[1;32m    409\u001b[0m                          \u001b[0;32melse\u001b[0m \u001b[0mdeprecation_addendum\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    410\u001b[0m                 **kwargs)\n\u001b[0;32m--> 411\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minner_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0minner_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    412\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    413\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/matplotlib/cbook/deprecation.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*inner_args, **inner_kwargs)\u001b[0m\n\u001b[1;32m    409\u001b[0m                          \u001b[0;32melse\u001b[0m \u001b[0mdeprecation_addendum\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    410\u001b[0m                 **kwargs)\n\u001b[0;32m--> 411\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minner_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0minner_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    412\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    413\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/matplotlib/cbook/deprecation.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*inner_args, **inner_kwargs)\u001b[0m\n\u001b[1;32m    409\u001b[0m                          \u001b[0;32melse\u001b[0m \u001b[0mdeprecation_addendum\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    410\u001b[0m                 **kwargs)\n\u001b[0;32m--> 411\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minner_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0minner_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    412\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    413\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/matplotlib/backends/backend_agg.py\u001b[0m in \u001b[0;36mprint_jpg\u001b[0;34m(self, filename_or_obj, dryrun, pil_kwargs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    568\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    569\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_facecolor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 570\u001b[0;31m             \u001b[0mFigureCanvasAgg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    571\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    572\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_facecolor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/matplotlib/backends/backend_agg.py\u001b[0m in \u001b[0;36mdraw\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    405\u001b[0m              (self.toolbar._wait_cursor_for_draw_cm() if self.toolbar\n\u001b[1;32m    406\u001b[0m               else nullcontext()):\n\u001b[0;32m--> 407\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    408\u001b[0m             \u001b[0;31m# A GUI class may be need to update a window using this draw, so\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    409\u001b[0m             \u001b[0;31m# don't forget to call the superclass.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/matplotlib/artist.py\u001b[0m in \u001b[0;36mdraw_wrapper\u001b[0;34m(artist, renderer, *args, **kwargs)\u001b[0m\n\u001b[1;32m     39\u001b[0m                 \u001b[0mrenderer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0martist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrenderer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0martist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_agg_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/matplotlib/figure.py\u001b[0m in \u001b[0;36mdraw\u001b[0;34m(self, renderer)\u001b[0m\n\u001b[1;32m   1862\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1863\u001b[0m             mimage._draw_list_compositing_images(\n\u001b[0;32m-> 1864\u001b[0;31m                 renderer, self, artists, self.suppressComposite)\n\u001b[0m\u001b[1;32m   1865\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1866\u001b[0m             \u001b[0mrenderer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose_group\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'figure'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/matplotlib/image.py\u001b[0m in \u001b[0;36m_draw_list_compositing_images\u001b[0;34m(renderer, parent, artists, suppress_composite)\u001b[0m\n\u001b[1;32m    129\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mnot_composite\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhas_images\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0martists\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m             \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m         \u001b[0;31m# Composite any adjacent images together\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/matplotlib/artist.py\u001b[0m in \u001b[0;36mdraw_wrapper\u001b[0;34m(artist, renderer, *args, **kwargs)\u001b[0m\n\u001b[1;32m     39\u001b[0m                 \u001b[0mrenderer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0martist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrenderer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0martist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_agg_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/matplotlib/cbook/deprecation.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*inner_args, **inner_kwargs)\u001b[0m\n\u001b[1;32m    409\u001b[0m                          \u001b[0;32melse\u001b[0m \u001b[0mdeprecation_addendum\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    410\u001b[0m                 **kwargs)\n\u001b[0;32m--> 411\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minner_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0minner_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    412\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    413\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36mdraw\u001b[0;34m(self, renderer, inframe)\u001b[0m\n\u001b[1;32m   2746\u001b[0m             \u001b[0mrenderer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_rasterizing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2747\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2748\u001b[0;31m         \u001b[0mmimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_draw_list_compositing_images\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0martists\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2749\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2750\u001b[0m         \u001b[0mrenderer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose_group\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'axes'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/matplotlib/image.py\u001b[0m in \u001b[0;36m_draw_list_compositing_images\u001b[0;34m(renderer, parent, artists, suppress_composite)\u001b[0m\n\u001b[1;32m    129\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mnot_composite\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhas_images\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0martists\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m             \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m         \u001b[0;31m# Composite any adjacent images together\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/matplotlib/artist.py\u001b[0m in \u001b[0;36mdraw_wrapper\u001b[0;34m(artist, renderer, *args, **kwargs)\u001b[0m\n\u001b[1;32m     39\u001b[0m                 \u001b[0mrenderer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0martist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrenderer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0martist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_agg_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/matplotlib/image.py\u001b[0m in \u001b[0;36mdraw\u001b[0;34m(self, renderer, *args, **kwargs)\u001b[0m\n\u001b[1;32m    636\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    637\u001b[0m             im, l, b, trans = self.make_image(\n\u001b[0;32m--> 638\u001b[0;31m                 renderer, renderer.get_image_magnification())\n\u001b[0m\u001b[1;32m    639\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mim\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    640\u001b[0m                 \u001b[0mrenderer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/matplotlib/image.py\u001b[0m in \u001b[0;36mmake_image\u001b[0;34m(self, renderer, magnification, unsampled)\u001b[0m\n\u001b[1;32m    921\u001b[0m                 else self.figure.bbox)\n\u001b[1;32m    922\u001b[0m         return self._make_image(self._A, bbox, transformed_bbox, clip,\n\u001b[0;32m--> 923\u001b[0;31m                                 magnification, unsampled=unsampled)\n\u001b[0m\u001b[1;32m    924\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    925\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_check_unsampled_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/matplotlib/image.py\u001b[0m in \u001b[0;36m_make_image\u001b[0;34m(self, A, in_bbox, out_bbox, clip_bbox, magnification, unsampled, round_to_pixel_border)\u001b[0m\n\u001b[1;32m    547\u001b[0m                     self, A[..., 3], out_shape, t, alpha=alpha)\n\u001b[1;32m    548\u001b[0m                 output = _resample(  # resample rgb channels\n\u001b[0;32m--> 549\u001b[0;31m                     self, _rgb_to_rgba(A[..., :3]), out_shape, t, alpha=alpha)\n\u001b[0m\u001b[1;32m    550\u001b[0m                 \u001b[0moutput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m...\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput_alpha\u001b[0m  \u001b[0;31m# recombine rgb and alpha\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/matplotlib/image.py\u001b[0m in \u001b[0;36m_resample\u001b[0;34m(image_obj, data, out_shape, transform, resample, alpha)\u001b[0m\n\u001b[1;32m    195\u001b[0m                     \u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m                     \u001b[0mimage_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_filternorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m                     image_obj.get_filterrad())\n\u001b[0m\u001b[1;32m    198\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW4AAAD8CAYAAABXe05zAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/d3fzzAAAACXBIWXMAAAsTAAALEwEAmpwYAAAXDElEQVR4nO3df5BU5Z3v8fdn+DmgMAgjkhkSNCGKroI6IkbrBnWTq24qWKWrWFvL6FI1KcW9xrVqF++tMrWp/P6xrlZtsUskLm68MYpxpSzKH0GSm1sVNeMvEFAYBXaYADNEA15BkfC9f/QzpMGB6WZ6bJ6Zz6uqq8/5nud0P0+d4cPpp093KyIwM7N81FS7A2ZmVh4Ht5lZZhzcZmaZcXCbmWXGwW1mlhkHt5lZZvoluCVdIekNSW2SFvbHc5iZDVaq9HXckoYAG4AvAFuB3wI3RMS6ij6Rmdkg1R9n3DOBtoh4KyL2AQ8Bc/rheczMBqWh/fCYDUB70fpW4MLDG0lqAVoARo8eff4ZZ5zRD10xM8vT5s2b2blzp3ra1h/BXZKIWAwsBmhqaorW1tZqdcXM7LjT1NR0xG39MVXSAUwuWm9MNTMzq4D+CO7fAlMlnSppODAXWN4Pz2NmNihVfKokIvZLuhV4ChgC/Dgi1lb6eczMBqt+meOOiBXAiv54bDOzwc6fnDQzy4yD28wsMw5uM7PMOLjNzDLj4DYzy4yD28wsMw5uM7PMOLjNzDLj4DYzy4yD28wsMw5uM7PMOLjNzDLj4DYzy4yD28wsMw5uM7PMOLjNzDLj4DYzy4yD28wsM70Gt6QfS+qU9FpR7SRJz0jamO7Hpbok3SupTdJqSef1Z+fNzAajUs64/x244rDaQmBlREwFVqZ1gCuBqenWAiyqTDfNzKxbr8EdEf8HePuw8hxgaVpeClxdVH8gCp4D6iRNqlBfzcyMY5/jnhgR29LydmBiWm4A2ovabU01MzOrkD6/ORkRAUS5+0lqkdQqqbWrq6uv3TAzGzSONbh3dE+BpPvOVO8AJhe1a0y1j4iIxRHRFBFN9fX1x9gNM7PB51iDeznQnJabgceL6vPS1SWzgF1FUypmZlYBQ3trIOmnwGxggqStwNeA7wAPS5oPbAGuS81XAFcBbcAe4KZ+6LOZ2aDWa3BHxA1H2HR5D20DWNDXTpmZ2ZH5k5NmZplxcJuZZcbBbWaWGQe3mVlmHNxmZplxcJuZZcbBbWaWGQe3mVlmHNxmZplxcJuZZcbBbWaWGQe3mVlmHNxmZplxcJuZZcbBbWaWGQe3mVlmHNxmZplxcJuZZcbBbWaWmV6DW9JkSaskrZO0VtJtqX6SpGckbUz341Jdku6V1CZptaTz+nsQZmaDSSln3PuBOyLiTGAWsEDSmcBCYGVETAVWpnWAK4Gp6dYCLKp4r83MBrFegzsitkXES2n5XWA90ADMAZamZkuBq9PyHOCBKHgOqJM0qdIdNzMbrMqa45Y0BTgXeB6YGBHb0qbtwMS03AC0F+22NdUOf6wWSa2SWru6usrtt5nZoFVycEs6AXgU+GpE7C7eFhEBRDlPHBGLI6IpIprq6+vL2dXMbFArKbglDaMQ2g9GxM9TeUf3FEi670z1DmBy0e6NqWZmZhVQylUlApYA6yPin4o2LQea03Iz8HhRfV66umQWsKtoSsXMzPpoaAltLgb+Glgj6ZVU+5/Ad4CHJc0HtgDXpW0rgKuANmAPcFMlO2xmNtj1GtwR8X8BHWHz5T20D2BBH/tlZmZH4E9OmpllxsFtZpYZB7eZWWYc3GZmmXFwm5llxsFtZpYZB7eZWWYc3GZmmXFwm5llxsFtZpYZB7eZWWYc3GZmmXFwm5llxsFtZpYZB7eZWWYc3GZmmXFwm5llxsFtZpaZUn4seKSkFyS9KmmtpH9M9VMlPS+pTdLPJA1P9RFpvS1tn9LPYzAzG1RKOeP+ALgsIqYDM4Ar0q+3fxe4OyI+A7wDzE/t5wPvpPrdqZ2ZmVVIr8EdBf8vrQ5LtwAuA5al+lLg6rQ8J62Ttl8u6Ug/NmxmZmUqaY5b0hBJrwCdwDPAm8AfImJ/arIVaEjLDUA7QNq+Cxjfw2O2SGqV1NrV1dWnQZiZDSYlBXdE/DEiZgCNwEzgjL4+cUQsjoimiGiqr6/v68OZmQ0aZV1VEhF/AFYBFwF1koamTY1AR1ruACYDpO1jgd9XorNmZlbaVSX1kurSci3wBWA9hQC/NjVrBh5Py8vTOmn7sxERFeyzmdmgNrT3JkwClkoaQiHoH46IJyStAx6S9A3gZWBJar8E+A9JbcDbwNx+6LeZ2aDVa3BHxGrg3B7qb1GY7z68/j7wlxXpnZmZfYQ/OWlmlhkHt5lZZhzcZmaZcXCbmWXGwW1mlhkHt5lZZhzcZmaZcXCbmWXGwW1mlhkHt5lZZhzcZmaZcXCbmWXGwW1mlplSvtbVzMz6QUTw5ptvsmjRInbu3MlZZ51FS0sLdXV1R93PwW1mViXbt2/nmmuuYfXq1QBIYseOHXz/+98/6n6eKjEzq5Jf//rXrFmz5uB6RPDggw+ybdu2o+7n4DYzq5Kamo9G8JAhQ3qsH7Jff3XIzMyObvbs2cyaNevgek1NDddccw0nn3zyUfcreY47/eZkK9AREV+SdCrwEDAeeBH464jYJ2kE8ABwPoVfd78+IjaXOR4zswFv/PjxLFu2jAceeICdO3cybdo05s6dy5AhQ466XzlvTt5G4dfdx6T17wJ3R8RDkv4VmA8sSvfvRMRnJM1N7a4vd0BmZgOdJD7xiU+wcOHCsvYraapEUiPwF8B9aV3AZcCy1GQpcHVanpPWSdsvT+3NzKwCSp3j/mfg74EDaX088IeI2J/WtwINabkBaAdI23el9oeQ1CKpVVJrV1fXsfXezGwQ6jW4JX0J6IyIFyv5xBGxOCKaIqKpvr6+kg9tZjaglTLHfTHwZUlXASMpzHHfA9RJGprOqhuBjtS+A5gMbJU0FBhL4U1KMzOrgF7PuCPizohojIgpwFzg2Yj4K2AVcG1q1gw8npaXp3XS9mcjIiraazOzQawv13H/A/B3ktoozGEvSfUlwPhU/zugvLdLzczsqMr6rpKI+CXwy7T8FjCzhzbvA39Zgb6ZmVkP/MlJM7PMOLjNzDLj4DYzy4yD28wsMw5uM7PMOLjNzDLj4DYzy4yD28wsMw5uM7PMOLjNzDLj4DYzy4yD28wsMw5uM7PMOLjNzDLj4DYzy4yD28wsMw5uM7PMOLjNzDJTUnBL2ixpjaRXJLWm2kmSnpG0Md2PS3VJuldSm6TVks7rzwGYmQ025ZxxXxoRMyKiKa0vBFZGxFRgJX/6UeArganp1gIsqlRnzcysb1Mlc4ClaXkpcHVR/YEoeA6okzSpD89jZmZFSg3uAJ6W9KKkllSbGBHb0vJ2YGJabgDai/bdmmqHkNQiqVVSa1dX1zF03cxscBpaYrtLIqJD0snAM5JeL94YESEpynniiFgMLAZoamoqa18zs8GspDPuiOhI953AY8BMYEf3FEi670zNO4DJRbs3ppqZmVVAr8EtabSkE7uXgS8CrwHLgebUrBl4PC0vB+alq0tmAbuKplTMzKyPSpkqmQg8Jqm7/f+OiCcl/RZ4WNJ8YAtwXWq/ArgKaAP2ADdVvNdmZoNYr8EdEW8B03uo/x64vId6AAsq0jszM/sIf3LSzCwzDm4zs8w4uM3MMuPgNjPLjIPbzCwzDm4zs8yU+pF3MytTRLB3714+/PBDRo0axdChQ0mfhzDrE59xm/WDiGDZsmV8/vOf5+yzz2bu3Lm0t7dT+JiDWd/4jNusH6xfv54FCxbQ/c2X7e3t7N69m+XLl1NbW1vl3lnufMZt1g9ef/11du7ceUhtw4YN7Nu3r0o9soHEwW3WD0455RSGDRt2SK22tpYhQ4ZUqUc2kDi4zfrBjBkzuPrqqxk+fDgAY8aM4dZbb2X06NFV7pkNBJ7jNusHtbW13H///dx8882sWbOGSy+9lGnTpvmqEqsIB7dZP5DEqFGjmD17NrNnz652d2yA8VSJmVlmHNxmZplxcJuZZcZz3GYliggOHDjAnj17ePDBB1m9ejWnn346N954I2PHjq1292wQKSm4JdUB9wF/BgTwN8AbwM+AKcBm4LqIeEeFt83vofC7k3uAGyPipUp33Kyv/vjHP9LR0cH777/PlClTDl6615MDBw7wm9/8hm9961ts27aNV199lQMHDiCJl156iR/96EdH3d+skkqdKrkHeDIizqDw+5PrgYXAyoiYCqxM6wBXAlPTrQVYVNEem1XA7t27ueWWW7jggguYOXMmX/nKVz7yScdimzdv5tprr2XFihW8/PLLHDhwACichT/22GNs2bIFgP3797N3714++OAD1q5dy+LFi7nvvvvYtGmTv6fEKqbXM25JY4H/BtwIEBH7gH2S5gCzU7OlwC+BfwDmAA+kHw1+TlKdpEkRsa3ivTc7BhHBI488wn333XcwgJcuXcrEiRP59re/3eO11uvWraOzs7PHx9u/fz/79+9n06ZNLFy4kLVr1zJ69Gi2bNnCjh07APjsZz/LU089xZQpU/ptXDZ4lDJVcirQBdwvaTrwInAbMLEojLcDE9NyA9BetP/WVHNw23EhIvjVr351MLS7axs2bDjiPieeeCI1NTWH7NNt2LBhLFmyhJdffplnn322x/03bNjAkiVL+PrXv+4P4ViflTJVMhQ4D1gUEecC7/GnaREA0tl1Wa8DJbVIapXU2v0NamYfB0lMnz79IwH6yU9+8oj7zJgxg4suuujgPrW1tXz605+mpqaG3bt388Mf/vCIod2to6Oj7503o7Qz7q3A1oh4Pq0voxDcO7qnQCRNArpfR3YAk4v2b0y1Q0TEYmAxQFNTkyf/7GMjiXnz5vH0008fDNtZs2Zx++23H/FseMyYMSxbtownn3ySPXv28KlPfYr33nuP66+/vqTnHDVqFM3NzRUbgw1uvQZ3RGyX1C7p9Ih4A7gcWJduzcB30v3jaZflwK2SHgIuBHZ5ftuONxMmTOCRRx5hzZo1SOKss85izJgxR2wviZNPPpl58+YdrC1btqzHqZPu9meffTaSqK2tpaWlhYsvvtjTJFYRpV7H/bfAg5KGA28BN1GYZnlY0nxgC3BdaruCwqWAbRQuB7ypoj02qwBJjBkzhosvvviQekTQ2dlJZ2cntbW1TJkyhaFDe/5nMm3aNCZMmHDI1SjXX389F1xwAaeddhqXXnopw4YNo6amhpEjRzq0rWJKCu6IeAVo6mHT5T20DWBB37pl9vGLCH7xi19w880309HRwciRI7nlllu46667GDFixEfaT5s2jZ/85Cd873vfY/PmzVx22WV84xvfYOLEiT08ulnl6Hi4trSpqSlaW1ur3Q0b5Pbu3cvs2bN54YUXDtZGjhzJqlWrmDVrVo/7RAT79u3jgw8+4IQTTkCSz6ytIpqammhtbe3xj8nfVWKWvP3227S1tR1Se//993n11VePuI8kRowYwZgxY6ipqXFo28fCwW2WTJgwgbPOOuuQ2qhRozj99NOr1COznjm4zZLhw4fzgx/8gPPPP59x48bR2NjIN7/5TS655JJqd83sEP52QLNEEhdccAGrVq1i9+7dDB8+nPHjx1NTU0NEsHv3btrb22loaKCurs7TIlY1PuM2KyKJE088kYaGBurr6w+G9tNPP83nPvc5Zs6cyYUXXsijjz7qL42yqnFwm/Vi165d3H777axbt469e/eyceNG7rjjDrZv317trtkg5eA268W7777L7373u0Nq27dvP/hVrmYfNwe3WS9qa2upq6s7pDZ27FhOOeWU6nTIBj0Ht1kvxo8fz1133XUwvE844QTuvPPOo36boFl/8lUlZr2QRHNzM01NTWzatInGxkamT59OTY3Pe6w6HNxmJRgyZAjnnHMO55xzTrW7YuapEjOz3Di4zcwy4+A2M8uMg9vMLDMObjOzzDi4zcwy02twSzpd0itFt92SvirpJEnPSNqY7sel9pJ0r6Q2Saslndf/wzAzGzx6De6IeCMiZkTEDOB8Cj8A/BiwEFgZEVOBlWkd4Epgarq1AIv6od9mZoNWuVMllwNvRsQWYA6wNNWXAlen5TnAA1HwHFAnaVIlOmtmZuUH91zgp2l5YkRsS8vbge6ftm4A2ov22ZpqZmZWASUHt6ThwJeBRw7fFoVvlC/rW+UltUhqldTa1dVVzq5mZoNaOWfcVwIvRcSOtL6jewok3XemegcwuWi/xlQ7REQsjoimiGiqr68vv+dmZoNUOcF9A3+aJgFYDjSn5Wbg8aL6vHR1ySxgV9GUipmZ9VFJ3w4oaTTwBeArReXvAA9Lmg9sAa5L9RXAVUAbhStQbqpYb83MrLTgjoj3gPGH1X5P4SqTw9sGsKAivTMzs4/wJyfNzDLj4DYzy4yD28wsMw5uM7PMOLjNzDLj4DYzy4yD28wsMw5uM7PMOLjNzDLj4DYzy4yD28wsMw5uM7PMOLjNzDLj4DYzy4yD28wsMw5uM7PMOLjNzDLj4DYzy4yD28wsMw5uM7PMOLjNzDLj4DYzy4wiotp9QNK7wBvV7kc/mQDsrHYn+oHHlZ+BOraBOq5PRUR9TxuGftw9OYI3IqKp2p3oD5JaB+LYPK78DNSxDdRxHY2nSszMMuPgNjPLzPES3Iur3YF+NFDH5nHlZ6CObaCO64iOizcnzcysdMfLGbeZmZXIwW1mlpmqB7ekKyS9IalN0sJq96cckiZLWiVpnaS1km5L9ZMkPSNpY7ofl+qSdG8a62pJ51V3BEcnaYiklyU9kdZPlfR86v/PJA1P9RFpvS1tn1LVjvdCUp2kZZJel7Re0kUD4ZhJuj39Hb4m6aeSRuZ6zCT9WFKnpNeKamUfI0nNqf1GSc3VGEt/qGpwSxoC/AtwJXAmcIOkM6vZpzLtB+6IiDOBWcCC1P+FwMqImAqsTOtQGOfUdGsBFn38XS7LbcD6ovXvAndHxGeAd4D5qT4feCfV707tjmf3AE9GxBnAdApjzPqYSWoA/gfQFBF/BgwB5pLvMft34IrDamUdI0knAV8DLgRmAl/rDvvsRUTVbsBFwFNF63cCd1azT30cz+PAFyh8CnRSqk2i8AEjgH8Dbihqf7Dd8XYDGin847gMeAIQhU+nDT382AFPARel5aGpnao9hiOMayyw6fD+5X7MgAagHTgpHYMngP+e8zEDpgCvHesxAm4A/q2ofki7nG/Vnirp/mPrtjXVspNeap4LPA9MjIhtadN2YGJazmm8/wz8PXAgrY8H/hAR+9N6cd8Pjitt35XaH49OBbqA+9M00H2SRpP5MYuIDuAHwH8B2ygcgxcZGMesW7nHKItjdyyqHdwDgqQTgEeBr0bE7uJtUfivPqtrLiV9CeiMiBer3Zd+MBQ4D1gUEecC7/Gnl9xAtsdsHDCHwn9MnwBG89GphgEjx2NUSdUO7g5gctF6Y6plQ9IwCqH9YET8PJV3SJqUtk8COlM9l/FeDHxZ0mbgIQrTJfcAdZK6v9+muO8Hx5W2jwV+/3F2uAxbga0R8XxaX0YhyHM/Zn8ObIqIroj4EPg5heM4EI5Zt3KPUS7HrmzVDu7fAlPTO9/DKbyZsrzKfSqZJAFLgPUR8U9Fm5YD3e9gN1OY++6uz0vvgs8CdhW99DtuRMSdEdEYEVMoHJNnI+KvgFXAtanZ4ePqHu+1qf1xeTYUEduBdkmnp9LlwDoyP2YUpkhmSRqV/i67x5X9MStS7jF6CviipHHpFckXUy1/1Z5kB64CNgBvAv+r2v0ps++XUHi5thp4Jd2uojBXuBLYCPwCOCm1F4WraN4E1lC4AqDq4+hljLOBJ9LyacALQBvwCDAi1Uem9ba0/bRq97uXMc0AWtNx+09g3EA4ZsA/Aq8DrwH/AYzI9ZgBP6UwV/8hhVdJ84/lGAF/k8bYBtxU7XFV6uaPvJuZZabaUyVmZlYmB7eZWWYc3GZmmXFwm5llxsFtZpYZB7eZWWYc3GZmmfn/ILGqFg1ngvEAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "h2test = plot_test(agent2, env, \n",
    "          fnames=['RewardAgent_acc_test1', 'RewardAgent_acc_test2'], \n",
    "          num_iteration=num_iteration, action_space=action_space, imdir='screencaps/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Took  32  steps to converge\n",
      "Finished episode  0  with  32 ;\n",
      "cumulative reward =  -3354.8674759449877\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW4AAAD8CAYAAABXe05zAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/d3fzzAAAACXBIWXMAAAsTAAALEwEAmpwYAAAR1UlEQVR4nO3dYYyd1X3n8e9vcSBdusvYdGp5bWuhxAoiKwXcETVKterGmyzQKuZFikDVYrGWvEhsm7SVus7ui6jSvkikVWmQVqhWnNZU2SSEJsVCKAl1iKq+gGTSsIRAKBMKtS3AEwpOm6ilpP99cc8kF2Mz93ruZHx8vx/p6p7nf86de44e8+OZM8+dSVUhSerHv1jrCUiSxmNwS1JnDG5J6ozBLUmdMbglqTMGtyR1ZlWCO8m1SZ5KspBk32q8hyRNq0z6Pu4k5wF/BbwHOAp8Dbi5qp6Y6BtJ0pRajSvuq4GFqnqmql4FPg3sWoX3kaSptG4VvuZm4MjQ8VHgF04elGQvsBfgwgsv/PnLL798FaYiSX169tln+e53v5tT9a1GcI+kqvYD+wHm5uZqfn5+raYiSWedubm50/atxlbJMWDr0PGWVpMkTcBqBPfXgG1JLk1yPnATcGgV3keSptLEt0qq6rUk/w34InAe8Imq+tak30eSptWq7HFX1QPAA6vxtSVp2vnJSUnqjMEtSZ0xuCWpMwa3JHXG4Jaks8wPf/hDXn311dP2G9ySdBZ56aWXuO2223jiidP/Xr41+8i7JOn1qoq7776bAwcO8Ga/udUrbkk6S1QVX/rSl940tMHglqSzRhLe8Y53LDvO4Jaks0QSbrvtNrZv3/6m4wxuSTqLXHbZZRw+fJjLLrvstGMMbkk6iyRhZmaGmZmZ044xuCWpMwa3JHXG4JakzhjcktQZg1uSOmNwS1Jnlg3uJJ9IcjzJ40O1DUkeTPJ0e17f6klyZ5KFJI8lefO7yCVJYxvlivuPgGtPqu0DDlfVNuBwOwa4DtjWHnuBuyYzTUnSkmWDu6r+HPjbk8q7gIOtfRC4Yah+dw08DMwk2TShuUqSOPM97o1V9XxrvwBsbO3NwJGhcUdbTZI0ISv+4WQNfv/gm/8OwlNIsjfJfJL5xcXFlU5DkqbGmQb3i0tbIO35eKsfA7YOjdvSam9QVfuraq6q5mZnZ89wGpI0fc40uA8Bu1t7N3DfUP2WdnfJDuDE0JaKJGkClv3TZUk+BfwS8DNJjgIfBj4C3JNkD/AccGMb/gBwPbAA/AC4dRXmLElTbdngrqqbT9O18xRjC7h9pZOSJJ2en5yUpM4Y3JLUGYNbkjpjcEtSZwxuSeqMwS1JnTG4JakzBrckdcbglqTOGNyS1BmDW5I6Y3BLUmcMbknqjMEtSZ0xuCWpMwa3JHXG4JakzhjcktQZg1uSOrNscCfZmuShJE8k+VaSD7T6hiQPJnm6Pa9v9SS5M8lCkseSbF/tRUjSNBnlivs14Ler6gpgB3B7kiuAfcDhqtoGHG7HANcB29pjL3DXxGctSVNs2eCuquer6i9b+++AJ4HNwC7gYBt2ELihtXcBd9fAw8BMkk2TnrgkTaux9riTXAJcBTwCbKyq51vXC8DG1t4MHBl62dFWO/lr7U0yn2R+cXFx3HlL0tQaObiT/DTwJ8AHq+p7w31VVUCN88ZVtb+q5qpqbnZ2dpyXStJUGym4k7yFQWh/sqo+18ovLm2BtOfjrX4M2Dr08i2tJkmagFHuKglwAHiyqn5vqOsQsLu1dwP3DdVvaXeX7ABODG2pSJJWaN0IY94F/Gfgm0kebbX/AXwEuCfJHuA54MbW9wBwPbAA/AC4dZITlqRpt2xwV9VfADlN985TjC/g9hXOS5J0Gn5yUpI6Y3BLUmcMbknqjMEtSZ0xuCWpMwa3JHXG4JakzhjcktQZg1uSOmNwS1JnDG5J6ozBLUmdMbglqTMGtyR1xuCWpM4Y3JLUGYNbkjpjcEtSZ0b5Y8FvTfLVJP8vybeS/G6rX5rkkSQLST6T5PxWv6AdL7T+S1Z5DZI0VUa54v5H4N1V9U7gSuDa9tfbPwrcUVVvA14G9rTxe4CXW/2ONk6SNCHLBncN/H07fEt7FPBu4N5WPwjc0Nq72jGtf2eS0/2xYUnSmEba405yXpJHgePAg8B3gFeq6rU25CiwubU3A0cAWv8J4OJTfM29SeaTzC8uLq5oEZI0TUYK7qr6YVVdCWwBrgYuX+kbV9X+qpqrqrnZ2dmVfjlJmhpj3VVSVa8ADwHXADNJ1rWuLcCx1j4GbAVo/RcBL01ispKk0e4qmU0y09o/BbwHeJJBgL+/DdsN3Nfah9oxrf/LVVUTnLMkTbV1yw9hE3AwyXkMgv6eqro/yRPAp5P8L+AbwIE2/gDwx0kWgL8FblqFeUvS1Fo2uKvqMeCqU9SfYbDffXL9H4BfncjsJElv4CcnJakzBrckdcbglqTOGNyS1BmDW5I6Y3BLUmcMbknqjMEtSZ0xuCWpMwa3JHXG4JakzhjcktQZg1uSOmNwS1JnDG5J6ozBLUmdMbglqTMGtyR1ZuTgTnJekm8kub8dX5rkkSQLST6T5PxWv6AdL7T+S1Zp7pI0lca54v4Ag7/uvuSjwB1V9TbgZWBPq+8BXm71O9o4SdKEjBTcSbYAvwx8vB0HeDdwbxtyELihtXe1Y1r/zjZekjQBo15x/z7wO8A/t+OLgVeq6rV2fBTY3NqbgSMArf9EG/86SfYmmU8yv7i4eGazl6QptGxwJ/kV4HhVfX2Sb1xV+6tqrqrmZmdnJ/mlJemctm6EMe8C3pfkeuCtwL8GPgbMJFnXrqq3AMfa+GPAVuBoknXARcBLE5+5JE2pZa+4q+pDVbWlqi4BbgK+XFW/BjwEvL8N2w3c19qH2jGt/8tVVROdtSRNsZXcx/3fgd9KssBgD/tAqx8ALm713wL2rWyKkqRho2yV/EhVfQX4Sms/A1x9ijH/APzqBOYmSToFPzkpSZ0xuCWpMwa3JHXG4JakzhjcktQZg1uSOmNwS1JnDG5J6ozBLUmdMbglqTMGtyR1xuCWpM4Y3JLUGYNbkjpjcEtSZwxuSeqMwS1JnTG4JakzIwV3kmeTfDPJo0nmW21DkgeTPN2e17d6ktyZZCHJY0m2r+YCJGnajHPF/R+q6sqqmmvH+4DDVbUNOMyP/yjwdcC29tgL3DWpyUqSVrZVsgs42NoHgRuG6nfXwMPATJJNK3gfSdKQUYO7gC8l+XqSva22saqeb+0XgI2tvRk4MvTao632Okn2JplPMr+4uHgGU5ek6bRuxHG/WFXHkvws8GCSbw93VlUlqXHeuKr2A/sB5ubmxnqtJE2zka64q+pYez4OfB64GnhxaQukPR9vw48BW4devqXVJEkTsGxwJ7kwyb9aagPvBR4HDgG727DdwH2tfQi4pd1dsgM4MbSlIklaoVG2SjYCn0+yNP7/VtUXknwNuCfJHuA54MY2/gHgemAB+AFw68RnLUlTbNngrqpngHeeov4SsPMU9QJun8jsJElv4CcnJakzBrckdcbglqTOGNyS1BmDW5I6Y3BLUmcMbknqjMEtSZ0xuCWpMwa3JHXG4JakzhjcktQZg1uSOmNwS1JnDG5J6ozBLUmdMbglqTMGtyR1ZqTgTjKT5N4k307yZJJrkmxI8mCSp9vz+jY2Se5MspDksSTbV3cJkjRdRr3i/hjwhaq6nMHfn3wS2AccrqptwOF2DHAdsK099gJ3TXTGkjTllg3uJBcB/x44AFBVr1bVK8Au4GAbdhC4obV3AXfXwMPATJJNE563JE2tUa64LwUWgT9M8o0kH09yIbCxqp5vY14ANrb2ZuDI0OuPtpokaQJGCe51wHbgrqq6Cvg+P94WAaCqCqhx3jjJ3iTzSeYXFxfHeakkTbVRgvsocLSqHmnH9zII8heXtkDa8/HWfwzYOvT6La32OlW1v6rmqmpudnb2TOcvSVNn2eCuqheAI0ne3ko7gSeAQ8DuVtsN3Nfah4Bb2t0lO4ATQ1sqkqQVWjfiuF8HPpnkfOAZ4FYGoX9Pkj3Ac8CNbewDwPXAAvCDNlaSNCEjBXdVPQrMnaJr5ynGFnD7yqYlSTodPzkpSZ0xuCWpMwa3JHXG4JakzhjcktQZg1uSOmNwS1JnDG5J6ozBLUmdMbglqTMGtyR1xuCWpM4Y3JLUGYNbkjpjcEtSZwxuSeqMwS1JnTG4JakzywZ3krcneXTo8b0kH0yyIcmDSZ5uz+vb+CS5M8lCkseSbF/9ZUjS9Bjlr7w/VVVXVtWVwM8z+APAnwf2AYerahtwuB0DXAdsa4+9wF2rMG9JmlrjbpXsBL5TVc8Bu4CDrX4QuKG1dwF318DDwEySTZOYrCRp/OC+CfhUa2+squdb+wVgY2tvBo4MveZoq0mSJmDk4E5yPvA+4LMn91VVATXOGyfZm2Q+yfzi4uI4L5WkqTbOFfd1wF9W1Yvt+MWlLZD2fLzVjwFbh163pdVep6r2V9VcVc3Nzs6OP3NJmlLjBPfN/HibBOAQsLu1dwP3DdVvaXeX7ABODG2pSJJWaN0og5JcCLwH+K9D5Y8A9yTZAzwH3NjqDwDXAwsM7kC5dWKzlSSNFtxV9X3g4pNqLzG4y+TksQXcPpHZSZLewE9OSlJnDG5J6ozBLUmdMbglqTMGtyR1xuCWpM4Y3JLUGYNbkjpjcEtSZwxuSeqMwS1JnTG4JakzBrckdcbglqTOGNyS1BmDW5I6Y3BLUmcMbknqjMEtSZ0xuCWpMwa3JHXG4JakzqSq1noOJPk74Km1nscq+Rngu2s9iVXguvpzrq7tXF3Xv62q2VN1rPtJz+Q0nqqqubWexGpIMn8urs119edcXdu5uq4341aJJHXG4Jakzpwtwb1/rSewis7Vtbmu/pyraztX13VaZ8UPJyVJoztbrrglSSMyuCWpM2se3EmuTfJUkoUk+9Z6PuNIsjXJQ0meSPKtJB9o9Q1JHkzydHte3+pJcmdb62NJtq/tCt5ckvOSfCPJ/e340iSPtPl/Jsn5rX5BO15o/Zes6cSXkWQmyb1Jvp3kySTXnAvnLMlvtn+Hjyf5VJK39nrOknwiyfEkjw/Vxj5HSXa38U8n2b0Wa1kNaxrcSc4D/g9wHXAFcHOSK9ZyTmN6DfjtqroC2AHc3ua/DzhcVduAw+0YBuvc1h57gbt+8lMeyweAJ4eOPwrcUVVvA14G9rT6HuDlVr+jjTubfQz4QlVdDryTwRq7PmdJNgO/AcxV1b8DzgNuot9z9kfAtSfVxjpHSTYAHwZ+Abga+PBS2HevqtbsAVwDfHHo+EPAh9ZyTitcz33Aexh8CnRTq21i8AEjgD8Abh4a/6NxZ9sD2MLgP453A/cDYfDptHUnnzvgi8A1rb2ujctar+E067oI+OuT59f7OQM2A0eADe0c3A/8p57PGXAJ8PiZniPgZuAPhuqvG9fzY623Spb+sS052mrdad9qXgU8Amysqudb1wvAxtbuab2/D/wO8M/t+GLglap6rR0Pz/1H62r9J9r4s9GlwCLwh20b6ONJLqTzc1ZVx4D/DfwN8DyDc/B1zo1ztmTcc9TFuTsTax3c54QkPw38CfDBqvrecF8N/lff1T2XSX4FOF5VX1/ruayCdcB24K6qugr4Pj/+lhvo9pytB3Yx+B/TvwEu5I1bDeeMHs/RJK11cB8Dtg4db2m1biR5C4PQ/mRVfa6VX0yyqfVvAo63ei/rfRfwviTPAp9msF3yMWAmydLvtxme+4/W1fovAl76SU54DEeBo1X1SDu+l0GQ937O/iPw11W1WFX/BHyOwXk8F87ZknHPUS/nbmxrHdxfA7a1n3yfz+CHKYfWeE4jSxLgAPBkVf3eUNchYOkn2LsZ7H0v1W9pPwXfAZwY+tbvrFFVH6qqLVV1CYNz8uWq+jXgIeD9bdjJ61pa7/vb+LPyaqiqXgCOJHl7K+0EnqDzc8Zgi2RHkn/Z/l0urav7czZk3HP0ReC9Sda370je22r9W+tNduB64K+A7wD/c63nM+bcf5HBt2uPAY+2x/UM9goPA08DfwZsaOPD4C6a7wDfZHAHwJqvY5k1/hJwf2v/HPBVYAH4LHBBq7+1HS+0/p9b63kvs6Yrgfl23v4UWH8unDPgd4FvA48Dfwxc0Os5Az7FYK/+nxh8l7TnTM4R8F/aGheAW9d6XZN6+JF3SerMWm+VSJLGZHBLUmcMbknqjMEtSZ0xuCWpMwa3JHXG4Jakzvx/FSPnqLQtpwcAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "h3test = plot_test(agent3, env, \n",
    "          fnames=['RewardActionAgent_acc_test1'], \n",
    "          num_iteration=num_iteration, action_space=action_space, imdir='screencaps/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished episode  0  with  199 ;\n",
      "cumulative reward =  -34823.41160897509\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-600226a89e75>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m h4test = plot_test(agent4, env2, \n\u001b[1;32m      2\u001b[0m           \u001b[0mfnames\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'screencaps/LearnerAgent_acc_oacc_test1'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'screencaps/LearnerAgent_acc_oacc_test2'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m           num_iteration=num_iteration, action_space=action_space)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-25-f9bba8966705>\u001b[0m in \u001b[0;36mplot_test\u001b[0;34m(agent, env, fnames, num_iteration, action_space)\u001b[0m\n\u001b[1;32m     32\u001b[0m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"rgb_array\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m             \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m             \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msavefig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'-{:03d}.jpg'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m             \u001b[0msteps\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36msavefig\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    842\u001b[0m     \u001b[0mfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgcf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    843\u001b[0m     \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msavefig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 844\u001b[0;31m     \u001b[0mfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcanvas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw_idle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m   \u001b[0;31m# need this if 'transparent=True' to reset colors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    845\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    846\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/matplotlib/backend_bases.py\u001b[0m in \u001b[0;36mdraw_idle\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2010\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_idle_drawing\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2011\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_idle_draw_cntx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2012\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2013\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2014\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mcbook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeprecated\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"3.2\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/matplotlib/backends/backend_agg.py\u001b[0m in \u001b[0;36mdraw\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    405\u001b[0m              (self.toolbar._wait_cursor_for_draw_cm() if self.toolbar\n\u001b[1;32m    406\u001b[0m               else nullcontext()):\n\u001b[0;32m--> 407\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    408\u001b[0m             \u001b[0;31m# A GUI class may be need to update a window using this draw, so\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    409\u001b[0m             \u001b[0;31m# don't forget to call the superclass.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/matplotlib/artist.py\u001b[0m in \u001b[0;36mdraw_wrapper\u001b[0;34m(artist, renderer, *args, **kwargs)\u001b[0m\n\u001b[1;32m     39\u001b[0m                 \u001b[0mrenderer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0martist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrenderer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0martist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_agg_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/matplotlib/figure.py\u001b[0m in \u001b[0;36mdraw\u001b[0;34m(self, renderer)\u001b[0m\n\u001b[1;32m   1862\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1863\u001b[0m             mimage._draw_list_compositing_images(\n\u001b[0;32m-> 1864\u001b[0;31m                 renderer, self, artists, self.suppressComposite)\n\u001b[0m\u001b[1;32m   1865\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1866\u001b[0m             \u001b[0mrenderer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose_group\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'figure'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/matplotlib/image.py\u001b[0m in \u001b[0;36m_draw_list_compositing_images\u001b[0;34m(renderer, parent, artists, suppress_composite)\u001b[0m\n\u001b[1;32m    129\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mnot_composite\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhas_images\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0martists\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m             \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m         \u001b[0;31m# Composite any adjacent images together\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/matplotlib/artist.py\u001b[0m in \u001b[0;36mdraw_wrapper\u001b[0;34m(artist, renderer, *args, **kwargs)\u001b[0m\n\u001b[1;32m     39\u001b[0m                 \u001b[0mrenderer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0martist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrenderer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0martist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_agg_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/matplotlib/cbook/deprecation.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*inner_args, **inner_kwargs)\u001b[0m\n\u001b[1;32m    409\u001b[0m                          \u001b[0;32melse\u001b[0m \u001b[0mdeprecation_addendum\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    410\u001b[0m                 **kwargs)\n\u001b[0;32m--> 411\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minner_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0minner_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    412\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    413\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36mdraw\u001b[0;34m(self, renderer, inframe)\u001b[0m\n\u001b[1;32m   2746\u001b[0m             \u001b[0mrenderer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_rasterizing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2747\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2748\u001b[0;31m         \u001b[0mmimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_draw_list_compositing_images\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0martists\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2749\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2750\u001b[0m         \u001b[0mrenderer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose_group\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'axes'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/matplotlib/image.py\u001b[0m in \u001b[0;36m_draw_list_compositing_images\u001b[0;34m(renderer, parent, artists, suppress_composite)\u001b[0m\n\u001b[1;32m    129\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mnot_composite\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhas_images\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0martists\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m             \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m         \u001b[0;31m# Composite any adjacent images together\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/matplotlib/artist.py\u001b[0m in \u001b[0;36mdraw_wrapper\u001b[0;34m(artist, renderer, *args, **kwargs)\u001b[0m\n\u001b[1;32m     39\u001b[0m                 \u001b[0mrenderer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0martist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrenderer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0martist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_agg_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/matplotlib/image.py\u001b[0m in \u001b[0;36mdraw\u001b[0;34m(self, renderer, *args, **kwargs)\u001b[0m\n\u001b[1;32m    636\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    637\u001b[0m             im, l, b, trans = self.make_image(\n\u001b[0;32m--> 638\u001b[0;31m                 renderer, renderer.get_image_magnification())\n\u001b[0m\u001b[1;32m    639\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mim\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    640\u001b[0m                 \u001b[0mrenderer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/matplotlib/image.py\u001b[0m in \u001b[0;36mmake_image\u001b[0;34m(self, renderer, magnification, unsampled)\u001b[0m\n\u001b[1;32m    921\u001b[0m                 else self.figure.bbox)\n\u001b[1;32m    922\u001b[0m         return self._make_image(self._A, bbox, transformed_bbox, clip,\n\u001b[0;32m--> 923\u001b[0;31m                                 magnification, unsampled=unsampled)\n\u001b[0m\u001b[1;32m    924\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    925\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_check_unsampled_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/matplotlib/image.py\u001b[0m in \u001b[0;36m_make_image\u001b[0;34m(self, A, in_bbox, out_bbox, clip_bbox, magnification, unsampled, round_to_pixel_border)\u001b[0m\n\u001b[1;32m    545\u001b[0m                 \u001b[0malpha\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_scalar_alpha\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m                 output_alpha = _resample(  # resample alpha channel\n\u001b[0;32m--> 547\u001b[0;31m                     self, A[..., 3], out_shape, t, alpha=alpha)\n\u001b[0m\u001b[1;32m    548\u001b[0m                 output = _resample(  # resample rgb channels\n\u001b[1;32m    549\u001b[0m                     self, _rgb_to_rgba(A[..., :3]), out_shape, t, alpha=alpha)\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/matplotlib/image.py\u001b[0m in \u001b[0;36m_resample\u001b[0;34m(image_obj, data, out_shape, transform, resample, alpha)\u001b[0m\n\u001b[1;32m    195\u001b[0m                     \u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m                     \u001b[0mimage_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_filternorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m                     image_obj.get_filterrad())\n\u001b[0m\u001b[1;32m    198\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "h4test = plot_test(agent4, env2, \n",
    "          fnames=['LearnerAgent_acc_oacc_test1', 'LearnerAgent_acc_oacc_test2'], \n",
    "          num_iteration=num_iteration, action_space=action_space, imdir='screencaps/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plots out reward history data\n",
    "def plot_reward_hist(reward_hists=[], ep_int=25, hist_names=[], log=True):\n",
    "    # reward_hist : List of histories of reward histories\n",
    "    # ep_int : number of episodes / size of intervals between two history lists\n",
    "    # reward_hist = [ [ [ [agent 1's reward history for 1st run in ep=0],\n",
    "    #                     [agent 1's reward history for 2nd run in ep=0],... ],\n",
    "    #                   [ [agent 1's reward history for 1st run in ep=25],\n",
    "    #                     [agent 1's reward history for 2nd run in ep=25],... ],... ],\n",
    "    #                 [ same bunch of lists for agent 2 ], ...  ]\n",
    "    fig, (ax1, ax2) = plt.subplots(2)\n",
    "    fig.suptitle('Top: Mean reward; Bottom: Iteration before done')\n",
    "    \n",
    "    for i,hhh in enumerate(reward_hists):\n",
    "        num_ep = np.arange(len(hhh))*ep_int\n",
    "        if log:\n",
    "            re_avg = [ np.log(-1/np.mean([sum(h) for h in hh])) for hh in hhh ] # Mean of cumulative rewards for each ep\n",
    "        else:\n",
    "            re_avg = [ np.mean([sum(h) for h in hh]) for hh in hhh ] # Mean of cumulative rewards for each ep\n",
    "        it_avg = [ np.mean([len(h) for h in hh]) for hh in hhh ] # Total number of iteartions for each ep\n",
    "        \n",
    "        wid = 1 / len(hist_names) * 0.8 * ep_int\n",
    "        offset = i * wid - 0.4 * ep_int\n",
    "        ax1.bar(num_ep+offset, re_avg, label=hist_names[i], width=wid)\n",
    "        ax2.bar(num_ep+offset, it_avg, label=hist_names[i], width=wid)\n",
    "#     ax2.title.set_text('# of episodes trained')\n",
    "    ax1.set_ylabel('Reward history (average)')\n",
    "    ax2.set_ylabel('Reward history (average)')\n",
    "#     ax1.set_xlabel('Reward')\n",
    "    ax2.set_xlabel('# of episodes trained')\n",
    "    ax2.legend()\n",
    "    ax1.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_reward_hist([a1hist, a2hist, a3hist, a4hist], 25, ['RA-vel', 'RA-acc','RAA','LA'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_reward_hist([a2hist, a3hist, a4hist], 25, ['RA','RAA','LA'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 2\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "def testfunc(a,b,**k):\n",
    "    print(a,b)\n",
    "    c = k.get('c',10)\n",
    "    print(c)\n",
    "\n",
    "testfunc(1,2,**{'c':9})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Copy some of the methods from the original documentatoin here, because useful\n",
    "BATCH_SIZE = 128\n",
    "GAMMA = 0.999\n",
    "EPS_START = 0.9\n",
    "EPS_END = 0.05\n",
    "EPS_DECAY = 100\n",
    "TARGET_UPDATE = 10\n",
    "n_actions = 5\n",
    "\n",
    "# policy_net = DQN(screen_height, screen_width, n_actions).to(device)\n",
    "# target_net = DQN(screen_height, screen_width, n_actions).to(device)\n",
    "# target_net.load_state_dict(policy_net.state_dict())\n",
    "# target_net.eval()\n",
    "\n",
    "# Throws a coin to decide whether to randomly sample or to choose according to reward.\n",
    "# Coin prob will change over time.\n",
    "# This method should be called for each individual agent.\n",
    "def select_action(state, steps_done, rand=True):\n",
    "    sample = random.random()\n",
    "    eps_threshold = EPS_END + (EPS_START - EPS_END) * \\\n",
    "        math.exp(-1. * steps_done / EPS_DECAY)\n",
    "    if sample > eps_threshold or (not rand):\n",
    "        with torch.no_grad():\n",
    "            # t.max(1) will return largest column value of each row (sample?).\n",
    "            # second column on max result is index of where max element was\n",
    "            # found, so we pick action with the larger expected reward.\n",
    "#             print(policy_net(torch.tensor([state])))\n",
    "#             print( policy_net(torch.tensor([state])).max(0))\n",
    "#             print(policy_net(torch.tensor([state])).max(0)[1])\n",
    "#             return policy_net(state).max(1)[1].view(1, 1)\n",
    "            return policy_net(state.view(1,-1,N)).max(1)[1].view(1, 1)\n",
    "    else:\n",
    "        return torch.tensor([[random.randrange(n_actions)]], device=device, dtype=torch.long)\n",
    "#         return torch.tensor([[random.randrange(n_actions) for i in range(state.shape[0])]], device=device, dtype=torch.long)\n",
    "\n",
    "# Steps over gradients from memory replay\n",
    "def optimize_model(batch):\n",
    "    # Compute a mask of non-final states and concatenate the batch elements\n",
    "    # (a final state would've been the one after which simulation ended)\n",
    "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n",
    "                                          batch.next_state)), device=device, dtype=torch.bool)\n",
    "    non_final_next_states = torch.cat([s for s in batch.next_state\n",
    "                                                if s is not None])\n",
    "    state_batch = torch.cat(batch.state)\n",
    "#     action_batch = torch.cat(torch.from_numpy(np.asarray(batch.action)))\n",
    "    action_batch = torch.from_numpy(np.asarray(batch.action))\n",
    "#     reward_batch = torch.cat(torch.from_numpy(batch.reward))\n",
    "    reward_batch = torch.from_numpy(np.asarray(batch.reward).astype('float32'))\n",
    "\n",
    "    # Compute Q(s_t, a) - the model computes Q(s_t), then we select the\n",
    "    # columns of actions taken. These are the actions which would've been taken\n",
    "    # for each batch state according to policy_net\n",
    "#     print(policy_net(state_batch.view(BATCH_SIZE, -1, N)).shape)\n",
    "#     print(action_batch.shape)\n",
    "    state_action_values = policy_net(state_batch.view(BATCH_SIZE, -1, N)).gather(1, action_batch.view(-1,1)) # gather() Gathers values along axis=1, indexed by action_batch.\n",
    "#     state_action_values = policy_net(state_batch).gather(1, action_batch) # gather() Gathers values along axis=1, indexed by action_batch.\n",
    "\n",
    "    # Compute V(s_{t+1}) for all next states.\n",
    "    # Expected values of actions for non_final_next_states are computed based\n",
    "    # on the \"older\" target_net; selecting their best reward with max(1)[0].\n",
    "    # This is merged based on the mask, such that we'll have either the expected\n",
    "    # state value or 0 in case the state was final.\n",
    "    next_state_values = torch.zeros(BATCH_SIZE, device=device)\n",
    "    \n",
    "    policy_net.eval()\n",
    "#     print(next_state_values[non_final_mask].shape)\n",
    "#     print(policy_net(non_final_next_states.view(BATCH_SIZE, 2, -1)).shape,\n",
    "#           policy_net(non_final_next_states.view(BATCH_SIZE, 2, -1)).max(1))\n",
    "    next_state_values[non_final_mask] = policy_net(non_final_next_states.view(BATCH_SIZE, 2, -1)).max(1)[0].detach()\n",
    "#     next_state_values[non_final_mask] = target_net(non_final_next_states).max(1)[0].detach()\n",
    "    # Compute the expected Q values\n",
    "    expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
    "    policy_net.train()\n",
    "\n",
    "    # Compute Huber loss\n",
    "    loss = F.smooth_l1_loss(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "#     print(state_action_values.dtype)\n",
    "#     print(reward_batch.dtype)\n",
    "#     print((next_state_values * GAMMA).dtype)\n",
    "#     print(loss.dtype)\n",
    "\n",
    "    # Optimize the model\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    for param in policy_net.parameters():\n",
    "        param.grad.data.clamp_(-1, 1)\n",
    "    optimizer.step()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "## Evaluation of \"baseline\" (randomly initialized) model without training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished episode  0  with  999  steps, and rewards =  [-1.96185936e+00 -1.32202322e-01 -8.78520483e-04  4.35960755e-02\n",
      "  5.64226895e-02] ;\n",
      "cumulative reward =  -2274.8263208153744\n",
      "Finished episode  1  with  999  steps, and rewards =  [-1.2544571  -1.42454866 -1.78495872 -3.05502998 -2.22425435] ;\n",
      "cumulative reward =  -9744.17219086518\n",
      "Finished episode  2  with  999  steps, and rewards =  [-2.13945091 -5.10391078 -3.23264593 -2.14586791 -2.06340293] ;\n",
      "cumulative reward =  -14736.28822871912\n",
      "Finished episode  3  with  999  steps, and rewards =  [0.345376   0.28955964 0.53227181 0.51226006 0.53927592] ;\n",
      "cumulative reward =  2054.903559778193\n",
      "Finished episode  4  with  999  steps, and rewards =  [ 0.18131797 -0.62875714  0.32106878  0.36668325  0.21029918] ;\n",
      "cumulative reward =  213.51931934050307\n",
      "Finished episode  5  with  999  steps, and rewards =  [-5.64247294 -5.55359473 -2.59796394 -2.46347959 -3.11616636] ;\n",
      "cumulative reward =  -19387.253427994336\n",
      "Finished episode  6  with  999  steps, and rewards =  [-1.65929651  0.03987499 -0.06434638  0.07668551  0.09203274] ;\n",
      "cumulative reward =  -1612.9562731748056\n",
      "Finished episode  7  with  999  steps, and rewards =  [-2.42287233 -1.92533414 -2.55887904 -2.04016939 -6.92625273] ;\n",
      "cumulative reward =  -15920.9955020595\n",
      "Finished episode  8  with  999  steps, and rewards =  [0.5378121  0.67703221 0.57931603 0.47101905 0.62820929] ;\n",
      "cumulative reward =  2778.1809929199744\n",
      "Finished episode  9  with  999  steps, and rewards =  [-0.21815597 -0.5440215  -1.26417337 -0.19871311 -0.25543587] ;\n",
      "cumulative reward =  -2597.2690273258786\n",
      "Finished episode  10  with  999  steps, and rewards =  [ 0.5015414  -0.03598672  0.5138819   0.54052355  0.34294221] ;\n",
      "cumulative reward =  1578.835101350174\n",
      "Finished episode  11  with  999  steps, and rewards =  [-0.14007402  0.49673862  0.53813164  0.38532906  0.51710473] ;\n",
      "cumulative reward =  1517.0309061041146\n",
      "Finished episode  12  with  999  steps, and rewards =  [0.56384709 0.66586483 0.63122397 0.45347929 0.64452256] ;\n",
      "cumulative reward =  2721.1726957482965\n",
      "Finished episode  13  with  999  steps, and rewards =  [-1.06295583 -4.35886421 -2.69605138 -1.21900032 -1.08415985] ;\n",
      "cumulative reward =  -10440.41398867765\n",
      "Finished episode  14  with  999  steps, and rewards =  [-2.11630639 -2.91246248 -1.51461697 -1.92342269 -1.37229044] ;\n",
      "cumulative reward =  -9875.0869903695\n",
      "Finished episode  15  with  999  steps, and rewards =  [-0.29040536  0.41476008  0.40833008  0.03946625  0.41307482] ;\n",
      "cumulative reward =  805.0145137831971\n",
      "Finished episode  16  with  999  steps, and rewards =  [-3.15334861 -5.74806479 -3.90657456 -3.97862518 -5.17050567] ;\n",
      "cumulative reward =  -21980.301773238636\n",
      "Finished episode  17  with  999  steps, and rewards =  [-1.12968884 -0.83585033 -0.94068957 -0.64815704 -2.74429871] ;\n",
      "cumulative reward =  -6384.175443860736\n",
      "Finished episode  18  with  999  steps, and rewards =  [-0.15488965 -0.08258899  0.02993468 -1.65920703 -0.07611462] ;\n",
      "cumulative reward =  -2277.6607587403532\n",
      "Finished episode  19  with  999  steps, and rewards =  [-0.13262512  0.29723386  0.35225322  0.25051644  0.16050128] ;\n",
      "cumulative reward =  645.2343409564985\n"
     ]
    }
   ],
   "source": [
    "reward_hist_hst = []\n",
    "for e in range(20):\n",
    "    steps = 0\n",
    "    policy_net.eval()\n",
    "    cum_reward = 0\n",
    "    reward_hist = []\n",
    "\n",
    "    state = env.reset()\n",
    "    state = torch.from_numpy(state).float()\n",
    "    state = Variable(state)\n",
    "    env.render()\n",
    "\n",
    "    for t in range(1000):  \n",
    "        # Try to pick an action, react, and store the resulting behavior in the pool here\n",
    "        actions = []\n",
    "        for i in range(N):\n",
    "            action = select_action(state[i], t, rand=False)\n",
    "            actions.append(action)\n",
    "        action = np.array(actions) \n",
    "\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        next_state = Variable(torch.from_numpy(next_state).float()) # The float() probably avoids bug in net.forward()\n",
    "        state = next_state\n",
    "        cum_reward += sum(reward)\n",
    "        reward_hist.append(reward)\n",
    "        \n",
    "        env.render()\n",
    "        steps += 1\n",
    "\n",
    "        if done:\n",
    "            episode_durations.append(t + 1)\n",
    "            print(\"Took \", t, \" steps to converge\")\n",
    "            break\n",
    "    print(\"Finished episode \", e, \" with \", t, \" steps, and rewards = \", reward, \";\\ncumulative reward = \", cum_reward)\n",
    "    reward_hist_hst.append(reward_hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode  0  finished; t =  999\n",
      "Episode  1  finished; t =  999\n",
      "Episode  2  finished; t =  999\n",
      "Episode  3  finished; t =  999\n",
      "Episode  4  finished; t =  999\n",
      "Episode  5  finished; t =  999\n",
      "Episode  6  finished; t =  999\n",
      "Episode  7  finished; t =  999\n",
      "Episode  8  finished; t =  999\n",
      "Episode  9  finished; t =  999\n",
      "Episode  10  finished; t =  999\n",
      "Episode  11  finished; t =  999\n",
      "Episode  12  finished; t =  999\n",
      "Episode  13  finished; t =  999\n",
      "Episode  14  finished; t =  999\n",
      "Episode  15  finished; t =  999\n",
      "Episode  16  finished; t =  999\n",
      "Episode  17  finished; t =  999\n",
      "Episode  18  finished; t =  999\n",
      "Episode  19  finished; t =  999\n",
      "Episode  20  finished; t =  999\n",
      "Episode  21  finished; t =  999\n",
      "Episode  22  finished; t =  999\n",
      "Episode  23  finished; t =  999\n",
      "Episode  24  finished; t =  999\n",
      "Episode  25  finished; t =  999\n",
      "Episode  26  finished; t =  999\n",
      "Episode  27  finished; t =  999\n",
      "Episode  28  finished; t =  999\n",
      "Episode  29  finished; t =  999\n",
      "Episode  30  finished; t =  999\n",
      "Episode  31  finished; t =  999\n",
      "Episode  32  finished; t =  999\n",
      "Episode  33  finished; t =  999\n",
      "Episode  34  finished; t =  999\n",
      "Episode  35  finished; t =  999\n",
      "Episode  36  finished; t =  999\n",
      "Episode  37  finished; t =  999\n",
      "Episode  38  finished; t =  999\n",
      "Episode  39  finished; t =  999\n",
      "Episode  40  finished; t =  999\n",
      "Episode  41  finished; t =  999\n",
      "Episode  42  finished; t =  999\n",
      "Episode  43  finished; t =  999\n",
      "Episode  44  finished; t =  999\n",
      "Episode  45  finished; t =  999\n",
      "Episode  46  finished; t =  999\n",
      "Episode  47  finished; t =  999\n",
      "Episode  48  finished; t =  999\n",
      "Episode  49  finished; t =  999\n"
     ]
    }
   ],
   "source": [
    "# Batch History\n",
    "state_pool = []\n",
    "action_pool = []\n",
    "reward_pool = []\n",
    "steps = 0\n",
    "\n",
    "for e in range(num_episode):\n",
    "    steps = 0\n",
    "    state = env.reset()\n",
    "    state = torch.from_numpy(state).float()\n",
    "    state = Variable(state)\n",
    "    env.render()\n",
    "\n",
    "    for t in range(num_iteration):\n",
    "        policy_net.train()\n",
    "        '''\n",
    "        # actions = []\n",
    "        # for i in range(N):\n",
    "        #     # print(torch.Tensor(i), torch.Tensor([i]), torch.Tensor([[i]]))\n",
    "        #     action = policy_net(state, Variable(torch.Tensor([i])))\n",
    "        #     # print(i, action, action.data.numpy())\n",
    "        #     action = action.data.numpy()\n",
    "        #     actions.append(action)\n",
    "        # # print(\"Final actions: \", actions)\n",
    "        # action = np.array(actions).T\n",
    "        # # print(action)\n",
    "        '''            \n",
    "        # Try to pick an action, react, and store the resulting behavior in the pool here\n",
    "        actions = []\n",
    "        for i in range(N):\n",
    "            # Should I normalize the values into probabilities and let the agent choose one here?\n",
    "            action = select_action(state[i], t)\n",
    "            actions.append(action)\n",
    "        # print(\"Final actions: \", actions)\n",
    "        action = np.array(actions) #.T\n",
    "        \n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        env.render()\n",
    "        \n",
    "#         state = next_state\n",
    "        next_state = Variable(torch.from_numpy(next_state).float()) # The float() probably avoids bug in net.forward()\n",
    "#         print(next_state)\n",
    "#         state = Variable(state)\n",
    "\n",
    "        for i in range(N):\n",
    "            memory.push(state[i], action[i], next_state[i], reward[i])\n",
    "#             state_pool.append(state[i])\n",
    "#             action_pool.append(action[i])\n",
    "#             reward_pool.append(reward[i])\n",
    "        state = next_state\n",
    "        steps += 1\n",
    "        \n",
    "        if len(memory) >= BATCH_SIZE:\n",
    "            transitions = memory.sample(BATCH_SIZE)\n",
    "            # Transpose the batch (see https://stackoverflow.com/a/19343/3343043 for\n",
    "            # detailed explanation). This converts batch-array of Transitions\n",
    "            # to Transition of batch-arrays.\n",
    "            batch = Transition(*zip(*transitions))\n",
    "#             print(batch.next_state)\n",
    "            optimize_model(batch)\n",
    "\n",
    "        if done:\n",
    "            episode_durations.append(t + 1)\n",
    "            print(\"Took \", t, \" steps to converge\")\n",
    "#             plot_durations()\n",
    "            break\n",
    "    print(\"Episode \", e, \" finished; t = \", t)\n",
    "#     if not memory_available:\n",
    "#         print(\"Samples taking too much memory. Quit? Remove half of them?\")\n",
    "#         memory.discard()\n",
    "#         print(\"Oh snap\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished episode  0  with  999  steps, and rewards =  [-7.23029778 -6.79407542 -5.17883092 -4.78657521 -4.19727563] ;\n",
      "cumulative reward =  -28170.69441934428\n",
      "Finished episode  1  with  999  steps, and rewards =  [ -3.19801046  -4.23965593  -4.5913873   -3.31960833 -10.36719812] ;\n",
      "cumulative reward =  -25719.250145150294\n",
      "Finished episode  2  with  999  steps, and rewards =  [-4.61045129 -6.75452837 -9.90853829 -4.88235819 -4.95635972] ;\n",
      "cumulative reward =  -31082.229419159103\n",
      "Finished episode  3  with  999  steps, and rewards =  [-4.53416681 -5.3311591  -7.23032993 -4.45295404 -5.71860783] ;\n",
      "cumulative reward =  -27284.125255387917\n",
      "Finished episode  4  with  999  steps, and rewards =  [-6.40542108 -6.17241013 -7.43870104 -4.79295261 -4.68106229] ;\n",
      "cumulative reward =  -29404.848217499988\n",
      "Finished episode  5  with  999  steps, and rewards =  [-6.84965117 -4.5171034  -5.18971541 -6.4939917  -5.15223549] ;\n",
      "cumulative reward =  -28751.523193964207\n",
      "Finished episode  6  with  999  steps, and rewards =  [-3.91853121 -7.05290593 -5.90063561 -7.2974785  -3.80741715] ;\n",
      "cumulative reward =  -27926.567736449644\n",
      "Finished episode  7  with  999  steps, and rewards =  [-5.11353382 -6.05993716 -7.08368842 -5.15236003 -5.48643186] ;\n",
      "cumulative reward =  -28868.34211874405\n",
      "Finished episode  8  with  999  steps, and rewards =  [-6.32059611 -4.3685768  -6.18402564 -6.45286468 -4.12369125] ;\n",
      "cumulative reward =  -27453.65101664944\n",
      "Finished episode  9  with  999  steps, and rewards =  [-6.37724724 -7.53312814 -6.24311254 -6.01614731 -6.10467763] ;\n",
      "cumulative reward =  -32245.55062312113\n",
      "Finished episode  10  with  999  steps, and rewards =  [-6.13091395 -4.37572275 -4.3301375  -9.08453734 -5.08344395] ;\n",
      "cumulative reward =  -29002.47824720109\n",
      "Finished episode  11  with  999  steps, and rewards =  [-0.8517587  -1.62114192 -1.49824102 -0.86387215 -2.15655094] ;\n",
      "cumulative reward =  -7003.09081642128\n",
      "Finished episode  12  with  999  steps, and rewards =  [-5.99514075 -4.23122644 -8.52199227 -4.46810423 -6.71128857] ;\n",
      "cumulative reward =  -29929.700741553894\n",
      "Finished episode  13  with  999  steps, and rewards =  [ -5.88476927  -6.61088267  -6.42090904  -6.21876927 -10.40115688] ;\n",
      "cumulative reward =  -35527.88844460209\n",
      "Finished episode  14  with  999  steps, and rewards =  [-2.89705184 -4.2193563  -2.49125479 -3.84412511 -2.13301939] ;\n",
      "cumulative reward =  -15591.467393585734\n",
      "Finished episode  15  with  999  steps, and rewards =  [-6.11696567 -4.42435375 -4.22917467 -4.37372267 -3.3854996 ] ;\n",
      "cumulative reward =  -22611.622730916017\n",
      "Finished episode  16  with  999  steps, and rewards =  [-4.19189749 -6.43453762 -4.22377742 -4.90940106 -8.78001192] ;\n",
      "cumulative reward =  -28529.554123820413\n",
      "Finished episode  17  with  999  steps, and rewards =  [-2.26950245 -2.30502575 -3.94546324 -2.60925506 -7.2551195 ] ;\n",
      "cumulative reward =  -18450.098555742123\n",
      "Finished episode  18  with  999  steps, and rewards =  [-8.95766801 -8.27132672 -4.93985712 -5.38076463 -5.28596884] ;\n",
      "cumulative reward =  -32808.09388783612\n",
      "Finished episode  19  with  999  steps, and rewards =  [-5.10734366 -2.00359449 -2.26999648 -5.52194534 -1.93084565] ;\n",
      "cumulative reward =  -16847.301511727834\n"
     ]
    }
   ],
   "source": [
    "reward_hist_hst = []\n",
    "for e in range(20):\n",
    "    steps=0\n",
    "    policy_net.eval()\n",
    "    cum_reward = 0\n",
    "    reward_hist = []\n",
    "\n",
    "    state = env.reset()\n",
    "    state = torch.from_numpy(state).float()\n",
    "    state = Variable(state)\n",
    "    env.render()\n",
    "\n",
    "    for t in range(1000):  \n",
    "        # Try to pick an action, react, and store the resulting behavior in the pool here\n",
    "        actions = []\n",
    "        for i in range(N):\n",
    "            action = select_action(state[i], t, rand=False)\n",
    "            actions.append(action)\n",
    "        action = np.array(actions) \n",
    "\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        next_state = Variable(torch.from_numpy(next_state).float()) # The float() probably avoids bug in net.forward()\n",
    "        state = next_state\n",
    "        cum_reward += sum(reward)\n",
    "        reward_hist.append(reward)\n",
    "        \n",
    "        env.render()\n",
    "        steps += 1\n",
    "\n",
    "        if done:\n",
    "            episode_durations.append(t + 1)\n",
    "            print(\"Took \", t, \" steps to converge\")\n",
    "            break\n",
    "    print(\"Finished episode \", e, \" with \", t, \" steps, and rewards = \", reward, \";\\ncumulative reward = \", cum_reward)\n",
    "    reward_hist_hst.append(reward_hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.nf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2, 2, 2, 3, 4],\n",
       "       [5, 6, 7, 8, 9]])"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.arange(10).reshape((2,5))\n",
    "np.clip(a,2,9)\n",
    "# np.clip(a, np.array([[2,8]]).T, np.array([[4,9]]).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.1780,  0.0162, -1.4030, -2.1048,  0.0000],\n",
       "        [-0.8565, -1.3593, -1.3299, -1.3903,  0.0000],\n",
       "        [-1.2942,  0.0000, -1.4692, -2.0210, -0.1162],\n",
       "        ...,\n",
       "        [ 0.0000, -0.1528, -0.1734, -0.1338,  1.0065],\n",
       "        [ 0.0000,  1.2942, -0.1250, -0.7268,  1.2280],\n",
       "        [ 0.0000, -0.3528, -0.2734, -0.1838,  0.9565]])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# state_batch = torch.cat(batch.state)\n",
    "# bs = np.asarray(batch.state)\n",
    "torch.cat([s for s in batch.state])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_batch.view(BATCH_SIZE, -1, N).dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "savs = policy_net(state_batch.view(BATCH_SIZE, -1, N))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = env.render(mode=\"rgb_array\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(800, 1200, 3)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW4AAAD8CAYAAABXe05zAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/d3fzzAAAACXBIWXMAAAsTAAALEwEAmpwYAAARyklEQVR4nO3df6zd9X3f8edrOJCMbtimd5ZnW4MqVhCaFKBX1CjV1MVLBm4V80eKQFWxqCVPCtuStlLnbH8klaYokabSIE2oFqQ1VZaEuslsIZTAHKqqf0BzUxgh/Cg3FGpbBt9QcLpEHWW898f5XHIwdu45vuf2+uPzfEhH5/N9fz/nns9HH+fF937u9+SkqpAk9eMfrfYAJEnjMbglqTMGtyR1xuCWpM4Y3JLUGYNbkjqzIsGd5PokzyaZT7J3Jd5DkqZVJn0fd5ILgL8EPgQcBb4F3FJVT030jSRpSq3EFfe1wHxVPV9VrwNfBnauwPtI0lRaswI/cxNwZOj4KPBzp3ZKsgfYA3DxxRf/7BVXXLECQ5GkPr3wwgt8//vfz+nOrURwj6Sq9gH7AGZnZ2tubm61hiJJ55zZ2dkznluJrZJjwJah482tJkmagJUI7m8BW5NcnuRC4Gbg0Aq8jyRNpYlvlVTVG0n+PfAN4ALgC1X13Um/jyRNqxXZ466qB4AHVuJnS9K085OTktQZg1uSOmNwS1JnDG5J6ozBLUmdMbglqTMGtyR1xuCWpM4Y3JLUGYNbkjpjcEtSZwxuSeqMwS1JnTG4JakzBrckdcbglqTOGNyS1BmDW5I6s2RwJ/lCkhNJnhyqrU/yUJLn2vO6Vk+SO5PMJ3kiyTUrOXhJmkajXHH/AXD9KbW9wOGq2gocbscANwBb22MPcNdkhilJWrRkcFfVnwJ/c0p5J7C/tfcDNw7V762BR4C1STZOaKySJM5+j3tDVR1v7ZeADa29CTgy1O9oq0mSJmTZf5ysqgJq3Ncl2ZNkLsncwsLCcochSVPjbIP75cUtkPZ8otWPAVuG+m1utXeoqn1VNVtVszMzM2c5DEmaPmcb3IeAXa29Czg4VL+13V2yDTg5tKUiSZqANUt1SPIl4BeAn05yFPgU8FngviS7gReBm1r3B4AdwDzwI+C2FRizJE21JYO7qm45w6ntp+lbwO3LHZQk6cz85KQkdcbglqTOGNyS1BmDW5I6Y3BLUmcMbknqjMEtSZ0xuCWpMwa3JHXG4JakzhjcktQZg1uSOmNwS1JnDG5J6ozBLUmdMbglqTMGtyR1xuCWpM4Y3JLUmSWDO8mWJA8neSrJd5N8vNXXJ3koyXPteV2rJ8mdSeaTPJHkmpWehCRNk1GuuN8AfrOqrgS2AbcnuRLYCxyuqq3A4XYMcAOwtT32AHdNfNSSNMWWDO6qOl5Vf9Hafws8DWwCdgL7W7f9wI2tvRO4twYeAdYm2TjpgUvStBprjzvJZcDVwKPAhqo63k69BGxo7U3AkaGXHW21U3/WniRzSeYWFhbGHbckTa2RgzvJTwF/DHyiqn4wfK6qCqhx3riq9lXVbFXNzszMjPNSSZpqIwV3kncxCO0vVtVXW/nlxS2Q9nyi1Y8BW4ZevrnVJEkTMMpdJQHuAZ6uqt8ZOnUI2NXau4CDQ/Vb290l24CTQ1sqkqRlWjNCnw8Avwp8J8njrfafgc8C9yXZDbwI3NTOPQDsAOaBHwG3TXLAkjTtlgzuqvozIGc4vf00/Qu4fZnjkiSdgZ+clKTOGNyS1BmDW5I6Y3BLUmcMbknqjMEtSZ0xuCWpMwa3JHXG4JakzhjcktQZg1uSOmNwS1JnDG5J6ozBLUmdMbglqTMGtyR1xuCWpM4Y3JLUmVG+LPjdSf48yf9O8t0kv93qlyd5NMl8kq8kubDVL2rH8+38ZSs8B0maKqNccf9f4INV9X7gKuD69u3tnwPuqKr3Aq8Cu1v/3cCrrX5H6ydJmpAlg7sG/k87fFd7FPBB4ECr7wdubO2d7Zh2fnuSM33ZsCRpTCPtcSe5IMnjwAngIeB7wGtV9UbrchTY1NqbgCMA7fxJ4NLT/Mw9SeaSzC0sLCxrEpI0TUYK7qr6f1V1FbAZuBa4YrlvXFX7qmq2qmZnZmaW++MkaWqMdVdJVb0GPAxcB6xNsqad2gwca+1jwBaAdv4S4JVJDFaSNNpdJTNJ1rb2e4APAU8zCPCPtm67gIOtfagd085/s6pqgmOWpKm2ZukubAT2J7mAQdDfV1X3J3kK+HKS/wo8BtzT+t8D/GGSeeBvgJtXYNySNLWWDO6qegK4+jT15xnsd59a/zvglycyOknSO/jJSUnqjMEtSZ0xuCWpMwa3JHXG4JakzhjcktQZg1uSOmNwS1JnDG5J6ozBLUmdMbglqTMGtyR1xuCWpM4Y3JLUGYNbkjpjcEtSZwxuSeqMwS1JnRk5uJNckOSxJPe348uTPJpkPslXklzY6he14/l2/rIVGrskTaVxrrg/zuDb3Rd9Drijqt4LvArsbvXdwKutfkfrJ0makJGCO8lm4BeBu9txgA8CB1qX/cCNrb2zHdPOb2/9JUkTMOoV9+8CvwW82Y4vBV6rqjfa8VFgU2tvAo4AtPMnW/+3SbInyVySuYWFhbMbvSRNoSWDO8kvASeq6tuTfOOq2ldVs1U1OzMzM8kfLUnntTUj9PkA8JEkO4B3A/8U+DywNsmadlW9GTjW+h8DtgBHk6wBLgFemfjIJWlKLXnFXVWfrKrNVXUZcDPwzar6FeBh4KOt2y7gYGsfase089+sqproqCVpii3nPu7/BPxGknkGe9j3tPo9wKWt/hvA3uUNUZI0bJStkrdU1Z8Af9LazwPXnqbP3wG/PIGxSZJOw09OSlJnDG5J6ozBLUmdMbglqTMGtyR1xuCWpM4Y3JLUGYNbkjpjcEtSZwxuSeqMwS1JnTG4JakzBrckdcbglqTOGNyS1BmDW5I6Y3BLUmcMbknqzEjBneSFJN9J8niSuVZbn+ShJM+153WtniR3JplP8kSSa1ZyApI0bca54v7XVXVVVc22473A4araChzmx18KfAOwtT32AHdNarCSpOVtlewE9rf2fuDGofq9NfAIsDbJxmW8jyRpyKjBXcCDSb6dZE+rbaiq4639ErChtTcBR4Zee7TV3ibJniRzSeYWFhbOYuiSNJ3WjNjv56vqWJJ/BjyU5Jnhk1VVSWqcN66qfcA+gNnZ2bFeK0nTbKQr7qo61p5PAF8DrgVeXtwCac8nWvdjwJahl29uNUnSBCwZ3EkuTvJPFtvAh4EngUPArtZtF3CwtQ8Bt7a7S7YBJ4e2VCRJyzTKVskG4GtJFvv/j6r6epJvAfcl2Q28CNzU+j8A7ADmgR8Bt0181JI0xZYM7qp6Hnj/aeqvANtPUy/g9omMTpL0Dn5yUpI6Y3BLUmcMbknqjMEtSZ0xuCWpMwa3JHXG4JakzhjcktQZg1uSOmNwS1JnDG5J6ozBLUmdMbglqTMGtyR1xuCWpM4Y3JLUGYNbkjpjcEtSZ0YK7iRrkxxI8kySp5Ncl2R9koeSPNee17W+SXJnkvkkTyS5ZmWnIEnTZdQr7s8DX6+qKxh8/+TTwF7gcFVtBQ63Y4AbgK3tsQe4a6IjlqQpt2RwJ7kE+FfAPQBV9XpVvQbsBPa3bvuBG1t7J3BvDTwCrE2yccLjlqSpNcoV9+XAAvD7SR5LcneSi4ENVXW89XkJ2NDam4AjQ68/2mqSpAkYJbjXANcAd1XV1cAP+fG2CABVVUCN88ZJ9iSZSzK3sLAwzkslaaqNEtxHgaNV9Wg7PsAgyF9e3AJpzyfa+WPAlqHXb261t6mqfVU1W1WzMzMzZzt+SZo6SwZ3Vb0EHEnyvlbaDjwFHAJ2tdou4GBrHwJubXeXbANODm2pSJKWac2I/f4D8MUkFwLPA7cxCP37kuwGXgRuan0fAHYA88CPWl9J0oSMFNxV9Tgwe5pT20/Tt4DblzcsSdKZ+MlJSeqMwS1JnTG4JakzBrckdcbglqTOGNyS1BmDW5I6Y3BLUmcMbknqjMEtSZ0xuCWpMwa3JHXG4JakzhjcktQZg1uSOmNwS1JnDG5J6ozBLUmdWTK4k7wvyeNDjx8k+USS9UkeSvJce17X+ifJnUnmkzyR5JqVn4YkTY9RvuX92aq6qqquAn6WwRcAfw3YCxyuqq3A4XYMcAOwtT32AHetwLglaWqNu1WyHfheVb0I7AT2t/p+4MbW3gncWwOPAGuTbJzEYCVJ4wf3zcCXWntDVR1v7ZeADa29CTgy9JqjrSZJmoCRgzvJhcBHgD869VxVFVDjvHGSPUnmkswtLCyM81JJmmrjXHHfAPxFVb3cjl9e3AJpzyda/RiwZeh1m1vtbapqX1XNVtXszMzM+COXpCk1TnDfwo+3SQAOAbtaexdwcKh+a7u7ZBtwcmhLRZK0TGtG6ZTkYuBDwL8bKn8WuC/JbuBF4KZWfwDYAcwzuAPltomNVpI0WnBX1Q+BS0+pvcLgLpNT+xZw+0RGJ0l6Bz85KUmdMbglqTMGtyR1xuCWpM4Y3JLUGYNbkjpjcEtSZwxuSeqMwS1JnTG4JakzBrckdcbglqTOGNyS1BmDW5I6Y3BLUmcMbknqjMEtSZ0xuCWpMwa3pJ+oqnjzzTcZfCuhzgUGt6QzOn78OJ/5zGfYsWMHn/70p3nttddWe0hixC8LljR9Xn/9dT72sY9x8OBBqooHH3yQ+fl57r77bt7znves9vCmmlfckk7rmWee4cEHH3xri6SqOHDgAI899tgqj0w5F/atkvwt8Oxqj2OF/DTw/dUexApwXv05X+d2vs7rX1TVzOlOnCtbJc9W1exqD2IlJJk7H+fmvPpzvs7tfJ3XT+JWiSR1xuCWpM6cK8G9b7UHsILO17k5r/6cr3M7X+d1RufEHyclSaM7V664JUkjMrglqTOrHtxJrk/ybJL5JHtXezzjSLIlycNJnkry3SQfb/X1SR5K8lx7XtfqSXJnm+sTSa5Z3Rn8ZEkuSPJYkvvb8eVJHm3j/0qSC1v9onY8385ftqoDX0KStUkOJHkmydNJrjsf1izJr7d/h08m+VKSd/e6Zkm+kOREkieHamOvUZJdrf9zSXatxlxWwqoGd5ILgP8O3ABcCdyS5MrVHNOY3gB+s6quBLYBt7fx7wUOV9VW4HA7hsE8t7bHHuCuf/ghj+XjwNNDx58D7qiq9wKvArtbfTfwaqvf0fqdyz4PfL2qrgDez2COXa9Zkk3AfwRmq+pfAhcAN9Pvmv0BcP0ptbHWKMl64FPAzwHXAp9aDPvuVdWqPYDrgG8MHX8S+ORqjmmZ8zkIfIjBp0A3ttpGBh8wAvg94Jah/m/1O9cewGYG/+P4IHA/EAafTltz6toB3wCua+01rV9Wew5nmNclwF+dOr7e1wzYBBwB1rc1uB/4tz2vGXAZ8OTZrhFwC/B7Q/W39ev5sdpbJYv/2BYdbbXutF81rwYeBTZU1fF26iVgQ2v3NN/fBX4LeLMdXwq8VlVvtOPhsb81r3b+ZOt/LrocWAB+v20D3Z3kYjpfs6o6Bvw34K+B4wzW4NucH2u2aNw16mLtzsZqB/d5IclPAX8MfKKqfjB8rgb/qe/qnsskvwScqKpvr/ZYVsAa4Brgrqq6GvghP/6VG+h2zdYBOxn8h+mfAxfzzq2G80aPazRJqx3cx4AtQ8ebW60bSd7FILS/WFVfbeWXk2xs5zcCJ1q9l/l+APhIkheALzPYLvk8sDbJ4v+/zfDY35pXO38J8Mo/5IDHcBQ4WlWPtuMDDIK89zX7N8BfVdVCVf098FUG63g+rNmicdeol7Ub22oH97eAre0v3xcy+GPKoVUe08iSBLgHeLqqfmfo1CFg8S/YuxjsfS/Wb21/Bd8GnBz61e+cUVWfrKrNVXUZgzX5ZlX9CvAw8NHW7dR5Lc73o63/OXk1VFUvAUeSvK+VtgNP0fmaMdgi2ZbkH7d/l4vz6n7Nhoy7Rt8APpxkXfuN5MOt1r/V3mQHdgB/CXwP+C+rPZ4xx/7zDH5dewJ4vD12MNgrPAw8B/wvYH3rHwZ30XwP+A6DOwBWfR5LzPEXgPtb+2eAPwfmgT8CLmr1d7fj+Xb+Z1Z73EvM6Spgrq3b/wTWnQ9rBvw28AzwJPCHwEW9rhnwJQZ79X/P4Lek3WezRsCvtTnOA7et9rwm9fAj75LUmdXeKpEkjcnglqTOGNyS1BmDW5I6Y3BLUmcMbknqjMEtSZ35/3iG9Rn1YjCjAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(img)\n",
    "plt.savefig('test.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.savefig('test.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('float32')"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.asarray(reward).astype('float32').dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_durations():\n",
    "    plt.figure(2)\n",
    "    plt.clf()\n",
    "    durations_t = torch.FloatTensor(episode_durations)\n",
    "    plt.title('Training...')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Duration')\n",
    "    plt.plot(durations_t.numpy())\n",
    "    # Take 100 episode averages and plot them too\n",
    "    if len(durations_t) >= 100:\n",
    "        means = durations_t.unfold(0, 100, 1).mean(1).view(-1)\n",
    "        means = torch.cat((torch.zeros(99), means))\n",
    "        plt.plot(means.numpy())\n",
    "\n",
    "    plt.pause(0.001)  # pause a bit so that plots are updated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
