{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trying it out for fixed environment (or compromised environment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple\n",
    "from itertools import count\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "from torch.autograd import Variable\n",
    "\n",
    "# set up matplotlib\n",
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "\n",
    "plt.ion()\n",
    "\n",
    "# if gpu is to be used\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'utils.train_test_methods' from '/Users/zz/Documents/GT 20F/7000/GNN_experiments/utils/train_test_methods.py'>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from utils.ReplayMemory import * \n",
    "from utils.networks import *\n",
    "from utils.agents import *\n",
    "from utils.plotting import *\n",
    "from utils.train_test_methods import *\n",
    "from utils.params import *\n",
    "\n",
    "from importlib import reload\n",
    "import utils\n",
    "reload(utils.ReplayMemory)\n",
    "reload(utils.networks)\n",
    "reload(utils.agents)\n",
    "reload(utils.plotting)\n",
    "reload(utils.train_test_methods)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_type = U_VELOCITY\n",
    "observe_type = O_VELOCITY\n",
    "observe_action = O_ACTION\n",
    "reward_mode=ALL_REWARD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try out Actor-Critic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test out Actor-Critic methods. Note that tuning parameters might be required.\n",
    "reload(utils.ReplayMemory)\n",
    "reload(utils.networks)\n",
    "reload(utils.agents)\n",
    "reload(utils.plotting)\n",
    "reload(utils.train_test_methods)\n",
    "from utils.ReplayMemory import * \n",
    "from utils.networks import *\n",
    "from utils.agents import *\n",
    "from utils.plotting import *\n",
    "from utils.train_test_methods import *\n",
    "\n",
    "num_episode=500#500\n",
    "test_interval=10#0\n",
    "num_test=10#50\n",
    "num_iteration=200\n",
    "BATCH_SIZE=64#128\n",
    "debug=False\n",
    "num_sample=50\n",
    "seed=22222\n",
    "\n",
    "# rand_mode = NO_RAND\n",
    "rand_mode = GAUSS_RAND\n",
    "\n",
    "N_list = [5,10,20]\n",
    "env_list = []\n",
    "for N_ in N_list:\n",
    "    env_list.append(\n",
    "        gym.make('ConsensusEnv:ConsensusContEnv-v0', N=N_, dt=0.1, Delta=0.05,\n",
    "              input_type=input_type, observe_type=observe_type, observe_action=O_ACTION, reward_mode=DIST_REWARD, \n",
    "                 boundary_policy=SOFT_PENALTY, finish_reward_policy=REWARD_IF_CONSENSUS\n",
    "        ).unwrapped\n",
    "    )\n",
    "\n",
    "AC2_list = []\n",
    "for i,N_ in enumerate(N_list[:4]):\n",
    "    AC2_list.append(\n",
    "        AC2Agent(device, N_, env_list[i].nf, env_list[i].na, hidden, rand_modeA=rand_mode,\n",
    "                 learning_rateA=0.01, learning_rateC=0.02)\n",
    "    )\n",
    "\n",
    "AC2_hist = []\n",
    "AC2_loss = []\n",
    "for i,N_ in enumerate(N_list[:4]):\n",
    "    AC2_loss.append([])\n",
    "    AC2_hist.append(\n",
    "        train(AC2_list[i], env_list[i], \n",
    "              num_episode=num_episode, test_interval=test_interval, num_test=num_test, num_iteration=num_iteration, \n",
    "              BATCH_SIZE=BATCH_SIZE, num_sample=num_sample, action_space=[-1,1], debug=debug,\n",
    "              update_mode=UPDATE_PER_ITERATION, reward_mode=FUTURE_REWARD_YES_NORMALIZE, loss_history=AC2_loss[i])\n",
    "    )\n",
    "    print(\"Finished training env with {0} agents for AC\".format(N_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# additional testing\n",
    "num_episode=1500\n",
    "test_interval=5#0\n",
    "num_test=10#50\n",
    "num_iteration=200\n",
    "N_list = [5,10,20]\n",
    "\n",
    "for i,N_ in enumerate(N_list[:4]):\n",
    "#     AC2_list[i].optimizerA.learning_rate = 0.05\n",
    "#     AC2_list[i].optimizerC.learning_rate = 0.08\n",
    "    AC2_hist[i] += train(AC2_list[i], env_list[i], \n",
    "              num_episode=num_episode, test_interval=test_interval, num_test=num_test, num_iteration=num_iteration, \n",
    "              BATCH_SIZE=BATCH_SIZE, num_sample=num_sample, action_space=[-1,1], debug=debug,\n",
    "              update_mode=UPDATE_PER_ITERATION, reward_mode=FUTURE_REWARD_YES_NORMALIZE, loss_history=AC2_loss[i])\n",
    "    print(\"Finished training env with {0} agents for AC\".format(N_))\n",
    "\n",
    "for i,N_ in enumerate(N_list[:4]):\n",
    "    AC2_test_hist[i] += plot_test(AC2_list[i], env_list[i], fnames=['']*num_test,\n",
    "            num_iteration=num_iteration, action_space=action_space, imdir='screencaps/',debug=debug)\n",
    "    print(\"Finished testnig env with {0} agents for AC\".format(N_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(utils.plotting)\n",
    "from utils.plotting import *\n",
    "skip = 1\n",
    "plot_reward_hist([h[::skip] for h in AC2_hist], test_interval*skip, \n",
    "                 ['AC2_N{0}'.format(N_) for N_ in N_list[:4]], \n",
    "                 log=False, num_iteration=num_iteration, N_list=([1 for N in N_list[:4]]), bar=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot loss per # of agent\n",
    "skip=1\n",
    "plot_loss_hist(hists=[h[::skip] for h in AC2_loss], hist_names=['AC2_N{0}'.format(N_) for N_ in N_list[:4]], log=False, \n",
    "               num_iteration=num_iteration, update_mode=UPDATE_PER_ITERATION, bar=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished episode  0  with  99 ;\n",
      "cumulative reward =  -4838.5194699456115\n",
      "Finished episode  0  with  99 ;\n",
      "cumulative reward =  -20258.766523015714\n",
      "Finished episode  0  with  99 ;\n",
      "cumulative reward =  -28968.251461913635\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "### Save their animations and see how they behave.\n",
    "for i,N_ in enumerate(N_list):\n",
    "    plot_test(AC2_list[i], env_list[i], fnames=['AC2_dist_reward_test{0}_N{1}'.format(j,N_) for j in range(1)],\n",
    "        num_iteration=100, action_space=action_space, imdir='screencaps/',debug=debug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model to models/AC2Agent_AC2_negGausstest_onlydistreward_normalized_N5\n",
      "Saving model to models/AC2Agent_AC2_negGausstest_onlydistreward_normalized_N10\n",
      "Saving model to models/AC2Agent_AC2_negGausstest_onlydistreward_normalized_N20\n"
     ]
    }
   ],
   "source": [
    "AC2_list[0].save_model(\"AC2_negGausstest_onlydistreward_normalized_N5\")\n",
    "AC2_list[1].save_model(\"AC2_negGausstest_onlydistreward_normalized_N10\")\n",
    "AC2_list[2].save_model(\"AC2_negGausstest_onlydistreward_normalized_N20\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The above, but uses all reward terms together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_episode=500\n",
    "test_interval=10#0\n",
    "num_test=10#50\n",
    "env_list_allreward = []\n",
    "N_list = [5,10,20]\n",
    "for N_ in N_list:\n",
    "    env_list_allreward.append(\n",
    "        gym.make('ConsensusEnv:ConsensusContEnv-v0', N=N_, dt=0.1, Delta=0.05,\n",
    "              input_type=input_type, observe_type=observe_type, observe_action=O_ACTION, reward_mode=ALL_REWARD\n",
    "        ).unwrapped\n",
    "    )\n",
    "\n",
    "AC2_list_allreward = []\n",
    "for i,N_ in enumerate(N_list[:4]):\n",
    "    AC2_list_allreward.append(\n",
    "        AC2Agent(device, N_, env_list_allreward[i].nf, env_list_allreward[i].na, hidden, \n",
    "                 learning_rateA=0.01, learning_rateC=0.02)\n",
    "    )\n",
    "\n",
    "AC2_hist_allreward = []\n",
    "AC2_loss_allreward = []\n",
    "for i,N_ in enumerate(N_list[:4]):\n",
    "    AC2_loss_allreward.append([])\n",
    "    AC2_hist_allreward.append(\n",
    "        train(AC2_list_allreward[i], env_list_allreward[i], \n",
    "              num_episode=num_episode, test_interval=test_interval, num_test=num_test, num_iteration=num_iteration, \n",
    "              BATCH_SIZE=BATCH_SIZE, num_sample=num_sample, action_space=[-1,1], debug=debug,\n",
    "              update_mode=UPDATE_PER_ITERATION, reward_mode=FUTURE_REWARD_YES_NORMALIZE, loss_history=AC2_loss_allreward[i])\n",
    "    )\n",
    "    print(\"Finished training env with {0} agents for AC\".format(N_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_reward_hist(AC2_hist_allreward, test_interval, \n",
    "                 ['AC2_N{0}'.format(N_) for N_ in N_list[:4]], \n",
    "                 log=False, num_iteration=num_iteration, N_list=([1 for N in N_list[:4]]), bar=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot loss\n",
    "plot_loss_hist(hists=[h[5::skip] for h in AC2_loss_allreward], hist_names=['AC2_N{0}'.format(N_) for N_ in N_list[:4]], log=False, \n",
    "               num_iteration=num_iteration, update_mode=UPDATE_PER_ITERATION, bar=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model to models/AC2Agent_AC2_allreward_normalized_N5\n",
      "Saving model to models/AC2Agent_AC2_allreward_normalized_N10\n",
      "Saving model to models/AC2Agent_AC2_allreward_normalized_N20\n"
     ]
    }
   ],
   "source": [
    "AC2_list_allreward[0].save_model(\"AC2_allreward_normalized_N5\")\n",
    "AC2_list_allreward[1].save_model(\"AC2_allreward_normalized_N10\")\n",
    "AC2_list_allreward[2].save_model(\"AC2_allreward_normalized_N20\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished episode  0  with  99 ;\n",
      "cumulative reward =  -4731.4494197514305\n",
      "Finished episode  1  with  99 ;\n",
      "cumulative reward =  -5153.693242743248\n",
      "Finished episode  2  with  99 ;\n",
      "cumulative reward =  -5081.763071406728\n",
      "Finished episode  0  with  99 ;\n",
      "cumulative reward =  -11308.227650814204\n",
      "Finished episode  1  with  99 ;\n",
      "cumulative reward =  -10975.637769948189\n",
      "Finished episode  2  with  99 ;\n",
      "cumulative reward =  -10856.320880929872\n",
      "Finished episode  0  with  99 ;\n",
      "cumulative reward =  -26163.48483412173\n",
      "Finished episode  1  with  99 ;\n",
      "cumulative reward =  -25082.043036814284\n",
      "Finished episode  2  with  99 ;\n",
      "cumulative reward =  -25481.96634182731\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "reload(utils.plotting)\n",
    "from utils.plotting import *\n",
    "### Save their animations and see how they behave.\n",
    "for i,N_ in enumerate(N_list):\n",
    "    plot_test(AC2_list[i], env_list[i], fnames=['AC2_dist_reward_test{0}_N{1}'.format(j,N_) for j in range(3)],\n",
    "        num_iteration=100, action_space=action_space, imdir='screencaps/',debug=debug)\n",
    "    plot_test(AC2_list_allreward[i], env_list_allreward[i], fnames=['AC2_all_reward_test{0}_N{1}'.format(j,N_) for j in range(3)],\n",
    "        num_iteration=100, action_space=action_space, imdir='screencaps/',debug=debug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear recorded histories\n",
    "# AC2_hist_allreward = []\n",
    "# AC2_loss_allreward = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use advantage terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(utils.ReplayMemory)\n",
    "reload(utils.networks)\n",
    "reload(utils.agents)\n",
    "reload(utils.plotting)\n",
    "reload(utils.train_test_methods)\n",
    "from utils.ReplayMemory import * \n",
    "from utils.networks import *\n",
    "from utils.agents import *\n",
    "from utils.plotting import *\n",
    "from utils.train_test_methods import *\n",
    "\n",
    "num_episode=500#500\n",
    "test_interval=10#0\n",
    "num_test=10#50\n",
    "num_iteration=200\n",
    "BATCH_SIZE=128\n",
    "debug=False\n",
    "num_sample=50\n",
    "seed=22222\n",
    "\n",
    "N_list = [5,10,20]\n",
    "env_list = []\n",
    "for N_ in N_list:\n",
    "    env_list.append(\n",
    "        gym.make('ConsensusEnv:ConsensusContEnv-v0', N=N_, dt=0.1, Delta=0.05,\n",
    "              input_type=input_type, observe_type=observe_type, observe_action=O_ACTION, reward_mode=DIST_REWARD,\n",
    "                 boundary_policy=SOFT_PENALTY, finish_reward_policy=REWARD_IF_CONSENSUS\n",
    "        ).unwrapped\n",
    "    )\n",
    "\n",
    "AC3_list = []\n",
    "for i,N_ in enumerate(N_list):\n",
    "    AC3_list.append(\n",
    "        AC3Agent(device, N_, env_list[i].nf, env_list[i].na, hidden, rand_modeA=rand_mode,\n",
    "                 learning_rateA=0.01, learning_rateC=0.02)\n",
    "    )\n",
    "\n",
    "AC3_hist = []\n",
    "AC3_loss = []\n",
    "for i,N_ in enumerate(N_list):\n",
    "    AC3_loss.append([])\n",
    "    AC3_hist.append(\n",
    "        train(AC3_list[i], env_list[i], \n",
    "              num_episode=num_episode, test_interval=test_interval, num_test=num_test, num_iteration=num_iteration, \n",
    "              BATCH_SIZE=BATCH_SIZE, num_sample=num_sample, action_space=[-1,1], debug=debug,\n",
    "              update_mode=UPDATE_PER_EPISODE, reward_mode=FUTURE_REWARD_YES_NORMALIZE, loss_history=AC3_loss[i])\n",
    "    )\n",
    "    print(\"Finished training env with {0} agents for AC\".format(N_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep training\n",
    "num_episode=1000\n",
    "test_interval=10\n",
    "num_test=10\n",
    "num_iteration=200\n",
    "\n",
    "N_list = [5,10,20]\n",
    "env_list = []\n",
    "for N_ in N_list:\n",
    "    env_list.append(\n",
    "        gym.make('ConsensusEnv:ConsensusContEnv-v0', N=N_, dt=0.1, Delta=0.05,\n",
    "              input_type=input_type, observe_type=observe_type, observe_action=O_ACTION, reward_mode=DIST_REWARD\n",
    "        ).unwrapped\n",
    "    )\n",
    "\n",
    "for i,N_ in enumerate(N_list):\n",
    "#     AC3_list[i].optimizerA.learning_rate = 0.05\n",
    "#     AC3_list[i].optimizerC.learning_rate = 0.08\n",
    "    AC3_hist[i] += train(AC3_list[i], env_list[i], \n",
    "              num_episode=num_episode, test_interval=test_interval, num_test=num_test, num_iteration=num_iteration, \n",
    "              BATCH_SIZE=BATCH_SIZE, num_sample=num_sample, action_space=[-1,1], debug=debug,\n",
    "              update_mode=UPDATE_PER_ITERATION, reward_mode=FUTURE_REWARD_YES_NORMALIZE, loss_history=AC3_loss[i])\n",
    "    print(\"Finished training env with {0} agents for AC\".format(N_))\n",
    "\n",
    "for i,N_ in enumerate(N_list):\n",
    "    AC3_test_hist[i] += plot_test(AC3_list[i], env_list[i], fnames=['']*num_test,\n",
    "            num_iteration=num_iteration, action_space=action_space, imdir='screencaps/',debug=debug)\n",
    "    print(\"Finished testnig env with {0} agents for AC\".format(N_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skip=1\n",
    "plot_reward_hist([h[::skip] for h in AC3_hist], test_interval*skip, \n",
    "                 ['AC3_N{0}'.format(N_) for N_ in N_list], \n",
    "                 log=False, num_iteration=num_iteration, N_list=([1 for N in N_list]), bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot loss per # of agent\n",
    "skip=1\n",
    "plot_loss_hist(hists=[h[::skip] for h in AC3_loss], hist_names=['AC3_N{0}'.format(N_) for N_ in N_list], log=False, \n",
    "               num_iteration=num_iteration, update_mode=UPDATE_PER_ITERATION, bar=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model to models/AC3Agent_AC3_NegGauss_distreward_normalized_N5\n",
      "Saving model to models/AC3Agent_AC3_NegGauss_distreward_normalized_N10\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-89-9fcf6ead5f7b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mAC3_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"AC3_NegGauss_distreward_normalized_N5\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mAC3_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"AC3_NegGauss_distreward_normalized_N10\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mAC3_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"AC3_NegGauss_distreward_normalized_N20\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "AC3_list[0].save_model(\"AC3_NegGauss_distreward_normalized_N5\")\n",
    "AC3_list[1].save_model(\"AC3_NegGauss_distreward_normalized_N10\")\n",
    "AC3_list[2].save_model(\"AC3_NegGauss_distreward_normalized_N20\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished episode  0  with  99 ;\n",
      "cumulative reward =  -5021.702440149778\n",
      "Finished episode  0  with  99 ;\n",
      "cumulative reward =  -12571.596065265274\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "### Save their animations and see how they behave.\n",
    "for i,env_ in enumerate(env_list):\n",
    "    plot_test(AC3_list[i], env_, fnames=['AC3_distreward_normalized_N{1}_test{0}'.format(j,env_.N) for j in range(1)],\n",
    "        num_iteration=100, action_space=action_space, imdir='screencaps/',debug=debug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### The above, but with all reward terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_episode=500\n",
    "test_interval=10#0\n",
    "num_test=10#50\n",
    "env_list_allreward = []\n",
    "N_list = [5,10,20]\n",
    "for N_ in N_list:\n",
    "    env_list_allreward.append(\n",
    "        gym.make('ConsensusEnv:ConsensusContEnv-v0', N=N_, dt=0.1, Delta=0.05,\n",
    "              input_type=input_type, observe_type=observe_type, observe_action=O_ACTION, reward_mode=ALL_REWARD\n",
    "        ).unwrapped\n",
    "    )\n",
    "\n",
    "AC3_list_allreward = []\n",
    "for i,N_ in enumerate(N_list):\n",
    "    AC3_list_allreward.append(\n",
    "        AC2Agent(device, N_, env_list_allreward[i].nf, env_list_allreward[i].na, hidden, \n",
    "                 learning_rateA=0.01, learning_rateC=0.02)\n",
    "    )\n",
    "\n",
    "AC3_hist_allreward = []\n",
    "AC3_loss_allreward = []\n",
    "for i,N_ in enumerate(N_list):\n",
    "    AC3_loss_allreward.append([])\n",
    "    AC3_hist_allreward.append(\n",
    "        train(AC3_list_allreward[i], env_list_allreward[i], \n",
    "              num_episode=num_episode, test_interval=test_interval, num_test=num_test, num_iteration=num_iteration, \n",
    "              BATCH_SIZE=BATCH_SIZE, num_sample=num_sample, action_space=[-1,1], debug=debug,\n",
    "              update_mode=UPDATE_PER_ITERATION, reward_mode=FUTURE_REWARD_YES_NORMALIZE, loss_history=AC3_loss_allreward[i])\n",
    "    )\n",
    "    print(\"Finished training env with {0} agents for AC\".format(N_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skip = 1\n",
    "plot_reward_hist([h[2::skip] for h in AC3_hist_allreward], test_interval*skip, \n",
    "                 ['AC3_N{0}'.format(N_) for N_ in N_list], \n",
    "                 log=False, num_iteration=num_iteration, N_list=([1 for N in N_list]), bar=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot loss\n",
    "skip=1\n",
    "plot_loss_hist(hists=[h[5::skip] for h in AC3_loss_allreward], hist_names=['AC3_N{0}'.format(N_) for N_ in N_list], log=False, \n",
    "               num_iteration=num_iteration, update_mode=UPDATE_PER_ITERATION, bar=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model to models/AC2Agent_AC3_allreward_normalized_N5\n",
      "Saving model to models/AC2Agent_AC3_allreward_normalized_N10\n",
      "Saving model to models/AC2Agent_AC3_allreward_normalized_N20\n"
     ]
    }
   ],
   "source": [
    "AC3_list_allreward[0].save_model(\"AC3_allreward_normalized_N5\")\n",
    "AC3_list_allreward[1].save_model(\"AC3_allreward_normalized_N10\")\n",
    "AC3_list_allreward[2].save_model(\"AC3_allreward_normalized_N20\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished episode  0  with  99 ;\n",
      "cumulative reward =  -4878.524448529784\n",
      "Finished episode  1  with  99 ;\n",
      "cumulative reward =  -4878.9808283917355\n",
      "Finished episode  2  with  99 ;\n",
      "cumulative reward =  -4935.346769331861\n",
      "Finished episode  0  with  99 ;\n",
      "cumulative reward =  -38275.258413423464\n",
      "Finished episode  1  with  99 ;\n",
      "cumulative reward =  -40579.144749352716\n",
      "Finished episode  2  with  99 ;\n",
      "cumulative reward =  -45368.396375618795\n",
      "Finished episode  0  with  99 ;\n",
      "cumulative reward =  -124197.28412464196\n",
      "Finished episode  1  with  99 ;\n",
      "cumulative reward =  -159430.7937167494\n",
      "Finished episode  2  with  99 ;\n",
      "cumulative reward =  -119916.70460990266\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "reload(utils.plotting)\n",
    "from utils.plotting import *\n",
    "### Save their animations and see how they behave.\n",
    "for i,N_ in enumerate(N_list):\n",
    "    plot_test(AC3_list[i], env_list[i], fnames=['AC3_dist_reward_test{0}_N{1}'.format(j,N_) for j in range(1)],\n",
    "        num_iteration=100, action_space=action_space, imdir='screencaps/',debug=debug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear some histories\n",
    "# AC3_hist_allreward = []\n",
    "# AC3_loss_allreward = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try out AC with more interesting reward options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test out Actor-Critic methods. Note that tuning parameters might be required.\n",
    "reload(utils.ReplayMemory)\n",
    "reload(utils.networks)\n",
    "reload(utils.agents)\n",
    "reload(utils.plotting)\n",
    "reload(utils.train_test_methods)\n",
    "from utils.ReplayMemory import * \n",
    "from utils.networks import *\n",
    "from utils.agents import *\n",
    "from utils.plotting import *\n",
    "from utils.train_test_methods import *\n",
    "\n",
    "num_episode=500\n",
    "test_interval=10#0\n",
    "num_test=10#50\n",
    "num_iteration=200\n",
    "BATCH_SIZE=64#128\n",
    "debug=False\n",
    "num_sample=50\n",
    "seed=22222\n",
    "\n",
    "N_listv = [5,10]#,20]\n",
    "env_listv = []\n",
    "for N_ in N_listv:\n",
    "    # Distance-based reward only, with hard penalty on touching the boundary.\n",
    "    # Control group that doesn't give surviving reward, and instead stops immediately.\n",
    "    env_listv.append(\n",
    "        gym.make('ConsensusEnv:ConsensusContEnv-v0', N=N_, dt=0.1, Delta=0.05,\n",
    "              input_type=input_type, observe_type=observe_type, observe_action=O_ACTION, reward_mode=DIST_REWARD\n",
    "        ).unwrapped\n",
    "    )\n",
    "    # Drop-dead immediately on touching the boundary\n",
    "    env_listv.append(\n",
    "        gym.make('ConsensusEnv:ConsensusContEnv-v0', N=N_, dt=0.1, Delta=0.05,\n",
    "              input_type=input_type, observe_type=observe_type, observe_action=O_ACTION, reward_mode=DIST_REWARD,\n",
    "                 boundary_policy=DEAD_ON_TOUCH\n",
    "        ).unwrapped\n",
    "    )\n",
    "    # Hard penalty on boundary, coupled with positive convergence reward. This means it never stops on consensus.\n",
    "    env_listv.append(\n",
    "        gym.make('ConsensusEnv:ConsensusContEnv-v0', N=N_, dt=0.1, Delta=0.05,\n",
    "              input_type=input_type, observe_type=observe_type, observe_action=O_ACTION, reward_mode=DIST_REWARD, \n",
    "                 finish_reward_policy=REWARD_IF_CONSENSUS\n",
    "        ).unwrapped\n",
    "    )\n",
    "    # Soft penalty with consensus rewards. I don't expect it to successfully discover\n",
    "    # achieving consensus would bring reward, though.\n",
    "    env_listv.append(\n",
    "        gym.make('ConsensusEnv:ConsensusContEnv-v0', N=N_, dt=0.1, Delta=0.05,\n",
    "              input_type=input_type, observe_type=observe_type, observe_action=O_ACTION, reward_mode=DIST_REWARD,\n",
    "                 boundary_policy=SOFT_PENALTY, finish_reward_policy=REWARD_IF_CONSENSUS\n",
    "        ).unwrapped\n",
    "    )\n",
    "    # Comparison group that uses dist and actuation rewards, with hard penalty\n",
    "    env_listv.append(\n",
    "        gym.make('ConsensusEnv:ConsensusContEnv-v0', N=N_, dt=0.1, Delta=0.05,\n",
    "              input_type=input_type, observe_type=observe_type, observe_action=O_ACTION, \n",
    "                 reward_mode=(DIST_REWARD|ACT_REWARD)\n",
    "        ).unwrapped\n",
    "    )\n",
    "\n",
    "labels = ['hard_bound_zero_posReward', \n",
    "          'dead_bound_zero_posReward', \n",
    "          'hard_bound_cumu_posReward', \n",
    "          'soft_bound_cumu_posReward', \n",
    "          'hard_bound_zero_posReward_v_penalty']*2\n",
    "labels = [labels[i]+'_N{0}'.format(env_.N) for i,env_ in enumerate(env_listv)]\n",
    "\n",
    "AC2_listv = []\n",
    "for i,env_ in enumerate(env_listv):\n",
    "    AC2_listv.append(\n",
    "        AC2Agent(device, env_.N, env_.nf, env_.na, hidden, rand_modeA=rand_mode,\n",
    "                 learning_rateA=0.01, learning_rateC=0.02)\n",
    "    )\n",
    "\n",
    "AC2_histv = []\n",
    "AC2_lossv = []\n",
    "for i,env_ in enumerate(env_listv):\n",
    "    AC2_lossv.append([])\n",
    "    AC2_histv.append(\n",
    "        train(AC2_listv[i], env_, \n",
    "              num_episode=num_episode, test_interval=test_interval, num_test=num_test, num_iteration=num_iteration, \n",
    "              BATCH_SIZE=BATCH_SIZE, num_sample=num_sample, action_space=[-1,1], debug=debug,\n",
    "#               update_mode=UPDATE_PER_ITERATION, reward_mode=FUTURE_REWARD_YES_NORMALIZE, loss_history=AC2_loss[i])\n",
    "              # Using normalization might ruine the effort?\n",
    "              update_mode=UPDATE_PER_ITERATION, reward_mode=FUTURE_REWARD_YES_NORMALIZE, loss_history=AC2_lossv[i])\n",
    "    )\n",
    "    print(\"Finished training env with {0} agents for AC\".format(env_.N))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# additional training\n",
    "num_episode=500\n",
    "test_interval=10\n",
    "num_test=10#50\n",
    "num_iteration=200\n",
    "\n",
    "for i,env_ in enumerate(env_listv):\n",
    "    AC2_listv[i].optimizerA.learning_rate = 0.1\n",
    "    AC2_listv[i].optimizerC.learning_rate = 0.1\n",
    "    AC2_histv[i] += train(AC2_listv[i], env_, \n",
    "              num_episode=num_episode, test_interval=test_interval, num_test=num_test, num_iteration=num_iteration, \n",
    "              BATCH_SIZE=BATCH_SIZE, num_sample=num_sample, action_space=[-1,1], debug=debug,\n",
    "              update_mode=UPDATE_PER_ITERATION, reward_mode=FUTURE_REWARD_YES_NORMALIZE, loss_history=AC2_lossv[i])\n",
    "    print(\"Finished training \"+labels[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(utils.plotting)\n",
    "from utils.plotting import *\n",
    "\n",
    "skip = 1\n",
    "i = 9\n",
    "plot_reward_hist([AC2_histv[i][::skip]], test_interval*skip, \n",
    "                 [labels[i]], # ['AC2_N{0}'.format(env_list[i].N)], \n",
    "                 log=False, num_iteration=num_iteration, \n",
    "                 N_list=[env_listv[i].N], # ([1 for env_ in env_list]), \n",
    "                 bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot loss per # of agent\n",
    "skip=1\n",
    "# i = 0\n",
    "# plot_loss_hist(hists=[AC2_loss[i][::skip]], \n",
    "#                hist_names=[labels[i]], \n",
    "#                log=False, num_iteration=num_iteration, update_mode=UPDATE_PER_ITERATION, bar=True)\n",
    "plot_loss_hist(hists=[h[::skip] for h in AC2_lossv], hist_names=labels, \n",
    "               log=False, num_iteration=num_iteration, update_mode=UPDATE_PER_ITERATION, bar=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model to models/AC2Agent_AC2_GaussTest_distreward_normalized_hard_bound_zero_posReward_N5\n",
      "Saving model to models/AC2Agent_AC2_GaussTest_distreward_normalized_dead_bound_zero_posReward_N5\n",
      "Saving model to models/AC2Agent_AC2_GaussTest_distreward_normalized_hard_bound_cumu_posReward_N5\n",
      "Saving model to models/AC2Agent_AC2_GaussTest_distreward_normalized_soft_bound_cumu_posReward_N5\n",
      "Saving model to models/AC2Agent_AC2_GaussTest_distreward_normalized_hard_bound_zero_posReward_v_penalty_N5\n",
      "Saving model to models/AC2Agent_AC2_GaussTest_distreward_normalized_hard_bound_zero_posReward_N10\n",
      "Saving model to models/AC2Agent_AC2_GaussTest_distreward_normalized_dead_bound_zero_posReward_N10\n",
      "Saving model to models/AC2Agent_AC2_GaussTest_distreward_normalized_hard_bound_cumu_posReward_N10\n",
      "Saving model to models/AC2Agent_AC2_GaussTest_distreward_normalized_soft_bound_cumu_posReward_N10\n",
      "Saving model to models/AC2Agent_AC2_GaussTest_distreward_normalized_hard_bound_zero_posReward_v_penalty_N10\n"
     ]
    }
   ],
   "source": [
    "for i,lab in enumerate(labels):\n",
    "    AC2_listv[i].save_model(\"AC2_GaussTest_distreward_normalized_\"+lab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Took  21  steps to converge\n",
      "Finished episode  0  with  21 ;\n",
      "cumulative reward =  -54302210.1283577\n",
      "Took  6  steps to converge\n",
      "Finished episode  0  with  6 ;\n",
      "cumulative reward =  -10100299.503687605\n",
      "Took  21  steps to converge\n",
      "Finished episode  0  with  21 ;\n",
      "cumulative reward =  -53702239.19595753\n",
      "Finished episode  0  with  99 ;\n",
      "cumulative reward =  -20397.49118748304\n",
      "Took  20  steps to converge\n",
      "Finished episode  0  with  20 ;\n",
      "cumulative reward =  -53702669.53144223\n",
      "Took  22  steps to converge\n",
      "Finished episode  0  with  22 ;\n",
      "cumulative reward =  -103706912.39399435\n",
      "Took  15  steps to converge\n",
      "Finished episode  0  with  15 ;\n",
      "cumulative reward =  -10104119.727468826\n",
      "Took  16  steps to converge\n",
      "Finished episode  0  with  16 ;\n",
      "cumulative reward =  -105802616.69716345\n",
      "Finished episode  0  with  99 ;\n",
      "cumulative reward =  -75435.10133687276\n",
      "Took  26  steps to converge\n",
      "Finished episode  0  with  26 ;\n",
      "cumulative reward =  -109013444.81394063\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "### Save their animations and see how they behave.\n",
    "for i,env_ in enumerate(env_listv):\n",
    "    plot_test(AC2_listv[i], env_, fnames=['AC2_distreward_normalized'+labels[i]+'_test{0}'.format(j) for j in range(1)],\n",
    "        num_iteration=100, action_space=action_space, imdir='screencaps/',debug=debug)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try using Additive noise instead of Gaussian or noise-free\n",
    "The environment reward configs here are slightly different from the ones above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test out Actor-Critic with noise\n",
    "reload(utils.ReplayMemory)\n",
    "reload(utils.networks)\n",
    "reload(utils.agents)\n",
    "reload(utils.plotting)\n",
    "reload(utils.train_test_methods)\n",
    "from utils.ReplayMemory import * \n",
    "from utils.networks import *\n",
    "from utils.agents import *\n",
    "from utils.plotting import *\n",
    "from utils.train_test_methods import *\n",
    "\n",
    "num_episode=500\n",
    "test_interval=10\n",
    "num_test=10#50\n",
    "num_iteration=200\n",
    "\n",
    "N_list = [5,10,20]\n",
    "env_listr = []\n",
    "for N_ in N_list:\n",
    "    # Distance-based reward only, with hard penalty on touching the boundary.\n",
    "    # Control group that doesn't give surviving reward, and instead stops immediately.\n",
    "    env_listr.append(\n",
    "        gym.make('ConsensusEnv:ConsensusContEnv-v0', N=N_, dt=0.1, Delta=0.05,\n",
    "              input_type=input_type, observe_type=observe_type, observe_action=O_ACTION, reward_mode=DIST_REWARD\n",
    "        ).unwrapped\n",
    "    )\n",
    "    # Drop-dead immediately on touching the boundary\n",
    "    env_listr.append(\n",
    "        gym.make('ConsensusEnv:ConsensusContEnv-v0', N=N_, dt=0.1, Delta=0.05,\n",
    "              input_type=input_type, observe_type=observe_type, observe_action=O_ACTION, reward_mode=DIST_REWARD,\n",
    "                 boundary_policy=DEAD_ON_TOUCH\n",
    "        ).unwrapped\n",
    "    )\n",
    "    # Hard penalty on boundary, coupled with positive convergence reward. This means it never stops on consensus.\n",
    "    env_listr.append(\n",
    "        gym.make('ConsensusEnv:ConsensusContEnv-v0', N=N_, dt=0.1, Delta=0.05,\n",
    "              input_type=input_type, observe_type=observe_type, observe_action=O_ACTION, reward_mode=DIST_REWARD, \n",
    "                 finish_reward_policy=REWARD_IF_CONSENSUS\n",
    "        ).unwrapped\n",
    "    )\n",
    "    # Soft penalty with consensus rewards. I don't expect it to successfully discover\n",
    "    # achieving consensus would bring reward, though.\n",
    "    env_listr.append(\n",
    "        gym.make('ConsensusEnv:ConsensusContEnv-v0', N=N_, dt=0.1, Delta=0.05,\n",
    "              input_type=input_type, observe_type=observe_type, observe_action=O_ACTION, reward_mode=DIST_REWARD,\n",
    "                 boundary_policy=SOFT_PENALTY, finish_reward_policy=REWARD_IF_CONSENSUS\n",
    "        ).unwrapped\n",
    "    )\n",
    "    # Comparison group that uses dist and actuation rewards, with hard penalty\n",
    "    env_listr.append(\n",
    "        gym.make('ConsensusEnv:ConsensusContEnv-v0', N=N_, dt=0.1, Delta=0.05,\n",
    "              input_type=input_type, observe_type=observe_type, observe_action=O_ACTION, \n",
    "                 reward_mode=(DIST_REWARD|ACT_REWARD)\n",
    "        ).unwrapped\n",
    "    )\n",
    "\n",
    "AC2_listr = []\n",
    "for i,env_ in enumerate(env_listr):\n",
    "    AC2_listr.append(\n",
    "        AC2Agent(device, env_.N, env_.nf, env_.na, hidden, add_noise=True,\n",
    "                 learning_rateA=0.01, learning_rateC=0.02)\n",
    "    )\n",
    "\n",
    "AC2_histr = []\n",
    "AC2_lossr = []\n",
    "for i,env_ in enumerate(env_listr):\n",
    "    AC2_lossr.append([])\n",
    "    AC2_histr.append(\n",
    "        train(AC2_listr[i], env_, \n",
    "              num_episode=num_episode, test_interval=test_interval, num_test=num_test, num_iteration=num_iteration, \n",
    "              BATCH_SIZE=BATCH_SIZE, num_sample=num_sample, action_space=[-1,1], debug=debug,\n",
    "#               update_mode=UPDATE_PER_ITERATION, reward_mode=FUTURE_REWARD_YES_NORMALIZE, loss_history=AC2_loss[i])\n",
    "              # Using normalization might ruine the effort?\n",
    "              update_mode=UPDATE_PER_ITERATION, reward_mode=FUTURE_REWARD_YES_NORMALIZE, loss_history=AC2_lossr[i])\n",
    "    )\n",
    "    print(\"Finished training env with {0} agents for AC\".format(env_.N))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# additional training\n",
    "num_episode=500\n",
    "test_interval=10\n",
    "num_test=10#50\n",
    "num_iteration=200\n",
    "\n",
    "for i,env_ in enumerate(env_listr):\n",
    "    AC2_listr[i].optimizerA.learning_rate = 0.1\n",
    "    AC2_listr[i].optimizerC.learning_rate = 0.1\n",
    "    AC2_histr[i] += train(AC2_listr[i], env_, \n",
    "              num_episode=num_episode, test_interval=test_interval, num_test=num_test, num_iteration=num_iteration, \n",
    "              BATCH_SIZE=BATCH_SIZE, num_sample=num_sample, action_space=[-1,1], debug=debug,\n",
    "              update_mode=UPDATE_PER_ITERATION, reward_mode=FUTURE_REWARD_YES_NORMALIZE, loss_history=AC2_lossr[i])\n",
    "    print(\"Finished training \"+labels[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skip = 1\n",
    "i = 4\n",
    "plot_reward_hist([AC2_histr[i][::skip]], test_interval*skip, \n",
    "                 [labels[i]], # ['AC2_N{0}'.format(env_list[i].N)], \n",
    "                 log=False, num_iteration=num_iteration, \n",
    "                 N_list=[env_listr[i].N], # ([1 for env_ in env_list]), \n",
    "                 bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot loss per # of agent\n",
    "skip=1\n",
    "# i = 0\n",
    "# plot_loss_hist(hists=[AC2_lossr[i][::skip]], \n",
    "#                hist_names=[labels[i]], \n",
    "#                log=False, num_iteration=num_iteration, update_mode=UPDATE_PER_ITERATION, bar=True)\n",
    "plot_loss_hist(hists=[h[::skip] for h in AC2_lossr], hist_names=labels, \n",
    "               log=False, num_iteration=num_iteration, update_mode=UPDATE_PER_ITERATION, bar=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,lab in enumerate(labels[:5]):\n",
    "    AC2_listr[i].save_model(\"AC2_randnoise_distreward_normalized_\"+lab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Save their animations and see how they behave.\n",
    "for i,env_ in enumerate(env_listr):\n",
    "    plot_test(AC2_listr[i], env_, fnames=['AC2_noise_distreward_normalized'+labels[i]+'_test{0}'.format(j) for j in range(2)],\n",
    "        num_iteration=100, action_space=action_space, imdir='screencaps/',debug=debug)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DDPG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(utils.agents)\n",
    "from utils.agents import *\n",
    "\n",
    "num_episode=500\n",
    "test_interval=50\n",
    "num_test=50\n",
    "num_iteration=200\n",
    "BATCH_SIZE=128\n",
    "debug=False\n",
    "num_sample=50\n",
    "\n",
    "# Do DDPG\n",
    "DDPG_list = []\n",
    "for i,N_ in enumerate(N_list):\n",
    "    DDPG_list.append(\n",
    "        DDPGAgent(device, N_, env_list[i].no, env_list[i].na, hidden)\n",
    "    )\n",
    "DDPG_list.append(\n",
    "    DDPGAgent(device, 10, env.no, env.na, hidden)\n",
    ")\n",
    "\n",
    "DDPG_hist = []\n",
    "for i,N_ in enumerate(N_list):\n",
    "    DDPG_hist.append(\n",
    "        train(DDPG_list[i], env_list[i], \n",
    "              num_episode, test_interval, num_test, num_iteration, BATCH_SIZE, num_sample, [-1,1], debug)\n",
    "    )\n",
    "    print(\"Finished training env with {0} agents for AC\".format(N_))\n",
    "DDPG_hist.append(\n",
    "    train(DDPG_list[-1], env, \n",
    "          num_episode, test_interval, num_test, num_iteration, BATCH_SIZE, num_sample, [-1,1], debug)\n",
    ")\n",
    "    \n",
    "DDPG_test_hist = []\n",
    "for i,N_ in enumerate(N_list):\n",
    "    DDPG_test_hist.append(\n",
    "        plot_test(DDPG_list[i], env_list[i], fnames=['']*num_test,\n",
    "            num_iteration=num_iteration, action_space=action_space, imdir='screencaps/', debug=debug)\n",
    "    )\n",
    "    print(\"Finished testnig env with {0} agents for AC\".format(N_))\n",
    "DDPG_test_hist.append(\n",
    "    plot_test(DDPG_list[-1], env, fnames=['']*num_test,\n",
    "        num_iteration=num_iteration, action_space=action_space, imdir='screencaps/', debug=debug)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "DDPG_test_hist = []\n",
    "for i,N_ in enumerate(N_list):\n",
    "    DDPG_test_hist.append(\n",
    "        plot_test(DDPG_list[i], env_list[i], fnames=['']*num_test,\n",
    "            num_iteration=num_iteration, action_space=action_space, imdir='screencaps/', debug=debug)\n",
    "    )\n",
    "    print(\"Finished testnig env with {0} agents for AC\".format(N_))\n",
    "DDPG_test_hist.append(\n",
    "    plot_test(DDPG_list[-1], env, fnames=['']*num_test,\n",
    "        num_iteration=num_iteration, action_space=action_space, imdir='screencaps/', debug=debug)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_reward_hist(DDPG_hist, test_interval, \n",
    "                 ['DDPG_N{0}'.format(N_) for N_ in N_list]+['DDPG_N10'], \n",
    "                 log=False, num_iteration=num_iteration, N_list=([np.sqrt(N) for N in N_list]+[np.sqrt(10)])*2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_reward_hist([[l] for l in DDPG_test_hist], test_interval, \n",
    "                 ['DDPG_N{0}'.format(N_) for N_ in N_list]+['DDPG_N10'], \n",
    "                 log=False, num_iteration=num_iteration, N_list=([N for N in N_list]+[10])*2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def save_model(agent, suffix=\"\", agent_path=None):\n",
    "    if not os.path.exists('models/'):\n",
    "        os.makedirs('models/')\n",
    "\n",
    "    if len(suffix) <= 0:\n",
    "        suffix = datetime.now().strftime('%Y_%m_%d_%H_%M_%S')\n",
    "    if agent_path is None:\n",
    "        agent_path = \"models/{}_{}\".format(agent.name, suffix)\n",
    "    print('Saving model to {}'.format(agent_path))\n",
    "    torch.save(agent.netA.state_dict(), agent_path+\"_A\")\n",
    "    torch.save(agent.netC.state_dict(), agent_path+\"_C\")\n",
    "    \n",
    "for i,N_ in enumerate(N_list+[10]):\n",
    "    save_model(AC2_list[i],'AC2_test2_N{0}'.format(N_))\n",
    "    \n",
    "def save_model(agent, suffix=\"\", agent_path=None):\n",
    "    if not os.path.exists('models/'):\n",
    "        os.makedirs('models/')\n",
    "\n",
    "    if len(suffix) <= 0:\n",
    "        suffix = datetime.now().strftime('%Y_%m_%d_%H_%M_%S')\n",
    "    if agent_path is None:\n",
    "        agent_path = \"models/{}_{}\".format(agent.name, suffix)\n",
    "    print('Saving model to {}'.format(agent_path))\n",
    "    torch.save(agent.netA.state_dict(), agent_path+\"_A\")\n",
    "    torch.save(agent.netC.state_dict(), agent_path+\"_C\")\n",
    "    torch.save(agent.netAT.state_dict(), agent_path+\"_AT\")\n",
    "    torch.save(agent.netCT.state_dict(), agent_path+\"_CT\")\n",
    "    \n",
    "for i,N_ in enumerate(N_list+[10]):\n",
    "    save_model(DDPG_list[i],'DDPG_test2_N{0}'.format(N_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test out different training setups and compare\n",
    "using the new reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(utils.networks)\n",
    "reload(utils.agents)\n",
    "reload(utils.train_test_methods)\n",
    "from utils.networks import *\n",
    "from utils.agents import *\n",
    "from utils.train_test_methods import *\n",
    "\n",
    "num_episode=250\n",
    "test_interval=25\n",
    "num_test=25\n",
    "num_iteration=200\n",
    "BATCH_SIZE=128\n",
    "debug=False\n",
    "num_sample=50\n",
    "transfer_num_episode=200\n",
    "seed=2333333\n",
    "\n",
    "# Bulid environments with different number of agents\n",
    "env_list = []\n",
    "env5_list = []\n",
    "N_list = [5]#, 10, 20, 32]#, 50, 64]\n",
    "for N_ in N_list:\n",
    "    env_list.append(\n",
    "        gym.make('ConsensusEnv:ConsensusContEnv-v0', N=N_, dt=0.1, Delta=0.05,\n",
    "              input_type=input_type, observe_type=observe_type, observe_action=O_NO_ACTION\n",
    "        ).unwrapped\n",
    "    )\n",
    "    env5_list.append(\n",
    "        gym.make('ConsensusEnv:ConsensusContEnv-v0', N=N_, dt=0.1, Delta=0.05,\n",
    "              input_type=input_type, observe_type=observe_type, observe_action=O_ACTION\n",
    "        ).unwrapped\n",
    "    )\n",
    "\n",
    "# Initialize 4 different groups of agents\n",
    "# Type 1: Learn from scratch using cumulative reward \n",
    "RA_cumu_list = []\n",
    "# Type 2: Learn from scratch using non-cumulative reward \n",
    "RA_inst_list = []\n",
    "# Type 3: Learn from scratch using normalized cumulative reward \n",
    "RA_norm_list = []\n",
    "# Type 4: Transfer learning to the new environment from models saved using old reward function\n",
    "RA_trsf_list = []\n",
    "RApath = 'models/RewardAgent_RA32_master_agent'\n",
    "prevN = 32\n",
    "# Type 4, v2: Trans train using cumulative reward\n",
    "RA_trfc_list = []\n",
    "# Type 5: Learn while observing others' past actions\n",
    "RA_obac_list = []\n",
    "\n",
    "for i,N_ in enumerate(N_list):\n",
    "    RA_cumu_list.append(\n",
    "        RewardAgent(device, N_, env_list[i].no, env_list[i].na, hidden, learning_rate=0.1)\n",
    "    )\n",
    "    RA_inst_list.append(\n",
    "        RewardAgent(device, N_, env_list[i].no, env_list[i].na, hidden, learning_rate=0.1)\n",
    "    )\n",
    "    RA_norm_list.append(\n",
    "        RewardAgent(device, N_, env_list[i].no, env_list[i].na, hidden, learning_rate=0.1)\n",
    "    )\n",
    "    RA_trsf_list.append(\n",
    "        RewardAgent(device, N_, env_list[i].no, env_list[i].na, hidden, learning_rate=0.1, prevN=prevN, load_path=RApath)\n",
    "    )\n",
    "    RA_trfc_list.append(\n",
    "        RewardAgent(device, N_, env_list[i].no, env_list[i].na, hidden, learning_rate=0.1, prevN=prevN, load_path=RApath)\n",
    "    )\n",
    "    RA_obac_list.append(\n",
    "        RewardAgent(device, N_, env5_list[i].nf, env_list[i].na, hidden, learning_rate=0.1)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train those agents one by one, I guess\n",
    "RA_cumu_hists = []\n",
    "RA_inst_hists = []\n",
    "RA_norm_hists = []\n",
    "RA_trsf_hists = []\n",
    "RA_obac_hists = []\n",
    "RA_cumu_lossess = [[] for i in N_list]\n",
    "RA_inst_lossess = [[] for i in N_list]\n",
    "RA_norm_lossess = [[] for i in N_list]\n",
    "RA_trsf_lossess = [[] for i in N_list]\n",
    "RA_obac_lossess = [[] for i in N_list]\n",
    "RA_trfc_lossess = [[] for i in N_list]\n",
    "RA_cumu_lr_hist = [[] for i in N_list]\n",
    "RA_inst_lr_hist = [[] for i in N_list]\n",
    "RA_norm_lr_hist = [[] for i in N_list]\n",
    "RA_trsf_lr_hist = [[] for i in N_list]\n",
    "RA_obac_lr_hist = [[] for i in N_list]\n",
    "RA_trfc_lr_hist = [[] for i in N_list]\n",
    "for i,N_ in enumerate(N_list):\n",
    "    RA_cumu_hists.append(\n",
    "        train(RA_cumu_list[i], env_list[i], \n",
    "              num_episode, test_interval, num_test, num_iteration, BATCH_SIZE, num_sample, [-1,1], debug, \n",
    "              seed=seed, reward_mode=FUTURE_REWARD_YES, loss_history=RA_cumu_lossess[i], lr_history=RA_cumu_lr_hist[i]\n",
    "        )\n",
    "    )\n",
    "    RA_inst_hists.append(\n",
    "        train(RA_inst_list[i], env_list[i], \n",
    "              num_episode, test_interval, num_test, num_iteration, BATCH_SIZE, num_sample, [-1,1], debug, \n",
    "              seed=seed, reward_mode=FUTURE_REWARD_NO, loss_history=RA_inst_lossess[i], lr_history=RA_inst_lr_hist[i]\n",
    "        )\n",
    "    )\n",
    "    RA_norm_hists.append(\n",
    "        train(RA_norm_list[i], env_list[i], \n",
    "              num_episode, test_interval, num_test, num_iteration, BATCH_SIZE, num_sample, [-1,1], debug, \n",
    "              seed=seed, reward_mode=FUTURE_REWARD_YES_NORMALIZE, loss_history=RA_norm_lossess[i], lr_history=RA_norm_lr_hist[i]\n",
    "        )\n",
    "    )\n",
    "    RA_trsf_hists.append(\n",
    "        train(RA_trsf_list[i], env_list[i], \n",
    "              transfer_num_episode, test_interval, num_test, num_iteration, BATCH_SIZE, num_sample, [-1,1], debug, \n",
    "              seed=seed, reward_mode=FUTURE_REWARD_NO, loss_history=RA_trsf_lossess[i], lr_history=RA_trsf_lr_hist[i]\n",
    "        )\n",
    "    )\n",
    "    RA_obac_hists.append(\n",
    "        train(RA_obac_list[i], env5_list[i], \n",
    "              num_episode, test_interval, num_test, num_iteration, BATCH_SIZE, num_sample, [-1,1], debug, \n",
    "              seed=seed, reward_mode=FUTURE_REWARD_YES, loss_history=RA_obac_lossess[i], lr_history=RA_obac_lr_hist[i]\n",
    "        )\n",
    "    )\n",
    "    RA_trfc_hists.append(\n",
    "        train(RA_trfc_list[i], env_list[i], \n",
    "              transfer_num_episode, test_interval, num_test, num_iteration, BATCH_SIZE, num_sample, [-1,1], debug, \n",
    "              seed=seed, reward_mode=FUTURE_REWARD_YES, loss_history=RA_trfc_lossess[i], lr_history=RA_trfc_lr_hist[i]\n",
    "        )\n",
    "    )\n",
    "    print(\"Finished training env with {0} agents for RA\".format(N_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_episode=250\n",
    "for i,N_ in enumerate(N_list):\n",
    "    RA_cumu_hists[i]+=train(RA_cumu_list[i], env_list[i], \n",
    "              num_episode, test_interval, num_test, num_iteration, BATCH_SIZE, num_sample, [-1,1], debug, \n",
    "              seed=seed, reward_mode=FUTURE_REWARD_YES, loss_history=RA_cumu_lossess[i]\n",
    "        )\n",
    "    \n",
    "    RA_inst_hists[i]+=train(RA_inst_list[i], env_list[i], \n",
    "              num_episode, test_interval, num_test, num_iteration, BATCH_SIZE, num_sample, [-1,1], debug, \n",
    "              seed=seed, reward_mode=FUTURE_REWARD_NO, loss_history=RA_inst_lossess[i]\n",
    "        )\n",
    "    \n",
    "    RA_norm_hists[i]+=train(RA_norm_list[i], env_list[i], \n",
    "              num_episode, test_interval, num_test, num_iteration, BATCH_SIZE, num_sample, [-1,1], debug, \n",
    "              seed=seed, reward_mode=FUTURE_REWARD_YES_NORMALIZE, loss_history=RA_norm_lossess[i]\n",
    "        )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RA_trfc_hists = []\n",
    "for i,N_ in enumerate(N_list):\n",
    "    RA_trfc_hists.append(\n",
    "        train(RA_trfc_list[i], env_list[i], \n",
    "              transfer_num_episode, test_interval, num_test, num_iteration, BATCH_SIZE, num_sample, [-1,1], debug, \n",
    "              seed=seed, reward_mode=FUTURE_REWARD_YES\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,N_ in enumerate(N_list):\n",
    "    RA_cumu_list[i].save_model('rewardV2_N{0}_cumu_test1'.format(N_))\n",
    "    RA_inst_list[i].save_model('rewardV2_N{0}_inst_test1'.format(N_))\n",
    "    RA_norm_list[i].save_model('rewardV2_N{0}_norm_test1'.format(N_))\n",
    "    RA_trsf_list[i].save_model('rewardV2_N{0}_trsf_test1'.format(N_))\n",
    "    RA_obac_list[i].save_model('rewardV2_N{0}_obac_test1'.format(N_))\n",
    "    RA_trfc_list[i].save_model('rewardV2_N{0}_trfc_test1'.format(N_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot per group\n",
    "comb_hists = [RA_cumu_hists,RA_inst_hists,RA_norm_hists,RA_trsf_hists,RA_trfc_hists,RA_obac_hists]\n",
    "comb_names = [['RA_{1}_N{0}'.format(N_,name) for N_ in N_list] for name in [\n",
    "    'cum_reward','no_cum_reward','cum_norm_reward','tf_no_cum','tf_cum_reward','observe_action_cum']]\n",
    "i = 5\n",
    "plot_reward_hist(comb_hists[i], test_interval, \n",
    "                 comb_names[i], \n",
    "                 log=False, num_iteration=num_iteration, N_list=[1 for N_ in N_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot per # of agent\n",
    "cumb_hists = [[RA_cumu_hists[i],RA_inst_hists[i],RA_norm_hists[i],\n",
    "               RA_trsf_hists[i],RA_trfc_hists[i],RA_obac_hists[i]] for i in range(len(N_list))]\n",
    "cumb_names = [['RA_{1}_N{0}'.format(N_,name) for name in [\n",
    "    'cum_reward','no_cum_reward','cum_norm_reward','tf_no_cum','tf_cum_reward','observe_action_cum']] for N_ in N_list]\n",
    "j = 3\n",
    "plot_reward_hist(cumb_hists[j], test_interval, \n",
    "                 cumb_names[j], \n",
    "                 log=False, num_iteration=num_iteration, N_list=[N_list[j]]*6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(utils.plotting)\n",
    "from utils.plotting import *\n",
    "# Plot loss per # of agent\n",
    "cumb_hists = [[RA_cumu_lossess[i],RA_inst_lossess[i],RA_norm_lossess[i],\n",
    "               RA_trsf_lossess[i],RA_trfc_lossess[i],RA_obac_lossess[i]] for i in range(len(N_list))]\n",
    "cumb_names = [['RA_{1}_N{0}'.format(N_,name) for name in [\n",
    "    'cum_reward','no_cum_reward','cum_norm_reward','tf_no_cum','tf_cum_reward','observe_action_cum']] for N_ in N_list]\n",
    "\n",
    "j = 0\n",
    "plot_loss_hist(hists=cumb_hists[j], hist_names=cumb_names[j], log=False, \n",
    "               num_iteration=num_iteration, update_mode=UPDATE_PER_ITERATION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How about using CNN?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(utils.networks)\n",
    "reload(utils.agents)\n",
    "reload(utils.train_test_methods)\n",
    "from utils.networks import *\n",
    "from utils.agents import *\n",
    "from utils.train_test_methods import *\n",
    "\n",
    "num_episode=500\n",
    "test_interval=25\n",
    "num_test=25\n",
    "num_iteration=200\n",
    "BATCH_SIZE=256\n",
    "debug=False\n",
    "num_sample=50\n",
    "seed=2333333\n",
    "hidden=8\n",
    "\n",
    "NCN_list = [5,10] \n",
    "envCNN_list = []\n",
    "for N_ in NCN_list:\n",
    "    envCNN_list.append(\n",
    "        gym.make('ConsensusEnv:ConsensusContEnv-v0', N=N_, dt=0.1, Delta=0.05,\n",
    "              input_type=input_type, observe_type=observe_type, observe_action=O_ACTION\n",
    "        ).unwrapped\n",
    "    )\n",
    "LA_CNN_list = []\n",
    "RA_CNN_list = []\n",
    "for i,N_ in enumerate(NCN_list):\n",
    "    LA_CNN_list.append(\n",
    "        LearnerCNNAgent(device, N_, ns=envCNN_list[i].nf, na=env_list[i].na, hidden=hidden, n_hid=2, in_features=1)\n",
    "    )\n",
    "    RA_CNN_list.append(\n",
    "        RewardCNNAgent(device, N_, ns=envCNN_list[i].nf, na=env_list[i].na, hidden=hidden, n_hid=2, in_features=1)\n",
    "    )\n",
    "LA_CNN_hists = []\n",
    "RA_CNN_hists = []\n",
    "for i,N_ in enumerate(NCN_list):\n",
    "    LA_CNN_hists.append(\n",
    "        train(LA_CNN_list[i], envCNN_list[i], \n",
    "              num_episode, test_interval, num_test, num_iteration, BATCH_SIZE, num_sample, [-1,1], debug, \n",
    "                             seed=seed, reward_mode=FUTURE_REWARD_YES, update_mode=UPDATE_PER_EPISODE)\n",
    "    )\n",
    "    RA_CNN_hists.append(\n",
    "        train(RA_CNN_list[i], envCNN_list[i], \n",
    "              num_episode, test_interval, num_test, num_iteration, BATCH_SIZE, num_sample, [-1,1], debug, \n",
    "                             seed=seed, reward_mode=FUTURE_REWARD_YES, update_mode=UPDATE_PER_EPISODE)\n",
    "    )\n",
    "    print(\"Finished training env with {0} agents for CNN\".format(N_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep training those lazy gradients\n",
    "num_episode=2600\n",
    "test_interval=20\n",
    "num_test=25\n",
    "num_iteration=200\n",
    "BATCH_SIZE=256\n",
    "debug=False\n",
    "num_sample=50\n",
    "seed=2333333\n",
    "hidden=8\n",
    "\n",
    "for i,N_ in enumerate(NCN_list):\n",
    "    LA_CNN_hists[i] += train(LA_CNN_list[i], envCNN_list[i], \n",
    "              num_episode, test_interval, num_test, num_iteration, BATCH_SIZE, num_sample, [-1,1], debug, \n",
    "                             seed=seed, reward_mode=FUTURE_REWARD_YES)\n",
    "    RA_CNN_hists[i] += train(RA_CNN_list[i], envCNN_list[i], \n",
    "              num_episode, test_interval, num_test, num_iteration, BATCH_SIZE, num_sample, [-1,1], debug, \n",
    "                             seed=seed, reward_mode=FUTURE_REWARD_YES)\n",
    "    print(\"Finished training env with {0} agents for CNN\".format(N_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot results\n",
    "plot_reward_hist(LA_CNN_hists, test_interval, ['LA_CNN_N{0}'.format(N_) for N_ in NCN_list], \n",
    "                 log=False, num_iteration=num_iteration, N_list=[1 for N_ in NCN_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_reward_hist(RA_CNN_hists, test_interval, ['RA_CNN_N{0}'.format(N_) for N_ in NCN_list], \n",
    "                 log=False, num_iteration=num_iteration, N_list=[1 for N_ in NCN_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LA_CNN_list[0].save_model(\"LA_CNN_test1_3000epi_N5\")\n",
    "LA_CNN_list[1].save_model(\"LA_CNN_test1_3000epi_N10\")\n",
    "RA_CNN_list[0].save_model(\"RA_CNN_test1_3000epi_N5\")\n",
    "RA_CNN_list[1].save_model(\"LA_CNN_test1_3000epi_N10\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(utils.networks)\n",
    "reload(utils.agents)\n",
    "reload(utils.train_test_methods)\n",
    "from utils.networks import *\n",
    "from utils.agents import *\n",
    "from utils.train_test_methods import *\n",
    "\n",
    "# Additional training and testing and probing\n",
    "num_episode=1#200\n",
    "test_interval=20\n",
    "num_test=1#25\n",
    "num_iteration=200\n",
    "BATCH_SIZE=32\n",
    "debug=False\n",
    "num_sample=50\n",
    "seed=2333333\n",
    "hidden=8\n",
    "\n",
    "i = 0\n",
    "N_ = 5\n",
    "LCtest = LearnerCNNAgent(device, N_, ns=envCNN_list[i].nf, na=env_list[i].na, hidden=hidden, n_hid=2, in_features=1)\n",
    "RCtest = RewardCNNAgent(device, N_, ns=envCNN_list[i].nf, na=env_list[i].na, hidden=hidden, n_hid=2, in_features=1)\n",
    "\n",
    "LCtesth = train(LCtest, envCNN_list[i], \n",
    "          num_episode, test_interval, num_test, num_iteration, BATCH_SIZE, num_sample, [-1,1], debug)\n",
    "RCtesth = train(RCtest, envCNN_list[i], \n",
    "          num_episode, test_interval, num_test, num_iteration, BATCH_SIZE, num_sample, [-1,1], debug)\n",
    "print(\"Finished training env with {0} agents for CNN\".format(N_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How about trying ResNet or similar structure?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(utils.networks)\n",
    "reload(utils.agents)\n",
    "reload(utils.train_test_methods)\n",
    "from utils.networks import *\n",
    "from utils.agents import *\n",
    "from utils.train_test_methods import *\n",
    "\n",
    "num_episode=50\n",
    "test_interval=5\n",
    "num_test=10#25\n",
    "num_iteration=50#200\n",
    "BATCH_SIZE=64\n",
    "debug=False\n",
    "num_sample=50\n",
    "seed=2333333\n",
    "\n",
    "NCN_list = [5] \n",
    "envCNN_list = []\n",
    "for N_ in NCN_list:\n",
    "    envCNN_list.append(\n",
    "        gym.make('ConsensusEnv:ConsensusContEnv-v0', N=N_, dt=0.1, Delta=0.05,\n",
    "              input_type=input_type, observe_type=observe_type, observe_action=O_ACTION\n",
    "        ).unwrapped\n",
    "    )\n",
    "RA_RN_list = []\n",
    "for i,N_ in enumerate(NCN_list):\n",
    "    RA_RN_list.append(\n",
    "        RewardRNAgent(device, N_, ns=envCNN_list[i].nf, na=env_list[i].na, n_hid=2, in_features=1)\n",
    "    )\n",
    "RA_RN_hists = []\n",
    "RA_RN_lossess=[]\n",
    "for i,N_ in enumerate(NCN_list):\n",
    "    RA_RN_hists.append(\n",
    "        train(RA_RN_list[i], envCNN_list[i], \n",
    "              num_episode, test_interval, num_test, num_iteration, BATCH_SIZE, num_sample, [-1,1], debug, \n",
    "                             seed=seed, reward_mode=FUTURE_REWARD_YES, update_mode=UPDATE_PER_EPISODE,\n",
    "                             loss_history=RA_RN_lossess)\n",
    "    )\n",
    "    print(\"Finished training env with {0} agents for RN\".format(N_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot per # of agent\n",
    "plot_reward_hist(RA_RN_hists, test_interval, ['RA_ResNet_N5'], \n",
    "                 log=False, num_iteration=num_iteration, N_list=NCN_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(utils.plotting)\n",
    "from utils.plotting import *\n",
    "# Plot loss per # of agent\n",
    "plot_loss_hist(hists=[RA_RN_lossess], hist_names=['RA_ResNet_N5'], log=False, \n",
    "               num_iteration=num_iteration, update_mode=UPDATE_PER_ITERATION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Play around with the gradient agent"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
